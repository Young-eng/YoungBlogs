<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Young&#39;s Blog</title>
  
  
  <link href="https://young-eng.github.io/YoungBlogs/atom.xml" rel="self"/>
  
  <link href="https://young-eng.github.io/YoungBlogs/"/>
  <updated>2024-10-23T14:39:55.147Z</updated>
  <id>https://young-eng.github.io/YoungBlogs/</id>
  
  <author>
    <name>Young</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>RT-DETR</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/10/23/RT-DETR/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/10/23/RT-DETR/</id>
    <published>2024-10-23T14:11:04.000Z</published>
    <updated>2024-10-23T14:39:55.147Z</updated>
    
    <content type="html"><![CDATA[<h3 id="detrs-beat-yolos-on-real-time-object-detection1">DETRs BeatYOLOs on Real-time Object Detection<sup>[1]</sup></h3><blockquote><p>作者是来自北大和百度的Yian Zhao, Wenyu Lv等人。论文引用[1]:Lv, Wenyuet al. “DETRs Beat YOLOs on Real-time Object Detection.” 2024 IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR) (2023):16965-16974.</p></blockquote><h3 id="time">Time</h3><ul><li>2024.Apr</li></ul><h3 id="key-words">Key Words</h3><ul><li>hybrid encoder to process multi-scale features</li><li>uncertainty-minimal query selection to provide high-quality initialqueries to the decoder</li></ul><h3 id="动机">动机</h3><p>YOLO系列受到了NMS的影响，会降低推理速度。在不同的scenarios下，需要仔细地选择NMS的阈值。DETR不需要手工设计的components，没有NMS，但是计算成本高。因此，探索DETR能够做到实时是一个重要的方向。</p><h3 id="总结">总结</h3><ol type="1"><li>YOLO系列变成了最流行的实施目标检测的框架因为trade-off betweenspeed和accuracy。然而，观察到，YOLO的速度和精度收到了NMS的负面影响。端到端的DETR不需要NMS。然而，它的计算成本还是很高。这个不仅降低了推理的速度，也引入了超参数，造成速度和精度的不稳定。DETR去掉了手工设计的component，然而它的计算成本很高，很难做到实时。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>multi-scalefeatures的interaction造成了计算成本，实时的DETR需要重新设计encoder；objectqueries很难优化，阻碍了DETRs的性能。提出了query selectionschemes来代替vanilla learnable embeddings with encoderfeatures。然而，观察到，<strong>当前的query selection直接采用分类的scoreforselection,忽略了detector需要同时对目标的category和location同时进行model，这俩都决定了features的质量</strong>。这个不可避免地造成了lowlocalization confidence的encoder features被选为了initialqueries。因此导致了大量的不确定性。将<strong>queryinitialization视为一个breakthrough，来进一步提高performance</strong>。</p></li><li></li></ol><figure><imgsrc="https://figures.semanticscholar.org/3a6c423cc0f687fc43a671e55dce06f681df7476/4-Figure3-1.png"alt="Overview" /><figcaption aria-hidden="true">Overview</figcaption></figure><figure><imgsrc="https://figures.semanticscholar.org/3a6c423cc0f687fc43a671e55dce06f681df7476/4-Figure4-1.png"alt="CCFM" /><figcaption aria-hidden="true">CCFM</figcaption></figure><figure><imgsrc="https://figures.semanticscholar.org/3a6c423cc0f687fc43a671e55dce06f681df7476/5-Figure5-1.png"alt="Different types of encoders" /><figcaption aria-hidden="true">Different types of encoders</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;detrs-beat-yolos-on-real-time-object-detection1&quot;&gt;DETRs Beat
YOLOs on Real-time Object Detection&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自北大和百度的Yian Zhao, Wenyu Lv等人。论文引用[1]:Lv, Wenyu
et al. “DETRs Beat YOLOs on Real-time Object Detection.” 2024 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) (2023):
16965-16974.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2024.Apr&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hybrid encoder to process multi-scale features&lt;/li&gt;
&lt;li&gt;uncertainty-minimal query selection to provide high-quality initial
queries to the decoder&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;动机&quot;&gt;动机&lt;/h3&gt;
&lt;p&gt;YOLO系列受到了NMS的影响，会降低推理速度。在不同的scenarios下，需要仔细地选择NMS的阈值。DETR不需要手工设计的components，没有NMS，但是计算成本高。因此，探索DETR能够做到实时是一个重要的方向。&lt;/p&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;YOLO系列变成了最流行的实施目标检测的框架因为trade-off between
speed和accuracy。然而，观察到，YOLO的速度和精度收到了NMS的负面影响。端到端的DETR不需要NMS。然而，它的计算成本还是很高。这个不仅降低了推理的速度，也引入了超参数，造成速度和精度的不稳定。DETR去掉了手工设计的component，然而它的计算成本很高，很难做到实时。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Object Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>YOWOv3_lightweight</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/10/16/YOWOv3-lightweight/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/10/16/YOWOv3-lightweight/</id>
    <published>2024-10-16T01:33:17.000Z</published>
    <updated>2024-10-16T08:20:51.980Z</updated>
    
    <content type="html"><![CDATA[<h3id="yowov3-a-lightweight-spatio-temporal-joint-network-for-video-action-detection1">YOWOv3:A Lightweight Spatio-Temporal Joint Network for Video ActionDetection<sup>[1]</sup></h3><blockquote><p>作者是来自江南大学的Anlei Zhu, Yinghui Wang等人。论文引用[1]:Zhu,Anlei et al. “YOWOv3: A Lightweight Spatio-Temporal Joint Network forVideo Action Detection.” IEEE Transactions on Circuits and Systems forVideo Technology 34 (2024): 8148-8160.</p></blockquote><h3 id="time">Time</h3><ul><li>2024.Sept</li></ul><h3 id="key-words">Key Words</h3><ul><li>Channel Fusion and Attention Convolution Mix(CFACM) module</li><li>spatio-temporal shift module</li><li>一句话总结：借鉴了前人的 <strong>ACmix(Attention Channelmix)</strong> 和 <strong>TSM(Temporal shift module)</strong>模块，将它们拿过来缝合一下，完了。。。</li></ul><h3 id="总结">总结</h3><ol type="1"><li>时空行为检测网络，需要同时提取和融合时间和空间特征，经常导致目前的模型变得膨胀和很难实时和部署在边缘设备上，文章引入了一个高效实时的时空检测网路模型，YOWOv3，这个模型用了3D和2D backbone来本别提取时间和时空特征，通过继承卷积和自注意力，来设计一个轻量的时空特征融合模块，进一步增强了时空特征的抽取。将这个模块称之为CFACM,这个方法不仅在lightness上超过了最新的高效的STAD模型，减小了24%的模型的大小，同时提高了mAP，保持了很好的速度。现有的模型用3D卷积来提取时序信息，会受限于特定的设备上，为了减小3D卷积操作的潜在的不被支持在边缘部署的问题，用了一个时空shiftmodule by 2D 卷积，使得模型能够获取时序信息，将得到的特征插入multi-levelspatio-temporal feature extraction models.这样不仅将模型从3D卷积操作中释放出来，同时也很好地balance了速度和精度。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>STAD相比于传统了视觉目标检测，不仅需要检测行为主体，也需要独立地分析行为主体的行为。那就是理解目标的行为，例如开和关。不同用单张图像信息完成，需要时序信息。然而，一旦时序提取网路引入之后，模型变得臃肿，很难部署和直接使用。因此设计一个高效、轻量的时空行为检测网络是一个深度学习追求的目标。基于2D卷积的方法能保证网络的轻量，但是很难捕捉时序信息。导致进度的下降。例如C3D、I3D用3D卷积来捕捉时序信息。尽管这种方法能够精确地捕捉时序信息，参数大小和计算成本比较大，使其很难部署和实时运行。基于Transformer的模型能够有效地捕捉时序信息。</p></li><li><p>STAD不等同于图像目标检测，主要的区别在于actionsequences的识别，不仅是单张图片的目标的识别这就要求模型能够检测到目标。基于3DCNN的方法，首先用I3D来产生action areasuggestions，然后用Transformer-style架构来汇聚特征。</p></li><li><p>受YOWO的启发，进一步研究设计了一个更先进的STAD网络，利用主流的YOLOv8来提取空间特征。为了避免的3DCNN造成了网路臃肿的问题，用了一个仅有2D CNN的module来做spatio-temporalaction recognition，通过<strong>Temporal ShiftModule</strong>来得到时空特征信息。</p></li><li><p>采用了YOLOv8作为spatial featureextraction的backbone。用了一个高效的3D CNN来提取时序信息。</p></li><li><p><strong>Channel Fusion and Convolution Attention MixModule</strong>：提取到的特征被压缩之后，仅用卷积来融合空间和时空特征是不能得到全局特征。为了融合时空和时序特征，首先将它们在channel维度上进行concatenate，抽取时序特征之后，在最后的特征层上进行upsampling。因此，由于自注意力会带来计算量和参数量，不直接执行自注意力的计算。受<strong>ACMix</strong>的启发，结合卷积和自注意力，首先，用两个标准的<span class="math inline">\(3 \times 3\)</span> 卷积从concatenatedfeatures中提取特征，然后用3个 <span class="math inline">\(1 \times1\)</span>卷积来映射features。然后将它们reshape成 <spanclass="math inline">\(N\)</span> pieces。得到丰富的中间特征 <spanclass="math inline">\(3 \times N\)</span> featuremappings。之后，中间特征分为两个branches，一个分支执行自注意力：其中，shift和aggregation操作用来实现从localreceptivefield。另一个分支进行卷积计算。最后将两个分支的结果进行concatenated。</p></li><li><p>在实际中，发现ACMix需要大量的时间开销，因为它的unfold的设计需要高的内存消耗。为了避免这个，将window进行划分，进行cross-sparse自注意力计算，来减小运行时间。如图所示，思路就是：先在窗口内进行注意力的计算，得到局部的信息，然后通过Permutemethod，执行window内的计算。</p></li><li><p>用3D卷积能实现捕捉时序特征，但是一些设备不支持卷积操作，很难部署。另一个问题是：轻量的3D卷积提取时序特征确保了模型的高效，但是精度会下降。为了解决3D卷积的问题，不需要对整个网络做很多改变。放弃了3Dbackbone，在2D 卷积上加了一个<font color=red>Temporal ShiftModule</font>来提取时序信息。通过这个时空shiftmodule，输入的视频帧序列are shifted forward or backward intime。增加了temporalinformation的diveristy，以2D的方式，获得了时序信息。这个过程如图所示。首先，输入的视频进行格式转换，<strong>TemporalShift Module</strong>取代了2Dbackbone中的<strong>InvertedResidual</strong>结构，使得其能够提取时序信息。在headprocessing之后，得到了时序特征。下一步就是融合时空特征，得到最后的时空行为检测模型。</p></li></ol><p><imgsrc="https://dt5vp8kor0orz.cloudfront.net/d6ce4ceeaf1ffa2d64a9d8fd1370dd59419364ed/4-Figure1-1.png"alt="Structure" /> <span class="math inline">\(Fig.1^{[1]}\)</span> TheStructure Diagram of the YOWOv3 Framework. The input video frames areseparately processed through the spatial backbone network and thespatio-temporal backbone network to extract features. The spatialbackbone network utilizes a feature pyramid to extract richermulti-level features. T represents the number of frames in the inputvideo, which is 16 in this context. Upsampling is utilized to align thespatio-temporal features with the spatial features.</p><p><imgsrc="https://dt5vp8kor0orz.cloudfront.net/d6ce4ceeaf1ffa2d64a9d8fd1370dd59419364ed/5-Figure2-1.png"alt="CFACM" /> <span class="math inline">\(Fig.2^{[1]}\)</span> TheStructure Diagram of the CFACM Module. The Decoupled Feature FusionHeader inputs <span class="math inline">\(F_{Cls i}\)</span> and <spanclass="math inline">\(F_{Reg i}\)</span> are respectively fused with theextracted temporal features using the CFCAM module. When engaging withthe advanced convolution and self-attention fusion module ACISA, 2Dpositional encoding is performed prior to the computation of theself-attention branch. The self-attention and convolution branches arethen concatenated using adaptive weights <spanclass="math inline">\(\alpha\)</span> and <spanclass="math inline">\(\beta\)</span>.</p><p><imgsrc="https://dt5vp8kor0orz.cloudfront.net/d6ce4ceeaf1ffa2d64a9d8fd1370dd59419364ed/5-Figure3-1.png"alt="CFACM Attention" /> $Fig.3^{[1]} Self-attention computing branch ofCFACM Module. The calculation process of the two attention mechanisms isconsistent, but their inputs Q, K, V come from different sources. In thefirst self-attention calculation, Q, K, V come from three differentfeatures. In the second self-attention calculation, Q, K, V come fromthe same feature that has been integrated</p><p><imgsrc="https://dt5vp8kor0orz.cloudfront.net/d6ce4ceeaf1ffa2d64a9d8fd1370dd59419364ed/6-Figure5-1.png"alt="Temporal Shift Module" /> <spanclass="math inline">\(Fig.4^{[1]}\)</span> Spatio-temporal extractionbranch based on temporal shift module with only 2D convolution.</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;yowov3-a-lightweight-spatio-temporal-joint-network-for-video-action-detection1&quot;&gt;YOWOv3:
A Lightweight Spatio-Temporal Joint Network for Video Action
Detection&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自江南大学的Anlei Zhu, Yinghui Wang等人。论文引用[1]:Zhu,
Anlei et al. “YOWOv3: A Lightweight Spatio-Temporal Joint Network for
Video Action Detection.” IEEE Transactions on Circuits and Systems for
Video Technology 34 (2024): 8148-8160.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2024.Sept&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Channel Fusion and Attention Convolution Mix(CFACM) module&lt;/li&gt;
&lt;li&gt;spatio-temporal shift module&lt;/li&gt;
&lt;li&gt;一句话总结：借鉴了前人的 &lt;strong&gt;ACmix(Attention Channel
mix)&lt;/strong&gt; 和 &lt;strong&gt;TSM(Temporal shift module)&lt;/strong&gt;
模块，将它们拿过来缝合一下，完了。。。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;时空行为检测网络，需要同时提取和融合时间和空间特征，经常导致目前的模型变得膨胀和很难实时和部署在边缘设备上，文章引入了一个高效实时的时空检测网路模型，YOWOv3，这个模型用了3D
和2D backbone
来本别提取时间和时空特征，通过继承卷积和自注意力，来设计一个轻量的时空特征融合模块，进一步增强了时空特征的抽取。将这个模块称之为CFACM,这个方法不仅在lightness上超过了最新的高效的STAD模型，减小了24%的模型的大小，同时提高了mAP，保持了很好的速度。现有的模型用3D
卷积来提取时序信息，会受限于特定的设备上，为了减小3D卷积操作的潜在的不被支持在边缘部署的问题，用了一个时空shift
module by 2D 卷积，使得模型能够获取时序信息，将得到的特征插入multi-level
spatio-temporal feature extraction models.这样不仅将模型从3D
卷积操作中释放出来，同时也很好地balance了速度和精度。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Deformable-DETR</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/10/08/Deformable-DETR/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/10/08/Deformable-DETR/</id>
    <published>2024-10-08T01:57:19.000Z</published>
    <updated>2024-10-08T01:57:19.387Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>non-local</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/10/08/non-local/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/10/08/non-local/</id>
    <published>2024-10-08T01:57:08.000Z</published>
    <updated>2024-10-09T09:02:03.822Z</updated>
    
    <content type="html"><![CDATA[<h3 id="non-local-neural-networks1">Non-local NeuralNetworks<sup>[1]</sup></h3><blockquote><p>作者是来自CMU和FAIR的Xiaolong Wang, Ross Girshick, Abhinav Gupta,Kaiming He.论文引用[1]:Wang, X. et al. “Non-local Neural Networks.” 2018IEEE/CVF Conference on Computer Vision and Pattern Recognition (2017):7794-7803.</p></blockquote><h3 id="time">Time</h3><ul><li>2017.Nov</li></ul><h3 id="key-words">Key Words</h3><ul><li>non-local 和 3D 卷积可以被视为将C2D扩展到时间维度的两种方式。</li><li>long-range dependencies</li><li>computes the response at a position as a weighted sum of thefeatures at all positions</li><li>consider all positions</li></ul><h3 id="动机">动机</h3><ol start="2" type="1"><li>计算长程依赖在神经网络中很重要，对于系列数据(speech,language)，循环操作是主流，对于图像数据，通过构建卷积的deepstacks，能够得到大的感受野，建模长程依赖。卷积和循环操作都处理局部相邻信息，eitherin space ortime。因此，长程依赖只有当这些操作重复应用的时候才能捕捉到，通过数据逐步地propagating信号。重复Localoperation有一些限制：首先，计算是不高效的；其次，造成了优化困难；最后，这些挑战造成了<em>multihopdependency modeling</em>。</li></ol><span id="more"></span><h3 id="总结">总结</h3><ol type="1"><li><p>卷积和recurrent操作都是基于处理局部相邻信息的blocks的，本文中，提出了non-local的操作，作为buildingblocks通用family，用于捕捉long-range dependencies。受CV中经典的non-local方法的启发，作者的non-local的方法，<strong>计算一个位置的response as aweighted sum of the features at all positions</strong>。这个buildingblock能够插入很多架构中，对于视频分类任务，non-local模型表现很好。</p></li><li><p>在这篇文章当中，提出了non-local操作，来捕捉长程操作。提出的non-local操作是经典<strong>non-localmean操作的</strong>泛化，non-local 操作是计算一个位置的response as aweighted sum of the features at allpositions。non-local操作可以插入到很多架构中，对于视频分类任务，non-local模型表现很好。setofpositions可以是空间、时间或者时空；表明这个操作是可以用于图像、序列和视频的。</p></li><li><p>用non-local操作有一些优点：</p><ul><li>相比于卷积和循环操作的progressivebehavior，non-local操作通过计算两个任意位置的interactions，捕捉了长程依赖，不管它们之间的距离；</li><li>像实验中所展示的，non-local操作是高效的，仅用少量的layers就能实现比较好的结果。</li><li>non-local操作保持了variable inputsizes，能够很容易和其它操作结合。</li></ul></li><li><p>non-local操作在视频分类、目标检测/分割/姿态估计上都有很好的结果。</p></li><li><p><strong>Non-local image processing</strong>：non-localmeans是一个经典的滤波算法：计算图像中所有像素的weightedmean，它允许distant pixels基于patch的外观相似性来 contribute to thefiltered response at alocation，这个non-local滤波的想法后来发展成了BM3D：在一组相似的，non-local，patches上进行滤波操作。BM3D是一个solid image denoising baseline，blockmatching和神经网络用在一起，用于image enoising。non-localmatching也是纹理合成、超分和impainting的精髓。</p></li><li><p><strong>图模型</strong>：长程依赖可以通过图模型进行建模，例如条件随机场(conditionalrandom fields,CRF)，在context ofDNN，一个CRF能够被利用，来后处理网络的语义分割的预测。这个iterativemean-field inferenceCRF可以变成一个神经网络进行训练。相比之下，作者的方法是一个简单的feedforwardblock for computing non-localfiltering。不同于用于分割的方法，通用的component适用于分类和检测。这些方法都和一个更抽象的模型(图神经网络)相关。</p></li><li><p><strong>feedforward modeling forsequences</strong>：最近出现了一个用feedforward网络来建模speech和language中序列的趋势，在这些方法中，长程依赖能够被大的感受野捕获到。这些feedforwardmodel are amenable 来并行化执行，比广泛应用的循环网络更加高效</p></li><li><p><strong>self-attention</strong>：本文的工作和最近的自注意力相关，一个<strong>自注意力模块计算theresponse at a position in a sequence by attending to all positions andtaking their weighted average in an embeddingspace</strong>，<strong>自注意力可以被视为一种形式的non-localmean</strong>，在这个意义上，我们的工作**讲自注意力连接到更一般的non-local滤波操作，这些滤波操作用于图像视频问题。</p></li><li><p><strong>Interactionnetworks</strong>：这个工作最近提出来用于建模物理系统，它们在参与pairwiseinteractions的objects图上进行操作，Hoshen提出了在多智能体预测建模环境下更高效的VertexAttention IN，另一个变体，称为RelationNetworks，在其输入的所有位置对的特征嵌入上计算一个函数。作者的方法也处理所有的pairs。当non-local网络连接到其他方法，实验表明：模型的<em>non-locality</em>，和attention/interaction/relation的想法是orthogonal的，non-local建模，一个long-time crucial element of imageprocessing。在最近的神经网络中被忽视了。</p></li><li><p>视频分类架构：一个自然的视频分类的解决方式是结合CNNs for imagesand RNNs for sequences。相比之下，feedforward模型通过3D卷积实现 ，3D滤波器可以通过膨胀预训练的2D滤波器来获得。除了原始视频进行端到端的建模之外，发现光流和轨迹也有帮助。光流和轨迹是现有的modules，能够发现长程的、non-local的依赖。</p></li><li><p><strong>Non-local NeuralNetworks</strong>：定义深度神经网络中通用的non-local的操作: <spanclass="math display">\[\mathbf{y}_i=\frac{1}{\mathcal{C}(\mathbf{x})}\sum_{\forallj}f(\mathbf{x}_i,\mathbf{x}_j)g(\mathbf{x}_j).\]</span> 这里 <spanclass="math inline">\(i\)</span>是输出位置的index，它的response被计算，<spanclass="math inline">\(j\)</span>是index，来枚举所有可能的位置, <spanclass="math inline">\(X\)</span>是输入信号(image,sequence,video)，<spanclass="math inline">\(y\)</span>是输出信号，和<spanclass="math inline">\(X\)</span>有相同的size。一个pairwise function<span class="math inline">\(f\)</span>计算一个标量(代表relationship)between <span class="math inline">\(i\)</span> 和 <spanclass="math inline">\(j\)</span>。这个一元的function <spanclass="math inline">\(g\)</span>计算输入信号在位置 <spanclass="math inline">\(j\)</span>的一个representation. 这个response 通过factor <span class="math inline">\(C(x)\)</span>进行归一化。</p><p><strong>公式中的non-local是考虑了所有的位置。相比之下，一个卷积操作sumsup the weighted input in a localneighborhood(卷积操作是对局部邻域的加权输入进行求和)，时间<spanclass="math inline">\(i\)</span>的循环操作通常只基于当前和最新的timesteps</strong>。</p><p>这个non-local操作不同于全连接层，公式基于不同位置之间的relationships计算responses。换句话说，<spanclass="math inline">\(X_j\)</span>和 <spanclass="math inline">\(X_i\)</span>之间的relationship不是 <spanclass="math inline">\(fc\)</span>中输入数据的function，不像non-locallayers。更进一步，公式支持不同输入的size，保持了输出中的一致的size，对比之下，一个<spanclass="math inline">\(fc\)</span>层要求一个固定的size的输入/输入，丢失了位置一致性。</p><p>一个non-local 操作是一个灵活的building block，能够很容易地和卷积/循环layers结合，可以加入到神经网络中earlier part，不同于<spanclass="math inline">\(fc\)</span>层只能用在最后。这样能够构建一个丰富的hierarchy，结合了non-local和localinformation。</p></li><li><p><strong>Instantiations</strong>：接下来描述 <spanclass="math inline">\(f\)</span>和 <spanclass="math inline">\(g\)</span>的不同的版本，有趣的是，实验表明：non-local模型对这些选择不sensitive，表明这个通用的non-localbehaviour是主要的理由。简单来说，考虑 <spanclass="math inline">\(g\)</span>是一种线性的embedding：<spanclass="math inline">\(g(x_j) = W_gX_j\)</span>，<spanclass="math inline">\(W_g\)</span>是一个可学习的权重矩阵，这个通过空间上的<span class="math inline">\(1 \times 1\)</span>卷积或者 时空上的 <spanclass="math inline">\(1 \times 1 \times 1\)</span>卷积实现。接下来塔伦pairwiwse functin <spanclass="math inline">\(f\)</span>。</p><ul><li><strong>高斯</strong>：跟随non-local mean 和 双向的滤波器， <spanclass="math inline">\(f\)</span>的一个自然的选择就是高斯函数，考虑：<spanclass="math display">\[f(\mathbf{x}_i,\mathbf{x}_j)=e^{\mathbf{x}_i^T\mathbf{x}_j}.\]</span></li></ul><p>这里 <span class="math inline">\(X^T_i X_j\)</span>是一个dot-productsimilarity。欧氏距离也可以用，但是dot-product执行起来更友好，归一化factor设为：<spanclass="math inline">\(\mathcal{C}(\mathbf{x})=\sum_{\forallj}f(\mathbf{x}_{i},\mathbf{x}_{j})\)</span></p><ul><li><strong>嵌入高斯</strong>：高斯函数的一个简单的扩展是计算向量空间中的相似性：<spanclass="math display">\[f(\mathbf{x}_i,\mathbf{x}_j)=e^{\theta(\mathbf{x}_i)^T\phi(\mathbf{x}_j)}.\]</span></li></ul><p>这里 <span class="math inline">\(\theta(\mathbf{x}_i) =W_\theta\mathbf{x}_i\)</span> 和 <spanclass="math inline">\(\phi(\mathbf{x}_j) = W_\phi\mathbf{x}_j\)</span>是两个嵌入，设置 <spanclass="math inline">\(\mathcal{C}(\mathbf{x})=\sum_{\forallj}f(\mathbf{x}_{i},\mathbf{x}_{j}).\)</span></p><p>注意到自注意力模块嵌入高斯版本的一个non-local的特例。可以从事实看出来：给定一个<spanclass="math inline">\(i\)</span>，<spanclass="math inline">\({\frac{1}{\mathcal{C}(\mathbf{x})}}f(\mathbf{x}_{i},\mathbf{x}_{j})\)</span>在 dimension <span class="math inline">\(j\)</span>上是一个 softmax计算。因此，有 <spanclass="math inline">\(\mathbf{y}=softmax(\mathbf{x}^TW_\theta^TW_\phi\mathbf{x})g(\mathbf{x})\)</span>，这个是自注意力的形式。这个工作将最近的自注意力模块和经典的non-localmeans联系起来，将sequential self-attentionnetwork扩展到通用的space/spacetime non-local network for image/videorecognition。</p><p>作者展示了注意力行为不是所研究的application重要。</p><ul><li><strong>Dot product</strong>：<span class="math inline">\(f\)</span>可以定义为： <span class="math display">\[f ( \mathbf{x}_{i}, \mathbf{x}_{j} )=\theta( \mathbf{x}_{i} )^{T} \phi(\mathbf{x}_{j} ).\]</span></li></ul><p>这里采用嵌入的版本，设置归一化factor <span class="math inline">\(C(x)= N\)</span>，这里 <span class="math inline">\(N\)</span>是 <spanclass="math inline">\(x\)</span>中的位置的数量，不同于 <spanclass="math inline">\(f\)</span>的和,因为它简化了梯度计算。一个归一化是有必要的。这个dot-product和嵌入高斯主要的不同是softmax，这个扮演了激活函数的角色。</p><ul><li><strong>Concatenation</strong>：在Relation Networks中的pairwisefunction用到了concatenation，评估 <span class="math inline">\(f\)</span>的一个 concatenation的形式：</li></ul><p><span class="math display">\[f ( \mathbf{x}_{i}, \mathbf{x}_{j} )=\mathrm{R e L U} (\mathbf{w}_{f}^{T} [ \theta( \mathbf{x}_{i} ), \; \phi( \mathbf{x}_{j} )] ).\]</span></p><p>这里，[.,.]表示 concatenation，<spanclass="math inline">\(w_f\)</span>是一个权重向量，将concatenatedvector投射到一个标量。如上，设置 <span class="math inline">\(C(x) =N\)</span>，这里，采用 ReLU。以上的变体展示了通用non-local操作的灵活性。</p></li><li><p><strong>Non-localBlock</strong>：将non-local操作弄到一个non-localblocks，能够用于很多现有的架构，定义一个non-local block如下：</p><p><span class="math display">\[\mathbf{z}_{i}=W_{z} \mathbf{y}_{i}+\mathbf{x}_{i},\]</span> 这里 <spanclass="math inline">\(&quot;+X_i&quot;\)</span>表示residualconnection，这个residual connection使得能够将一个新的non-localblock插入到任意一个预训练的模型，不需要破坏初始的behavior(如果 <spanclass="math inline">\(W_z\)</span>是初始化为0)。</p><ul><li><strong>Implementation of non-local blocks</strong>：设置 <spanclass="math inline">\(W_g、W_{\theta}、W_{\phi}\)</span>的通道数为x的一半。这个follow了bottleneck的设计，减小了block的一半的计算。这个weightmatrix <span class="math inline">\(W_z\)</span> 计算了一个position-wiseembedding on <span class="math inline">\(y_i\)</span>，匹配了<spanclass="math inline">\(x\)</span>的通道。一个subsample的trick可以用于进一步减小计算。修改公式(1)为：<br /><span class="math display">\[y_i = \frac{1} {\mathcal{C} ( \mathbf{\hat{x}} )} \sum_{\forall j} f (\mathbf{x}_{i}, \mathbf{\hat{x}}_{j} ) g ( \mathbf{\hat{x}}_{j} )\]</span></li></ul><p><span class="math inline">\(\hat{x}\)</span> 是x的subsample。在空间域中执行这个，能够减小pairwise大约1/4的计算。这个trick没有改变non-local的behavior。但是使得计算更稀疏。可以通过增加一个maxpooling layer after <span class="math inline">\(\phi\)</span> 和 <spanclass="math inline">\(g\)</span> 来实现。</p></li></ol><figure><imgsrc="https://dt5vp8kor0orz.cloudfront.net/8899094797e82c5c185a0893896320ef77f60e64/3-Figure2-1.png"alt="non-local" /><figcaption aria-hidden="true">non-local</figcaption></figure><p><span class="math inline">\(Fig.1^{[1]}\)</span> A spacetimenon-local block. The feature maps are shown as the shape of theirtensors, e.g., <span class="math inline">\(T \times H \times W \times1024\)</span> for 1024 channels (proper reshaping is performed whennoted). “⊗” denotes matrix multiplication, and “⊕” denotes element-wisesum. The softmax operation is performed on each row. The blue boxesdenote <span class="math inline">\(1 \times 1 \times 1\)</span>convolutions. Here we show the embedded Gaussian version, with abottleneck of 512 channels. The vanilla Gaussian version can be done byremoving <span class="math inline">\(\theta\)</span> and <spanclass="math inline">\(\phi\)</span>, and the dot-product version can bedone by replacing softmax with scaling by <spanclass="math inline">\(1/N\)</span>.</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;non-local-neural-networks1&quot;&gt;Non-local Neural
Networks&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自CMU和FAIR的Xiaolong Wang, Ross Girshick, Abhinav Gupta,
Kaiming He.论文引用[1]:Wang, X. et al. “Non-local Neural Networks.” 2018
IEEE/CVF Conference on Computer Vision and Pattern Recognition (2017):
7794-7803.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2017.Nov&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;non-local 和 3D 卷积可以被视为将C2D扩展到时间维度的两种方式。&lt;/li&gt;
&lt;li&gt;long-range dependencies&lt;/li&gt;
&lt;li&gt;computes the response at a position as a weighted sum of the
features at all positions&lt;/li&gt;
&lt;li&gt;consider all positions&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;动机&quot;&gt;动机&lt;/h3&gt;
&lt;ol start=&quot;2&quot; type=&quot;1&quot;&gt;
&lt;li&gt;计算长程依赖在神经网络中很重要，对于系列数据(speech,
language)，循环操作是主流，对于图像数据，通过构建卷积的deep
stacks，能够得到大的感受野，建模长程依赖。卷积和循环操作都处理局部相邻信息，either
in space or
time。因此，长程依赖只有当这些操作重复应用的时候才能捕捉到，通过数据逐步地propagating信号。重复Local
operation有一些限制：首先，计算是不高效的；其次，造成了优化困难；最后，这些挑战造成了&lt;em&gt;multihop
dependency modeling&lt;/em&gt;。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Recognition" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Recognition/"/>
    
  </entry>
  
  <entry>
    <title>Exploring Plain ViT for Object Detection</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/09/26/Exploring-Plain-ViT-for-Object-Detection/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/09/26/Exploring-Plain-ViT-for-Object-Detection/</id>
    <published>2024-09-26T14:32:12.000Z</published>
    <updated>2024-10-06T12:35:43.095Z</updated>
    
    <content type="html"><![CDATA[<h3id="exploring-plain-vision-transformer-backbones-for-object-detection1">ExploringPlain Vision Transformer Backbones for ObjectDetection<sup>[1]</sup></h3><blockquote><p>作者是来自FAIR的Yanghao Li, Hanzi Mao, Ross Girshick和Kaiming He.论文引用[1]:Li, Yanghao, et al. "Exploring plain vision transformerbackbones for object detection." European conference on computer vision.Cham: Springer Nature Switzerland, 2022.</p></blockquote><h3 id="time">Time</h3><ul><li>2022.Mar</li></ul><h3 id="key-words">Key Words</h3><ul><li>Plain ViT for Object Detection</li></ul><h3 id="总结">总结</h3><ol type="1"><li>作者探索了<strong>plain, non-hierarchical</strong>ViT作为backbone，用于objectdetection，这个涉及使得原始的ViT架构能够被fine-tuned，用于objectdetection，不需要重新设计一个hierarchical backbone forpre-training。只需很小的adaptations for fine-tuning，这个plain-backbonedetector能够实现很好的结果。作者观察到：<ul><li><strong>从单个尺度的feature map构建一个simple featurepyramid就足够了，不需要FPN的设计</strong></li><li><strong>用window attention(without shifting)，辅以很少的cross-windowpropagation blocks就足够了。</strong></li></ul>用经过MAE预训练的plain ViTbackbone，detector称之为(ViTDet)，能够和之前的基于hierarchicalbackbone的leading methods竞争。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>现在的目标检测器通常包括一个<strong>backbone featureextractor</strong>(agnostic to the detectiontask)，和一些<strong>necks</strong>，<strong>heads</strong>，这些包含了检测相关的先验知识。necks/heads中的通常的组件有RoI操作，RPN 或者anchors，FPN, etc. 如果task-specificnecks/heads的设计从backbone的设计中解耦出来，它们可能evolve inparallel。从经验上来看，目标检测的研究从大的独立的通用的backbones和detection-specificmodules中受益。很长一段时间，这些backbone都是<font color=red>multi-scale,hierarchical</font>的架构，是由于ConvNet的设计，这个影响了neck/head的设计。</p></li><li><p>过去的一段时间，ViT是一个powerful backbone for visualrecognition。不同于典型的ConvNets，原始的ViT是一个<em>plain,non-hierarchical</em> 的架构，保持了single-scale feature mapthroughout。当用于目标检测时，遇到了挑战。如何用上游预训练的plainbackbone解决下游任务中的多尺度的目标？plainViT是否不够高效用于high-resolution detection images?一个解决方式：重新引入hierarchical designs into backbone，例如<strong>SwinTransformers</strong>相关的，继承了ConvNet的检测器的设计。</p></li><li><p>在这篇工作中，<strong>作者探索只用 <em>plain,non-hierarchical</em>的 backbones来做目标检测。这个将pre-trainingdesign和fine-tuningdemands进行解耦</strong>。保持上游和下游任务的独立性。这个方向同样followsViT的<strong>Philosophy： 更少的inductive bias</strong>。局部自注意力能够学习<font color=red>translatio-equivariantfeatures</font>，它们也能够从特定的监督或者自监督中学习<font color=red>scale-equivariantfeatures</font>。</p></li><li><p>作者目的不在于开发新的components；相反，用最小的改变，来足够克服上述的challenges。<strong>只从plainViT backbone的最后一个feature map中构建一个简单的featurepyramid</strong>，这个抛弃了FPN的设计，去掉了很多hierarchicalbackbone的要求。为了高效地从high-resolutionimages提取特征，检测器用了简单的<strong>non-overlapping windowattention(without shifting)</strong>，用了一些cross-window blocks，couldbe global attention or convolutions。用于propagateinformation。<strong>这些调整只在fine-tuning中，不改变pre-training</strong>。</p></li><li><p>作者简单的设计取得了很好的结果。发现FPN的设计在plainViT中不是必须的，its benefit能够通过从一个large-stride的single-scalemap构建的简单的pyramid得到。同时也发现， windowattention是足够的，只要信息在windows之间很好地propagated。取得这么好的结果可能部分是由于MAE预训练的作用。</p></li><li><p>除了这些结果，作者的方法保持了<strong>将detector-specificdesigns从task-agnostic的backbone中解耦</strong>，这个思路和将Transformerbackbone重新设计成multi-scalehierarchies的趋势相反。使得检测器和ViT的不同方向的发展能够兼容。</p></li><li><p><strong>related work</strong>：UViT是一个single-scale Transformerfor object detection。 在pre-training中改变了网络架构。</p></li><li><p><strong>BackboenAdaptation</strong>：作者关注这个场景：<strong>预训练的backbone进行globalself-attention, 在fine-tuning中，adapted tohigher-resolution</strong>。这和近期的方法直接修改attention computationwith backbone pretraning不同，不需要重新设计预训练的架构。</p><ul><li>在fine-tuning的时候，将一个high-resolution的feature map分成regularnon-overlapping windows，自注意力是在每个window内部进行。</li><li>不同于Swin，不用'shift' acrosslayers，为了允许信息propagation,只用了很少的blocks that can go acrosswindows。将一个pre-trained backbone <em>evenly</em> 分成 4个 subsets ofblocks，在每个subset的最后一个block用一个propagationstrategy。研究了两个strategy：</li><li><em>Global propagation</em>：在每个subset的最后一个block机型globalself-attention,这样的话，计算和存储的成本就比较小。类似于hybrid windowattention</li><li><em>Conv Propagation</em>：在每个subset后加一个convblock，这个block是residual block，包含了一个或多个conv和一个identityshortcut。这个block的最后一层初始化为0，因此这个block的初始状态是identity，这样使得可以将其插入到pre-trainedbackbone的任何一个位置。</li></ul></li><li><p><strong>Discussion</strong>：目标检测器包含taskagnostic的components，例如backbone；还有其它的components是task-specific的，例如:RoIheads。这个model decomposition使得task-agnosticcomponents可以用non-detectiondata进行pre-trained，可能会有好处，因为detection trainingdata是相对稀缺的。从这个角度，<strong>追求一个backbone that involvesfewer inductivebiases是合理的，因为backbone可以用大量的数据进行训练或者进行自监督训练；相比之下，detectiontask-specific components有相对少的数据，可能从额外的inductivebiases中受益</strong>。追求更少的additional inductive biases的detectionheads是一个活跃的工作。受这些观察的启发，<strong>ViT关注在减小 inductivebiases on translationequivarnace。在这篇文章里，是关于有更少或者没有inductive biases on scaleequivariance in the backbone</strong>。作者猜想：plain backbone实现scaleequivariance的方式是从数据中学习先验知识，类似于它怎样学习translationequivariance 和locality without convolutions。</p></li><li><p>在IN-1K上进行监督预训练的结果比没有预训练的好，MAE预训练的效果更好。作者猜想：<strong>vanillaViT，with fewer inductive biases,可能需要higher-capacity来学习translation和scale equivariantfeatures，然而higher-capacity的模型可能更容易过拟合，MAE预训练能够缓解这个问题</strong>。</p></li><li><p><strong>结论</strong>：作者的探索展示了<font color=red>plain-backbonedetection is a promising researchdirection</font>。这个方法保持了通用backbone和下游task-specificdesigns的独立性，将pre-training从fine-tuning中解耦。</p></li></ol><p><imgsrc="https://dt5vp8kor0orz.cloudfront.net/a09cbcaac305884f043810afc4fa4053099b5970/2-Figure1-1.png"alt="hierarchical" /> $Fig.1^{[1]}A typical hierarchical-backbonedetector (left) vs. our plain-backbone detector (right). Traditionalhierarchical backbones can be naturally adapted for multi-scaledetection, e.g., using FPN. Instead, we explore building a simplepyramid from only the last, large-stride (16) feature map of a plainbackbone</p><p><imgsrc="https://dt5vp8kor0orz.cloudfront.net/a09cbcaac305884f043810afc4fa4053099b5970/5-Figure2-1.png"alt="feature pyramid" /> <spanclass="math inline">\(Fig.2^{[2]}\)</span> Building a feature pyramid ona plain backbone. (a) FPN-like: to mimic a hierarchical backbone, theplain backbone is artificially divided into multiple stages. (b)FPN-like, but using only the last feature map without stage division.(c) Our simple feature pyramid without FPN. In all three cases, stridedconvolutions/deconvolutions are used whenever the scale changes.</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;exploring-plain-vision-transformer-backbones-for-object-detection1&quot;&gt;Exploring
Plain Vision Transformer Backbones for Object
Detection&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自FAIR的Yanghao Li, Hanzi Mao, Ross Girshick和Kaiming He.
论文引用[1]:Li, Yanghao, et al. &quot;Exploring plain vision transformer
backbones for object detection.&quot; European conference on computer vision.
Cham: Springer Nature Switzerland, 2022.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2022.Mar&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Plain ViT for Object Detection&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;作者探索了&lt;strong&gt;plain, non-hierarchical&lt;/strong&gt;
ViT作为backbone，用于object
detection，这个涉及使得原始的ViT架构能够被fine-tuned，用于object
detection，不需要重新设计一个hierarchical backbone for
pre-training。只需很小的adaptations for fine-tuning，这个plain-backbone
detector能够实现很好的结果。作者观察到：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;从单个尺度的feature map构建一个simple feature
pyramid就足够了，不需要FPN的设计&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;用window attention(without shifting)，辅以很少的cross-window
propagation blocks就足够了。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
用经过MAE预训练的plain ViT
backbone，detector称之为(ViTDet)，能够和之前的基于hierarchical
backbone的leading methods竞争。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Object Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>人类群星闪耀时</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/09/23/%E4%BA%BA%E7%B1%BB%E7%BE%A4%E6%98%9F%E9%97%AA%E8%80%80%E6%97%B6/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/09/23/%E4%BA%BA%E7%B1%BB%E7%BE%A4%E6%98%9F%E9%97%AA%E8%80%80%E6%97%B6/</id>
    <published>2024-09-23T11:30:08.000Z</published>
    <updated>2024-09-23T11:46:44.870Z</updated>
    
    <content type="html"><![CDATA[<h3 id="人类群星闪耀时">人类群星闪耀时</h3><blockquote><p>这是斯蒂芬.茨威格的书，本科的时候看了《一个陌生女人的来信》，还有一些小说集，真心写的不错，茨威格的心思太细腻了，将人物的心理活动描写的活灵活现，感觉到书中的人物都似乎真实的在脑海中存在过；又似乎像是把自己的心里想法描写出来了，这是我之前读完茨威格的书的最大的感受。这次的《人类群星闪耀时》,十四篇历史特写，不知道又会是什么样的故事。</p></blockquote><h3 id="序言">序言</h3><p>"一个真正具有世界历史意义的时刻--一个人类的群星闪耀时刻出现以前，必然会有漫长的无谓岁月流逝而去”。“历史才是真正的诗人和戏剧家，任何一个作家都别想超越历史本身。”</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;人类群星闪耀时&quot;&gt;人类群星闪耀时&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;这是斯蒂芬.茨威格的书，本科的时候看了《一个陌生女人的来信》，还有一些小说集，真心写的不错，茨威格的心思太细腻了，将人物的心理活动描写的活灵活现，感觉到书中的人物都似乎真实的在脑海中存</summary>
      
    
    
    
    <category term="Books" scheme="https://young-eng.github.io/YoungBlogs/categories/Books/"/>
    
    
  </entry>
  
  <entry>
    <title>MViTv2</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/09/20/MViTv2/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/09/20/MViTv2/</id>
    <published>2024-09-20T02:36:18.000Z</published>
    <updated>2024-09-20T14:37:15.979Z</updated>
    
    <content type="html"><![CDATA[<h3id="mvitv2-improved-multiscale-vision-transformers-for-classification-and-detection1">MViTv2:Improved Multiscale Vision Transformers for Classification andDetection<sup>[1]</sup></h3><blockquote><p>作者和MViT一样，是来自FAIR和UC Berkeley的Yang hao Li, Chao-YuanWu等人。论文引用[1]:Li, Yanghao et al. “MViTv2: Improved MultiscaleVision Transformers for Classification and Detection.” 2022 IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR) (2021):4794-4804.</p></blockquote><h3 id="time">Time</h3><ul><li>2021.Dec</li></ul><h3 id="key-words">Key Words</h3><ul><li>MViT that incorporates decomposed relative positional embeddings andresidual pooling connections. <span id="more"></span></li></ul><h3 id="总结">总结</h3><ol type="1"><li>ViT用于高分辨率的object detection和space-time video understandingtasks仍然具有挑战。<strong>密集的visual signals 在计算和memoryrequirements上提出了挑战，因为在Transformer-based model中，scalequadratically in complexity within the self-attentionblocks</strong>。大家有不同的策略来解决这个问题：<ul><li><font color=red>local attention computation within a window forobject detection</font></li><li><font color=red>pooling attention that locally aggregates featuresbefore computing self-attention in video tasks</font>.MViT就是采用的后者，不同于ViT：ViT的整个网络是一个固定的resolution,MViT是一个feature hierarchy with multiple stages starting fromhigh-resolution to low-resolution.</li></ul></li><li>MViTv2在MViT的基础上，做了几点改进：<ul><li><strong>通过提高pooling attention along two axes</strong>:<ul><li>shift-invariant positional embeddings using <em>decomposed</em>location distances to inject position information in Transformerblocks</li><li>a residual pooling connection to compensate the effect of poolingstrides in attention computation.</li></ul></li><li>采用standard dense prediction framework: Mask RCNN with FPN, usingimproved structure of MViT, apply it to object detection and instancesegmentation. 作者研究MViT是否能通过 <strong>pooling attention</strong>来克服computation和memory cost, 来处理high-resolution visual input.实验表明：<strong>pooling attention is more effective than local windowattention mechanisms</strong>, 进一步develop了一个<em>simple-yet-effective Hybrid window attention</em>,能够<strong>complement pooling attention for better accuracy/computetradeoff</strong>.</li></ul></li><li>Improved Multiscale Vision Transformers.<ul><li><p><strong>Improved Pooling Attention</strong>：</p><ul><li><strong>Decomposed Relative Positional Embeddings</strong>: MViT展示了modeltokens之间的interaction的能力，它们关注content，而不是structure，space-timestructure建模仅依赖于<em>absolute postionalembedding</em>，来提供<font color=red>locationinformation</font>。这个忽略了<strong>shift-invariance invision</strong>的基本原则。<strong>也就是说：MViT对两个patches之间的interaction的建模会根据它们的绝对位置而变化，即使它们的relativepositions保持不变</strong>。为了解决这个问题，把仅依赖于tokens之间的相对locationdistance的相对位置嵌入，与pooled self-attentioncomputation相结合。将relative position between two input elements,<em>i</em> and <em>j</em> 编码到 positional embedding <spanclass="math inline">\(R_{p(i),p(j)}\in\mathbb{R}^d,\)</span> <spanclass="math inline">\(p(i),p(j)\)</span> 表示 element <em>i</em> 和<em>j</em>的 spatial position，这个成对的 encoding representation然后被嵌入到 self-attention module之中。 <spanclass="math display">\[\mathrm{Attn}(Q,K,V)=\mathrm{Softmax}\left((QK^{\top}+E^{(\mathrm{rel})})/\sqrt{d}\right)V,\\\mathrm{where}\quadE_{ij}^{(\mathrm{rel})}=Q_{i}\cdotR_{p(i),p(j)}.\text{(3)}\]</span></li></ul><p>然而，可能的embeddings的数量 <spanclass="math inline">\(R_{p(i),p(j)}\)</span> scale in <spanclass="math inline">\(O(TWH)\)</span>，计算比较expensive，为了减小complexity，将element<em>i</em> 和 <em>j</em>之间的 distance computation 在spatiotemporalaxes方向上进行 decompose， <spanclass="math display">\[R_{p(i),p(j)}=R_{h(i),h(j)}^\mathrm{h}+R_{w(i),w(j)}^\mathrm{w}+R_{t(i),t(j)}^\mathrm{t},\]</span></p><p><span class="math inline">\(R^h，R^w, R^t\)</span> 分别是height,width和temporal axes方向上的 positional embeddings，<spanclass="math inline">\(h(i), w(i), t(i)\)</span> 分别表示 vertical,horizontal 和temporal position of token <em>i</em>，注意到 <spanclass="math inline">\(R^t\)</span>是一个 optional，在video case中required，来support temporal dimension。相比之下，decomposedembeddings将 learned embeddings的数量减小到 <spanclass="math inline">\(O(T+W+H)\)</span>，对于early-stage,high-resolution的feature maps有很大的影响。</p><ul><li><strong>Residual pooling connection</strong>：pooling attention对于减小计算复杂度和memory requirements in attentionblocks是非常有效的，MViTv1有很大的strides on <spanclass="math inline">\(K\)</span>和 <spanclass="math inline">\(V\)</span> tensors than the stride of the <spanclass="math inline">\(Q\)</span> tensor,which is only downsampled if theresolution of the output sequence changes across stages。这个motivatesus 来增加 residual pooling connection with the (pooled)Qtensor，来增加information flow，促进pooling attentionblocks的训练。引入了一个新的residual pooling connection inside theattention blocks，增加一个pooled query tensor to output sequence <spanclass="math inline">\(Z\)</span>。</li></ul></li><li><p><strong>MViT for objectdetection</strong>：MViT的层次化结构，产生了多尺度的feature maps in fourstages，因此可以很自然地集成到FPN网络里。</p><ul><li><font color=red>Hybrid windowattention</font>Transformer中的自注意力有quadratic complexity w.r.t thenumber of tokens。对于目标检测来说，它有更高分辨率的输入和featuremaps，文章中，研究了两种方式，减小compute 和memorycomplexity：首先，pooling attention；第二个是 window attention。 Poolingattention和windowattention都是用来控制自注意力的复杂度的，通过减小query,key,value的size，它们的内在本质是不一样的：<strong>Pooling attention poolsfeatures by downsampling them via local aggregation，但是keeps a globalself-attention computation</strong>，然而，<strong>window attentionkeeps the resolution of tensors，但是performs self-attention locally bydividing the input(patchified tokens)into non-overlapping windows，onlycompute local self-attention within eachwindow</strong>。这个内在本质的不同，motivates us to study它们是否能够在目标检测任务上互补。</li></ul><p>尽管window attention仅在windows内进行localself-attention，缺乏跨windows的connections。不同于Swin(用了shiftedwindows来减弱这个问题),提出了简单的<em>hybrid window attention</em>design to add cross-window connections。</p><ul><li><font color=red>Positional embeddings indetecition</font>：不同于ImageNet的图像分类，其输入是fixedresolution，目标检测包括了varying sizes inputs。对于positionalembeddings in MViT(either absolute or relative),首先从ImageNet预训练的、对应于 <span class="math inline">\(224 \times224\)</span> input size的positinoalembeddings的weights进行初始化。然后将它们interpolate到各自的sizes，用于目标检测的训练。</li></ul></li></ul></li></ol><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/9137efc758f80dd22bb56f82cca5c94f78a5db3e/1-Figure1-1.png"alt="MViTv2" /> <span class="math inline">\(Fig.1^{[1]}\)</span> MViTv2is a multiscale transformer with state-of-the-art performance acrossthres visual recognition tasks.</p><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/9137efc758f80dd22bb56f82cca5c94f78a5db3e/3-Figure2-1.png"alt="Improved Pooling Attention" /> <spanclass="math inline">\(Fig.2^{[1]}\)</span> The improved PoolingAttention mechanism that incorporating decomposed relative positionembedding, <span class="math inline">\(R_{p(i),p(j)}\)</span>, andresidual pooling connection modules in the attention block.</p><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/9137efc758f80dd22bb56f82cca5c94f78a5db3e/4-Figure3-1.png"alt="Multiscale MViTv2" /> <spanclass="math inline">\(Fig.3^{[1]}\)</span> MViT backbone used with FPNfor object detection. The multiscale transformer features naturallyintegrate with standard FPN</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;mvitv2-improved-multiscale-vision-transformers-for-classification-and-detection1&quot;&gt;MViTv2:
Improved Multiscale Vision Transformers for Classification and
Detection&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者和MViT一样，是来自FAIR和UC Berkeley的Yang hao Li, Chao-Yuan
Wu等人。论文引用[1]:Li, Yanghao et al. “MViTv2: Improved Multiscale
Vision Transformers for Classification and Detection.” 2022 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) (2021):
4794-4804.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2021.Dec&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;MViT that incorporates decomposed relative positional embeddings and
residual pooling connections.</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Vision Transformer" scheme="https://young-eng.github.io/YoungBlogs/tags/Vision-Transformer/"/>
    
  </entry>
  
  <entry>
    <title>LW-DETR</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/09/16/LW-DETR/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/09/16/LW-DETR/</id>
    <published>2024-09-16T14:57:15.000Z</published>
    <updated>2024-10-23T14:07:59.502Z</updated>
    
    <content type="html"><![CDATA[<h3id="lw-detr-a-transformer-replacement-to-yolo-for-real-time-detection1">LW-DETR:A Transformer Replacement to YOLO for Real-TimeDetection<sup>[1]</sup></h3><blockquote><p>作者是来自百度、阿德莱德大学、北航、自动化所和澳洲国立大学的QiangChen,Xiangbo Su, Xinyu Zhang等人。论文引用[1]:</p></blockquote><h3 id="key-words">Key Words</h3><ul><li>Real-Time Detection With Transformer</li><li>interleaved window and global attention</li><li>window-major order feature map organization</li></ul><h3 id="time">Time</h3><ul><li>2024.Jun</li></ul><h3 id="总结">总结</h3><ol type="1"><li>作者提出了一个light-weight transformer,LW-DETR，在实时检测上超过了YOLOs，这个架构是简单地ViTencoder、projector、和一个浅的DETRdecoder的堆叠。这个方法利用了最近的技术包括training-effectivetechniques：improved loss和预训练，interleaved window 和globalattention用来减小ViT encoder的复杂度。通过汇聚多个level的featuremaps、intermediate 和final feature mapss来提高ViTencoder，形成更丰富的特征图，引入window-major featuremap，来提高interleavedattention计算的效率。结果展示提出的方法超过了现有的检测器，包括YOLO和它的变体。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>实时目标检测有很广的真实的应用。当前的解决方案是基于卷积网络的，例如YOLO系列。最近，Transformer的方法，例如DETR，有很多的进步，然而，DETR的实时的检测还没有fullyexplored。还不清楚它的性能和SOTA的卷积相比怎么样。本文提出了light-weightDETR用于目标检测，架构很简单，一个plain ViT encoder，一个DETR的decoderconnected by 一个卷积projector。提出了汇聚mulit-level featuremaps，中间层和最后的feature maps in the encoder，形成了更强的encodedfeature maps。这个方法利用了高效的训练techniques。例如：用了deformablecross-attention 来构成encoder。IoU-aware classification loss，encoder-decoder pretraining策略。另一方面，这个方法利用了inference-efficienttechniques。例如：采用了interleaved window和global attentions。用windowattention代替了global attentions in the plain ViTencoder来减小复杂度。通过一个window-major feature maporganization方法，来做一个高效的implementation for the interleavedattentions，能够有效地减小存储的permutation操作。</p></li><li><p>LW-DETR是由一个ViT encoder, 一个projector和一个DETRdecoder构成。</p><ul><li><strong>Encoder</strong>：采用ViT 用作detection encoder，一个plainViT 包括一个patchification layer和transformer encoderlayers。由于全局的attention成本很高，在一些encoder layers中用 windowself-attetion来减小计算复杂度。提出了汇聚多个level的featuremaps，中间层和最后的feature maps in the encoder，形成更强的encodedfeature maps。</li><li><strong>Decoder</strong>: decoder多个transformer decoderlayers的堆叠。每个layer包括一个自注意力，交叉注意力和一个FFN。采用deformablecross-attention用于高效计算。DETR及其变体通常采用6个decoderlayers。在我们的实施中，用了3个transformer decoderlayers。这导致时间的减小。<strong>采用一个mixed-query selectionscheme来形成object queries 作为额外的content queries和spatialqueries</strong>。这个content queries是learnableembeddings，类似于DETR。这个spatial queries是基于两阶段的方案：选择top-Kfeatures from last layer in the projector,然后预测boundingboxes，将对应的boxes变成embeddings 作为spatial queries。</li><li><strong>Projector</strong>：用一个projector来连接encoder和decoder。这个projector将来自encoder的aggregatedencoded feature maps作为输入，Projector是一个C2f block(an extention ofcross-stage partial DenseNet)，用在了YOLOv8中。当构成LW-DETR的large和xlarge版本的时候，修改projector，来输出两个尺寸的featuremaps，然后用multi-scale decoder。这个projector包含两个并行的C2fblocks。一个处理 <span class="math inline">\(\frac{1}{8}\)</span>的featuremaps(它是通过一个deconvolution对输入进行上采样得到)。然后通过一个strideconv对输入进行下采样得到 <spanclass="math inline">\(\frac{1}{32}\)</span> 的featuremap，用另一个block对其进行处理。</li><li><strong>目标函数</strong>：采用IoU-aware classification loss，IA-BCEloss, 这里 <span class="math inline">\(N_{pos}\)</span> 和 <spanclass="math inline">\(N_{neg}\)</span> 是正样本和负样本的数量。 <spanclass="math inline">\(s\)</span>是预测的分类的score， <spanclass="math inline">\(t\)</span>是目标score absorbing the IoU score<span class="math inline">\(u\)</span>(with the ground truth), <spanclass="math inline">\(t = s^{\alpha}u^{1-\alpha}\)</span>，<spanclass="math inline">\(\alpha\)</span>设为0.25。整个的loss是分类的loss和bboxloss的结合。 bbox loss和DETR框架中的一样。</li></ul><p><spanclass="math display">\[\ell_{\text{cls}}=\sum_{i=1}^{N_{pos}}\mathrm{BCE}(s_i,t_i)+\sum_{j=1}^{N_{neg}}s_j^2\mathrm{BCE}(s_j,0),\]</span></p><p><span class="math display">\[\ell_{\mathrm{c l s}}+\lambda_{\mathrm{i o u}} \ell_{\mathrm{i ou}}+\lambda_{\ell_{1}} \ell_{1}.\]</span></p><p><span class="math inline">\(\lambda_{iou}\)</span> 和 <spanclass="math inline">\(\lambda_{l1}\)</span> 分别设为 2.0和5.0。<spanclass="math inline">\(l_{iou}\)</span> 和 <spanclass="math inline">\(l_1\)</span> 是generalized IoU(GIoU) loss。L1 lossfor the box regression。</p></li><li><p>高效训练：</p><ul><li><strong>Moresupervision</strong>：多个techniques用于引入更多的supervision，用来加速DETR的训练，采用GroupDETR，很容易执行，不改变推理的过程。用了13个并行的weight-sharingdecoders for training。对于每个decoder，产生object queries for eachgroup from the output features of the projector。用primary decoder forthe inference。</li><li><strong>Pretraining onObject365</strong>：预训练的过程包含两个阶段，在object365数据集上用MIM的方法进行预训练。另外，跟着之气的方法，retrainencoder，用监督的方式在object365数据集上训练projector和decoder。</li></ul></li><li><p>高效推理：</p><ul><li>在之前的方法上进行了简单第修改，采用了interleaved window和globalattentions。用window attention代替一些globalattention。例如在一个6-layer的ViT中，1，3，5用windowattentions，<strong>window attention</strong>是通过将featuremap划分成不重叠的windows，然后在每个windows内进行注意力操作。</li><li><strong>采用一个window-major feature maporganization用于高效的interleaved attention</strong>。能够将featuremaps通过window by window的方式组织起来。ViTDet是将feature maps以row byrow的方式组织的，需要很高的permutation operations，来将feature maps从row-major transition到 window-major for windowattention。作者的方法去掉了这些operations，减小了model latency。window-major的解释如图：</li></ul><p><span class="math display">\[\begin{bmatrix}f_{11} f_{12} f_{13}f_{14}\\f_{21} f_{22} f_{23} f_{24}\\f_{31} f_{32} f_{33} f_{34}\\f_{41}f_{42} f_{43} f_{44}\end{bmatrix},\]</span></p><p>window-major organization for a window size <spanclass="math inline">\(2 \times 2\)</span>： <spanclass="math display">\[f_{11},f_{12},f_{21},f_{22};f_{13},f_{14},f_{23},f_{24};\\f_{31},f_{32},f_{41},f_{42};f_{33},f_{34},f_{43},f_{44}.\]</span></p><p>这个organization对于window attention和globalattention都是适用的，不需要rearranging the features。row-majororganization是这样的：</p><p><spanclass="math display">\[f_{11},f_{12},f_{13},f_{14};f_{21},f_{22},f_{23},f_{24};\\f_{31},f_{32},f_{33},f_{34};f_{41},f_{42},f_{43},f_{44},\]</span></p><p>用于global attention是可以的，需要很多的permutation操作，来执行windowattention。大的提升来自于在object365上的预训练，表明<strong>Transformer</strong>得益于大的数据。</p></li></ol><p><imgsrc="https://figures.semanticscholar.org/15acdab8b50e8d1c835a4a1ff6a23075332912eb/4-Figure2-1.png"alt="encoder" /> <span class="math inline">\(Fig.1^{[1]}\)</span></p><p><imgsrc="https://figures.semanticscholar.org/15acdab8b50e8d1c835a4a1ff6a23075332912eb/5-Figure3-1.png"alt="projector" /> <span class="math inline">\(Fig.2^{[2]}\)</span>Single-scale projector and multi-scale projector for (a) the tiny,small, and medium models, and (b) the large and xlarge models.</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;lw-detr-a-transformer-replacement-to-yolo-for-real-time-detection1&quot;&gt;LW-DETR:
A Transformer Replacement to YOLO for Real-Time
Detection&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自百度、阿德莱德大学、北航、自动化所和澳洲国立大学的Qiang
Chen,Xiangbo Su, Xinyu Zhang等人。论文引用[1]:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Real-Time Detection With Transformer&lt;/li&gt;
&lt;li&gt;interleaved window and global attention&lt;/li&gt;
&lt;li&gt;window-major order feature map organization&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2024.Jun&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;作者提出了一个light-weight transformer,
LW-DETR，在实时检测上超过了YOLOs，这个架构是简单地ViT
encoder、projector、和一个浅的DETR
decoder的堆叠。这个方法利用了最近的技术包括training-effective
techniques：improved loss和预训练，interleaved window 和global
attention用来减小ViT encoder的复杂度。通过汇聚多个level的feature
maps、intermediate 和final feature mapss来提高ViT
encoder，形成更丰富的特征图，引入window-major feature
map，来提高interleaved
attention计算的效率。结果展示提出的方法超过了现有的检测器，包括YOLO和它的变体。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Object Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>internLM 学习</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/09/07/internLM-%E5%AD%A6%E4%B9%A0/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/09/07/internLM-%E5%AD%A6%E4%B9%A0/</id>
    <published>2024-09-07T07:33:07.000Z</published>
    <updated>2024-09-20T08:45:16.338Z</updated>
    
    <content type="html"><![CDATA[<h3id="记录一下参加internlm活动的学习过程">记录一下参加internLM活动的学习过程</h3><ol type="1"><li>InternLM的链接为<code>https://github.com/InternLM/Tutorial</code>,</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3
id=&quot;记录一下参加internlm活动的学习过程&quot;&gt;记录一下参加internLM活动的学习过程&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;InternLM的链接为
&lt;code&gt;https://github.com/InternLM/Tutorial&lt;/cod</summary>
      
    
    
    
    <category term="Learning" scheme="https://young-eng.github.io/YoungBlogs/categories/Learning/"/>
    
    
    <category term="LLM" scheme="https://young-eng.github.io/YoungBlogs/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Docker配置及使用</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/09/04/Docker%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/09/04/Docker%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8/</id>
    <published>2024-09-04T06:22:14.000Z</published>
    <updated>2024-09-04T06:26:40.865Z</updated>
    
    <content type="html"><![CDATA[<h3 id="docker的配置及使用">Docker的配置及使用</h3><ol type="1"><li><p>windows和linux安装docker的方式有点不一样，但也不复杂，主要的地方在于需要弄一个registry_mirror，虽然不知道还有没有效，当然，能科学上网的话就方便很多了。</p></li><li></li></ol><span id="more"></span><h3 id="参考链接">参考链接：</h3><ul><li><p><code>https://www.ruanyifeng.com/blog/2018/02/docker-tutorial.html</code>，阮一峰老师的博客</p></li><li><p><code>https://docker-practice.github.io/zh-cn/image/pull.html</code>,DockerPractice</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;docker的配置及使用&quot;&gt;Docker的配置及使用&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;windows和linux安装docker的方式有点不一样，但也不复杂，主要的地方在于需要弄一个registry_mirror，虽然不知道还有没有效，当然，能科学上网的话就方便很多了。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Tools" scheme="https://young-eng.github.io/YoungBlogs/categories/Tools/"/>
    
    
    <category term="Docker" scheme="https://young-eng.github.io/YoungBlogs/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>SLAM 学习记录</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/09/03/SLAM-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/09/03/SLAM-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</id>
    <published>2024-09-03T08:25:10.000Z</published>
    <updated>2024-09-03T08:41:57.170Z</updated>
    
    <content type="html"><![CDATA[<h3 id="slam介绍">SLAM介绍</h3><ol type="1"><li>SLAM: <strong>S</strong>imultaneous <strong>L</strong>ocalization<strong>a</strong>nd<strong>M</strong>apping，翻译为“即时定位与建图”，是指<strong>搭载特定传感器的主体，在没有环境先验信息的情况下，于运动过程中建立环境的模型，同时估计自己的运动，如果这里的传感器主要为相机，那就称为“视觉SLAM”</strong>。</li></ol><h3 id="visual-slam">Visual SLAM</h3><ol type="1"><li>经典视觉SLAM的框架主要有几个步骤：<ul><li><strong>传感器信息读取</strong>：在视觉SLAM中主要为相机图像信息的读取和预处理；如果在机器人中，可能还有码盘、IMU等传感器信息的读取和同步。</li><li><strong>前端视觉里程计(Visual Odometry,VO)</strong>：视觉里程计的任务是估算相邻图像间相机的运动，以及局部地图的样子，VO又称为前端(FrontEnd)。</li><li><strong>后端(非线性)优化(Optimization)</strong>：后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行优化，得到全局一致的轨迹和地图，由于接在VO之后，又称为后端(BackEnd)。</li><li><strong>回环检测(Loop ClosureDetection)</strong>：回环检测判断机器人是否到达过先前的位置，如果检测到回环，它会把信息提供给后端进行处理。</li><li><strong>建图(Mapping)</strong>。它是根据估计的轨迹，建立与任务要求对应的地图。</li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;slam介绍&quot;&gt;SLAM介绍&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;SLAM: &lt;strong&gt;S&lt;/strong&gt;imultaneous &lt;strong&gt;L&lt;/strong&gt;ocalization
&lt;strong&gt;a&lt;/strong&gt;nd
&lt;st</summary>
      
    
    
    
    <category term="Learning" scheme="https://young-eng.github.io/YoungBlogs/categories/Learning/"/>
    
    
    <category term="SLAM" scheme="https://young-eng.github.io/YoungBlogs/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>WOO</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/27/WOO/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/27/WOO/</id>
    <published>2024-08-27T07:23:26.000Z</published>
    <updated>2024-08-29T05:58:17.880Z</updated>
    
    <content type="html"><![CDATA[<h3id="watch-only-oncean-end-to-end-video-action-detection-framework1">WatchOnly Once：An End-to-end Video Action DetectionFramework<sup>[1]</sup></h3><blockquote><p>作者是来自港大的罗平老师组的Shoufa Chen、Peize Sun、EnzeXie等人。论文引用[1]:Chen, Shoufa et al. “Watch Only Once: An End-to-EndVideo Action Detection Framework.” 2021 IEEE/CVF InternationalConference on Computer Vision (ICCV) (2021): 8158-8167.</p></blockquote><h3 id="time">Time</h3><ul><li>2021.Oct</li></ul><h3 id="key-words">Key Words</h3><ul><li>end-to-end unified network</li><li>task-specific features</li></ul><h3 id="总结">总结</h3><ol type="1"><li>提出了一个端到端的pipeline for video actiondetection。<strong>当前的方法要么是将video action detection这个任务解耦成action localization和actionclassification这两个分离的阶段，要么在一个阶段里训练两个separatedmodels</strong>。相比之下，作者的方法将actor localization和actionclassification弄在了一个网络里。通过统一backbone网络，去掉很多认为的手工components，整个pipeline被简化了。<strong>WOO</strong>用一个unifiedvideo backbone来提取features for actor location 和actionlocalization,另外，引入了<font color=red>spatial-temporal actionembeddings</font>，设计了一个 spatial-temporal fusionmodule来得到更多的含有丰富信息的discriminative features，提升了actionclassification的性能。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>Video action detection 包含 <strong>actor bboxlocalization</strong>和 <strong>action typeclassification</strong>，当前方法的复杂性来自于actorlocalization和action classification之间的基本的困境。thatis，*用一个single key frame is "positive" for actor localization，但是"negative" for actionclassification，然而用多个frames有相反的影响。这是因为actionlocalization需要一个2D的检测模型同预测video clip的key frame上的actorbbox。在这个阶段，考虑clip中的相邻帧会带来额外的计算和存储成本，相比之下，actionclassification严重依赖于一个 3D video model来提取videosequence中的时序的信息，单帧图像有很少的temporal motion representationfor action classification。</p><p>之前提出了两种可能的替代方法来解决这个困境。第一个是用离线的persondetector，用来产生actor proposals，不是和actionclassification一起训练的，然后一个独立的video model用这些actorproposals和raw frames作为输入来预测action classes。单独的一个persondetector是已经足够复杂了，which is pre-trained on the ImageNet and COCOhuman keypoint detection，and further fine-tuned on the target actiondetection dataset。这个方法比较复杂，计算成本高，需要两个separatemodels和两个训练阶段。更进一步，<strong>separate optimization on twosub-problems leads to a sub-optimal solution</strong>。</p><p>第二种类型的方法，是将actor detection和action classificationmodels一起在单个trainingstage中进行联合训练。虽然训练的pipeline在某种程度上是简化了，两个模型仍然需要独立的从rawimages中提取特征。因此，整个框架仍然有很高的computation和memorycost。</p><p><strong>一个很自然的问题是：有没有可能设计一个简单的unified的网络来解决actorlocalization和action classification in a single end-to-endmodel</strong>。</p><p>本文提出了 <strong>Watch OnlyOnce(WOO)</strong>的框架，WOO直接预测actors的bbox和action classes from avideo clip。"watch" the clip onlyonce，就能够预测actor的位置和action的类别。方法主要包含3个keycomponents：<font color=red>unified backbone、a spatial-temporal actionembedding、a spatial-temporal knowledge fusion mechanism</font>。</p><p>首先，设计了一个简单的有效的module，使得单个backbone能偶提供task-specificfeature maps for actor localization head 和action classificationhead。这个module是轻量的，能够用来isolate keyframe features from allframes in the early stage of thebackbone。这个动机是，当model走深的时候，keyframe能够得到moreinteraction with neighboringframes，提出的module能够很容易地插入到现有的backbone中，例如I3D,X3D等。</p><p><strong>另外，注意到，同一个的架构tends to behave well for actorlocalization，但是对于action classification是有限的。这个困难是actiondetection主要lies in action classification。因此，怀疑单个backbone forboth tasks是否会bias towards localization，然后找到一个undesiredsolution，因此对actionclassification的性能造成了影响。基于这个观察，提出了spatial和tempoalaction embedding和interaction mechanism between them，来使得actionclassification features在spatial 和temporal perspectives更加的discriminative。</strong></p><p>第三，提出了一个spatial-temporal fusion module来汇聚spatial和temporalknowledge，这个spatial properties例如shape和pose，temporalproperties例如dynamic motion和temporal scale ofaction,结合在一起，通过spatial-temporal fusion module，来生成actionfeatures for actio classification。</p><p>主要贡献如下：</p><ol type="1"><li><p>提出了一个端到端的框架for video action detection，给定一个videoclip作为输入，能够直接产生bboxes 和action classes，不需要独立的persondetector(这个是在之前的工作中必不可少的).</p></li><li><p>提出了一个spatial-temporal embedding，一个embedding interactionmechanism，能够提高features的discriminativeness for actionclassificatin。一个spatial-temporal fusionmodule来进一步从spatial和temporal来汇聚features。</p></li></ol></li><li><p>相关工作：</p><ul><li><p><font color=red>two-stage, twobackbone</font>：当前的STAD的SOTA模型通常用了两阶段的pipeline，用了两个backbone。这些方法简单地把STAD任务划分成actorlocalization和action classification。具体地说，在第一阶段，在COCOkeypoint上预训练一个model，然后再目标STADdataset上进行微调；第二阶段，用video clip的keyframe作为第一阶段中得到的detection model的输入，来预测actorbboxes。然后将video clip和actor bboxes作为3Dbackbone的输入，来提取RoI区域的特征 for action classprediction。<strong>这些方法有很高的复杂度和很低的效率，因为sequentialtraining stage和separatemodel架构。另外，在两个独立的阶段的独立的优化可能导致一个sub-optimal的结果</strong>。</p></li><li><p><font color=red>One-stage,two-backbone</font>：<strong>YOWO和ACRN通过同时训练2D的actor检测网络和3D video model，来简化网络。然而，这里仍然有两个separatemodels来优化</strong>。<strong>以YOWO为例，它包含一个在Kinetics上预训练的一个3D model和 YOLO pre-trained on PASCAL VOC上的 2Dmodel，有很高的计算量和memoryburden</strong>。是虽然这个pipeline在一定程度上简化了。</p></li></ul><p>相比于这些方法，WOOrefreshing的简单：给定一个videoclip，直接预测actor bboxes和对应的action classes。</p><ul><li><p><font color=red>End-to-end objectdetection</font>：最近的端到端的目标检测框架，不需要任何手工设计的过程例如NMS，直接输出预测，实现了很好的性能。在这些工作中，DETR可以被视为端到端的目标检测方法，它采用了globalattention mechanism和双向matching between predictions和ground truthobjects。DETR抛弃了NMS步骤，实现了很好的性能。它在小目标上性能不太好，相比主流的检测器需要更长的训练时间。为了解决上述的问题，提出了<strong>Deformable-DETR</strong>，来将每个objectquery 限制在 a small set of crucial sampling points around referencepoints，而不是all points in the featuremap。Deformable-DETR是高效和快速收敛的；Sparse RCNN利用一组稀疏的learnedobject proposals，以iterative way来进行分类和定位。SparseRCNN和well-established的检测器相比，展示了精度、实时和训练收敛。在本次工作中，主要采用<strong>SparseRCNN的检测头来做定位</strong>。</p></li><li><p><font color=red>Attention mechanism for actionrecognition</font>：对于language相关的任务，注意力机制是一个流行的概念。对于actiorecognition，Non-local网络利用自注意力来得到不同时间或者空间features的<strong>dependencies</strong>。使得注意力机制applicablefor action classification，有人利用non-local block作为一个long-termfeature bank operator，使得video models能得到long-terminformation，提高了action detection的性能。</p></li></ul></li><li><p>Methods: <spanclass="math inline">\(X\quad\in\quad\mathbb{R}^{C\times T\times H\timesW}\)</span> 是一个layer的输入的spatial-temporal featuremap。跟随前人的工作，in this work，将key frame放在videoclip的中间，<span class="math inline">\(X_{t=\lfloorT/2\rfloor}\in\quad\mathbb{R}^{C\times H\timesW}\)</span>表示<strong>keyframe feature map</strong>。</p><ul><li><strong>union backbone</strong>：在之前的video backbone中，key framefeatures将会和相邻的frame features通过temporalpooling或者3D卷积(temporal kernel size 大于1，会给keyframe特征带来意想不到的disturbance)进行interact。为了克服这个问题，video backbone设计的时候，将keyframe和temporal interaction之前的早期的网络的features隔离开。</li></ul><p>和之前的backbon Slowfast(将spatial stride of res5设为1，用a dilationof 2 for its filters，来增加spatial resolution of res5 by <spanclass="math inline">\(2 \times\)</span>)不同，作者去掉了res5中的dialtedconv，采用FPN module来做<strong>keyframe</strong>特征提取。FPNmodule用res2,res3,res4,res5的输出的<strong>keyframefeature</strong>作为输入。进一步用FPN的输出的特征 for actorlocalization,res5输出的特征来做actionclassification。为了这个目的，一个统一的actionbackbone用来提供task-relevant features。</p><p>以上的设计有几个好处，首先，actor localizationhead采用层次化的feature representation作为sourcefeatures。对于目标检测是有好处的。第二，用于actor localization的keyframefeatures通过FPN结构，和所有的video frames的features隔离开，starting atthe early stage of thebackbone。这能减少邻近帧的干扰，因为keyframe会随着model的深入，和邻近帧有更多的interaction。第三，相比于存在的两个backbone的网络(用独立的backbone)for actor localization，作者仅用一个轻量的FPN module that tasks imagefeatures as input，减少了参数和FLOPS。</p><ul><li><p><strong>Acotr Localization Head</strong>：受最近的SparseRCNN的启发，设计了一个端到端的actor detection head for actorlocalization,detection head在得到hierarchical features fromFPN之后，能够预测bbox和对应的scores indicating model's confidence on thebox containing an actor。另外，person detector利用 set prediction lossfor optimal bipartite matching between prediciton和ground truth attrainingstage，在验证的阶段不需要post-process。不同于two-backbone的方法，不许哟啊额外的预训练，因为persondetector和action classifier共享一个backbone。</p></li><li><p><strong>Action Classification Head</strong>：给定有persondetector生成的 <span class="math inline">\(N\)</span>个actor proposalboxes。用RoIAlign来提取每个box的spatial和temporalfeatures，这两种类型的features然后融合，得到最终的action classprediction。细节如下：</p><ol type="1"><li><p><strong>Spatial Action Features</strong>：<spanclass="math inline">\(X_{5} \in \mathbb{R}^{C\times T\times H\timesW}\)</span>表示 res5得到的feature，在时间维度上进行一个<strong>globalaverage pooling</strong>，得到一个spatial feature map，<spanclass="math inline">\(f^{s} \in \mathbb{R}^{C\times1\times H\timesW}\)</span>，在 <span class="math inline">\(f^s\)</span> 上用RoIAlignwith <span class="math inline">\(N\)</span> 个actor proposals，得到<span class="math inline">\(N\)</span>个 spatial RoI features。<spanclass="math inline">\(f_{1}^{s},f_{2}^{s},\cdots,f_{N}^{s} \in\mathbb{R}^{C\times S\times S},\)</span>， <span class="math inline">\(S\times S\)</span>是 RoIAlign输出的spatial output size。</p></li><li><p><strong>Temporal Action Features</strong>：除了spatial actionfeatures，temporal properties也很重要，为了得到temporal motioninformation，从feature volume <spanclass="math inline">\(X_5\)</span>中的every frame提取temporalfeatures。因为这里主要关注temporal information，在spatialdimension上用一个global average pooling，来提取temporal RoIfeatures。temporal action feature表示为<spanclass="math inline">\(f_{1}^{t},f_{2}^{t},\cdots,f_{N}^{t}\in\mathbb{R}^{C\times T\times1\times1}\)</span>。</p></li><li><p><strong>Embedding Interaction</strong>：为了得到discriminativefeatures，为了增强instance的特性，引入了spatial 和temporal embedding tobe convolved with aforementioned spatial and temporal features。spatialembedding期望能够encoder spatial properties例如shape,pose等。temporalembedding能够encode temporal dynamicproperties，例如dynamics和action的temporalscale。注意到embedding是对于每个 <span class="math inline">\(N\)</span>features是exclusive的。定义 <span class="math inline">\(E^{s}\in\mathbb{R}^{N\times d},E^{t}\in \mathbb{R}^{N\times d}\)</span> forspatial and temporal embedding。<span class="math inline">\(E_n^s \in\mathbb{R}^d,E_n^t \in \mathbb{R}^d\)</span> are working for n-th RoIfeature。为了获得不同actors之间的relation 信息，构建了一个attentionmodule for all RoI features。因为每个actor RoI有自己的spatial和temporalembedding，embedding相比于featuremap更lighter，在对不同给的embedding之间而不是featuremaps之间采用attention mechanism for efficiency。这里给定一个queryelement和一系列key elements，多头注意力module能够根据attentionweights(measure compatibility of query-key pairs adaptively)来汇聚keycontents。最后，<span class="math inline">\(x = (x_,...,x_n)\)</span>表示 <span class="math inline">\(n\)</span>个input elements，输出 <spanclass="math inline">\(z= (z_1,...,z_n)\)</span>，<spanclass="math inline">\(z_i\)</span>是weighted sum of a linearlytransformed input：</p></li></ol><p><spanclass="math display">\[z_i=\sum_{j=1}^n\alpha_{ij}(x_jW^V).\]</span></p><p>weight coefficient <spanclass="math inline">\(\alpha_{ij}\)</span>是通过softmax计算出来的：<span class="math display">\[\alpha_{ij}=\frac{\expz_{ij}}{\sum_{k=1}^n\exp z_{ik}}, \text{where}z_ij=\frac{(x_iW^Q)(x_jW^K)^T}{\sqrt{d_z}}.\]</span> 将 <spanclass="math inline">\(E^s\)</span>和 <spanclass="math inline">\(E^t\)</span>送到 self-attentionmodule，得到对应的输出 <spanclass="math inline">\(\phi^s，\phi^t\)</span>，和 原始的embeddings <spanclass="math inline">\(E^s, E^t\)</span>有相同的shape，最后的actionfeature是： <spanclass="math display">\[f=\mathcal{G}(\mathcal{F}(f^s,\phi^s),\mathcal{F}(f^t,\phi^t)),\]</span></p><p><span class="math inline">\(F\)</span>是一个卷积操作 with parameters<span class="math inline">\(\phi\)</span>，<spanclass="math inline">\(G\)</span>是spatio-temporal fusion操作。初始化<span class="math inline">\(F\)</span> with <spanclass="math inline">\(1 \times 1\)</span> kernels for efficiency。</p><ol start="4" type="1"><li><strong>ObjectiveFunction</strong>：提出的model以端到端的方式解决了定位和分类，整个的目标函数由对应的两部分组成：<spanclass="math display">\[\mathcal{L}=\underbrace{\lambda_{cls}\cdot\mathcal{L}_{cls}+\lambda_{L1}\cdot\mathcal{L}_{L1}+\lambda_{giou}\cdot\mathcal{L}_{giou}}_{\text{setpredictionloss}}+\underbrace{\lambda_{act}\cdot\mathcal{L}_{act}}_{\text{action}}.\]</span></li></ol><p>第一部分是 <em>set prediciton loss</em>，produces an optimal<strong>bipartite matching between predictions and ground truthobjects</strong>。用 <span class="math inline">\(L_{cls}\)</span>表示cross-entropy loss over two classes(containing actor vs notcontaining actor)。<span class="math inline">\(L_{L1}\)</span> 和 <spanclass="math inline">\(L_{giou}\)</span>是box loss。<spanclass="math inline">\(\lambda_{cls},\lambda_{L1},\lambda_{giou}\)</span>是常量，平衡这些loss的contributions。对于第二部分， <spanclass="math inline">\(L_{act}\)</span>是一个binary cross entropy lossused for action classification，<spanclass="math inline">\(\lambda_{act}\)</span> 是对应的weight。</p></li></ul></li><li><p>实验：</p><ul><li>Spatial-temporal fusion：有不同的instantiations of fusing temporal和spatial action features：summation、concatenation和cross-attention(CA)，结果表明CA效果比另外两个好。</li></ul></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/1-Figure1-1.png"alt="Motivation of WOO" /><figcaption aria-hidden="true">Motivation of WOO</figcaption></figure><p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Motivation ofWOO. (a) Previous dominant video action detection methods usually adopttwo separate networks: an independent 2D detection model for actorlocalization from every key frames, and a 3D video model for actionclassification from video clips. (b) Our end-to-end unified frameworkuses a single backbone network to handle both 2D image detection and 3Dvideo classification (i.e.2D spatial dimensions plus a temporaldimension). This unified backbone only “watches” an input video once,and directly produces both actor localization and actionclassification</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/3-Figure2-1.png"alt="Comparison of Backbone" /><figcaption aria-hidden="true">Comparison of Backbone</figcaption></figure><p><span class="math inline">\(Figure \ 2^{[1]}\)</span>: Comparisons ofbackbone architecture. (a) Two separate backbones for actor localizationand action classification. Video backbone adopts res5 stage with dilatedconvolution (DC5). (b) A single union backbone which can providetask-specific features for actor localization and action classificationsimultaneously, enabling nearly cost-free feature extraction for actorlocalization compared to (a). Key frame features are illustrated inlight orange color. Here we purposely omit the res2 features for visualsimplicity</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/4-Figure3-1.png"alt="Action classification head" /><figcaption aria-hidden="true">Action classification head</figcaption></figure><p><span class="math inline">\(Figure \ 3^{[1]}\)</span>: Actionclassification head. Given the RoI feature of a specific box for Tframes, spatial and temporal action features are generated. Then,spatial and temporal embedding is used to make action featurerepresentation more discriminative through the interaction module.Finally, the multi-layer perceptron (MLP) takes as input the fusedspatial-temporal feature and predicts the action class logits. See textfor details</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/5-Figure4-1.png"alt="Structure of interaction module" /><figcaption aria-hidden="true">Structure of interactionmodule</figcaption></figure><p><span class="math inline">\(Figure \ 4^{[1]}\)</span>: Structure ofinteraction module. Here we plot spatial embedding interaction as anexample. ‘⊗’ denotes matrix multiplication, and ‘⊛’ denotes 1 × 1convolution</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;watch-only-oncean-end-to-end-video-action-detection-framework1&quot;&gt;Watch
Only Once：An End-to-end Video Action Detection
Framework&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自港大的罗平老师组的Shoufa Chen、Peize Sun、Enze
Xie等人。论文引用[1]:Chen, Shoufa et al. “Watch Only Once: An End-to-End
Video Action Detection Framework.” 2021 IEEE/CVF International
Conference on Computer Vision (ICCV) (2021): 8158-8167.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2021.Oct&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;end-to-end unified network&lt;/li&gt;
&lt;li&gt;task-specific features&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;提出了一个端到端的pipeline for video action
detection。&lt;strong&gt;当前的方法要么是将video action detection
这个任务解耦成action localization和action
classification这两个分离的阶段，要么在一个阶段里训练两个separated
models&lt;/strong&gt;。相比之下，作者的方法将actor localization和action
classification弄在了一个网络里。通过统一backbone网络，去掉很多认为的手工components，整个pipeline被简化了。&lt;strong&gt;WOO&lt;/strong&gt;用一个unified
video backbone来提取features for actor location 和action
localization,另外，引入了&lt;font color=red&gt;spatial-temporal action
embeddings&lt;/font&gt;，设计了一个 spatial-temporal fusion
module来得到更多的含有丰富信息的discriminative features，提升了action
classification的性能。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>YOWOv3</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/26/YOWOv3/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/26/YOWOv3/</id>
    <published>2024-08-26T06:35:34.000Z</published>
    <updated>2024-09-02T02:06:53.166Z</updated>
    
    <content type="html"><![CDATA[<h3id="yowov3-an-efficient-and-generalized-framework-for-human-action-detection-and-recognition1">YOWOv3:An Efficient and Generalized Framework for Human Action Detection andRecognition<sup>[1]</sup></h3><blockquote><p>作者是Nguyen Dang Duc Manh, Duong Viet Hang等人。论文引用[1]:Dang,Duc M et al. “YOWOv3: An Efficient and Generalized Framework for HumanAction Detection and Recognition.” (2024).</p></blockquote><h3 id="time">Time</h3><ul><li>2024.Aug</li></ul><h3 id="key-words">Key Words</h3><ul><li>one-stage detector</li><li>different configurations to customie different model components</li><li>efficient while reducing computational resource requirements</li></ul><h3 id="总结">总结</h3><ol type="1"><li>YOWOv3是YOWOv2的增强版，提供了更多的approach,用了不同的configurations来定制不同的model，YOWOv3比YOWOv2更好。</li><li>STAD是计算机视觉中一个常见的任务，涉及到检测<strong>location(bbox),timing(exact frame),and type(class of action)</strong>，需要对时间和空间特征进行建模。有很多的方法来解决STAD的问题，例如ViT，ViT的效果很好，但是计算量比较大。例如Hieramodel由超过600M的参数，VideoMAEv2由超过1B的参数，增加了训练的成本和消耗。为了解决STAD问题，同时最大程度减弱训练和推理时间的成本，有人提出用了YOWO方法，虽然可以做到实时，但是也有限制：不是一个efficientmodel with low computationalrequirements。框架的作者已经停止维护了，但是还有很多的问题。本文的contribution如下：<ul><li>new lightweight framework for STAD</li><li>efficient model</li><li>multiple pretrained resources for application：creating a range ofpretrained resources spanning from lightweight to sophisticated modelsto cater to diverse requirements for real-world applications。</li></ul></li></ol><span id="more"></span><ol start="3" type="1"><li><p>YOWO的架构过时了，缺乏sophistication和davancements seen incontemporarymodels，限制了它的applicability和性能。YOWOv2基于YOWO，用了anchor-freeobjectdetection和FPN，提高了性能。但是增加了GLOP，和高效轻量的model的目标不相符。</p></li><li><p><strong>Framework</strong>：YOWOv3采用了<strong>two-stream</strong>网络，包括连个processingstreams，第一个用来提取 <strong>spatial information and context from theimage using a 2D CNN</strong>，第二个stream，一个3DCNN，主要用来提取temporalinformation和motion。两个stream的输出结合到一起，来得到包含关于视频的spatial和temporal的信息。最后，用一个CNNlayer基于这些extracted features进行预测。</p><ul><li><strong>Spatial Feature Extractor</strong>：模型需要一个spaitalfeature extracto来提供关于location的信息。为了满足这个目的，采用了YOLOv8model，去掉了其中的detection layer，输入这个module的是size为<spanclass="math inline">\([3,H,W]\)</span>的feature map，代表final frame ofthe input video。通过利用pyramid network架构，输出包含3个不同level的feature maps：<spanclass="math inline">\(F_{lv1}:[{C}_{2D},\frac{H}{8},\frac{W}{8}],F_{lv2}:[{C_{2D}},\frac{H}{16},\frac{W}{16}]\)</span>，<spanclass="math inline">\(F_{lv3}:[C_{2D},\frac{H}{32},\frac{W}{32}].\)</span></li><li><strong>Decoupled head</strong>：decoupled head是用来separate分类和回归的任务。<strong>YOLOXmodel团队发现，在早期的模型中，用单个feature map来做分类和回归 madetraining morechallenging</strong>。因此，用了相似的approach，用两个独立的CNN streamsfor each task，来增强模型的comprehension。</li></ul><p><spanclass="math display">\[F_{cls}=Conv_{cls2}(Conv_{cls1}(x))\\F_{box}=Conv_{box2}(Conv_{box1}(x))\]</span></p><p>2D backbone的输出包含3个feature map at different levels，每个featuremap 送到Decoupled Head中来产生<strong>two feature maps forclassification and regression</strong>，DecoupledHead的输入是一个tensor，<span class="math inline">\(F_{lv} :[C_{2D},H_{lv},W_{lv}]\)</span>,输出两个相同shape的tensors：<spanclass="math inline">\(F_{lv}:[C_{inter},H_{lv},W_{lv}].\)</span></p><ul><li><p><strong>temporal motion featureextractor</strong>：为了增强预测action label的精确度，用了3DCNN模型，采用了I3D的模型，输入3D backbone的是一个tensor <spanclass="math inline">\(F_{3D}：[3, D,H,W]\)</span>，是整个视频，输出是一个tensor：<spanclass="math inline">\(F_{3D}:[C_{3D},1,\frac{H}{32},\frac{W}{32}]\)</span>。</p></li><li><p><strong>Fusion Head</strong>：Fusion head用于整合2D CNN和3D CNNstreams的特征。输入这个layer包含两个tensors：<spanclass="math inline">\(F_{lv}:[C_{inter},H_{lv},W_{lv}]\)</span>，<spanclass="math inline">\(F_{3D} :[C_{3D},1,\frac{H}{32},\frac{W}{32}]\)</span>。首先 <spanclass="math inline">\(F_{3D}\)</span>被squeeze to shape <spanclass="math inline">\([C_{3D},\frac{H}{32},\frac{W}{32}]\)</span>，然后，upscale来matchdimension <span class="math inline">\(H_{lv}\)</span> 和 <spanclass="math inline">\(W_{lv}\)</span>。接下来，<spanclass="math inline">\(F_{3D}\)</span> 和 <spanclass="math inline">\(F_{lv}\)</span> 被concatenated，来得到 tensor<span class="math inline">\(F_{concat}\)</span>，最后， <spanclass="math inline">\(F_{concat}\)</span>输入到 CFAMmodule，这是一个attention mechanism。CFAM module的输出是一个feature map<spanclass="math inline">\(F_{final}:[C_{inter},H_{lv},W_{lv}]\)</span>。</p></li><li><p><strong>Detection Head</strong>：有人指出：拥有给<strong>Diarcdistribution</strong>来预测bboxes会使模型训练困难，因此提出了让模型学习更general的distribution，而不是简单地回归到single value。为了减小模型 dependence on selecting hyperparameters forpredefined bboxes as in previousstudies。也用了anchor-free。输入Detection Head的包含两个tensors：<spanclass="math inline">\(F_{cls}\)</span> 和 <spanclass="math inline">\(F_{box}\)</span> for classification and regressiontasks respectively。通过一系列的卷积实现最后的预测： <spanclass="math display">\[Predict_{cls}=\mathrm{conv}(Conv_{cls2}(Conv_{cls1}(F_{cls})))\\Predict_{box}=\mathrm{conv}(Conv_{box2}(Conv_{box1}(F_{box})))\]</span></p></li></ul></li><li><p><strong>Label Assignment</strong>：<strong>用了2个不同的labelassignment mechanisms来match 模型的predictions with the ground truthlabels from the data</strong>：<strong>TAL</strong> and<strong>SimOTA</strong>，SimOTA是OTA的简化版。两个机制都依赖于 <spanclass="math inline">\(d_{predict}\)</span>和 <spanclass="math inline">\(d_{truth}\)</span>之间的相似度测量函数，来performmatching between them。</p><ul><li><strong>TAL</strong>：prediction <spanclass="math inline">\(d_{pred} \in A\)</span> 和 ground truth <spanclass="math inline">\(d_{truth} \in T\)</span>之间的相似度测量函数如下：<span class="math display">\[\begin{aligned}&amp;metric&amp;&amp; =cls_err^{\alpha}box_err^{\beta} \\&amp;cls_err&amp;&amp; =BCE(cls_{pred},cls_{truth}) \\&amp;box_err&amp;&amp; =CIoU(box_{pred},box_{truth})\end{aligned}\]</span></li></ul><p>对于each <span class="math inline">\(d_{truth} \inT\)</span>，将它们和有最高的<spanclass="math inline">\(metric\)</span>的 <spanclass="math inline">\(top_k\)</span> 个 <spanclass="math inline">\(d_{pred}\)</span> 进行匹配，但每个 <spanclass="math inline">\(d_{pred}\)</span>只能和 most one <spanclass="math inline">\(d_{truth}\)</span>进行匹配，如果 <spanclass="math inline">\(d_{pred}\)</span> is in the <spanclass="math inline">\(top_k\)</span> of multiple <spanclass="math inline">\(d_{truth}\)</span>，将有着最高的 <spanclass="math inline">\(CIOU(box_{pred},boxJ_{truth})\)</span>的 <spanclass="math inline">\(d_{pred}\)</span>和<spanclass="math inline">\(d_{truth}\)</span>进行匹配。<strong>另外，只考虑receptive field的center落在 box of <spanclass="math inline">\(d_{truth}\)</span>内、且 receptive field center到box of <span class="math inline">\(d_{depth}\)</span> center的距离不超过</strong>radius<strong>的 <spanclass="math inline">\(d_{pred}\)</span></strong>。</p><p>如果 <span class="math inline">\(d_{pred}\)</span>和 <spanclass="math inline">\(d_{truth}\)</span>匹配，就认为 <spanclass="math inline">\(d_{pred}\)</span>的target是 <spanclass="math inline">\(d_{truth}\)</span>，probability of the target forcorresponding class is set to 1 if they appear,否则就是0。</p><ul><li><strong>SimOTA</strong>：和TAL类似，SimOTA利用了相似度测量，来matchingbetween <span class="math inline">\(d_{pred}\)</span>和 <spanclass="math inline">\(d_{truth}\)</span>： <spanclass="math display">\[\begin{aligned}metric&amp;=BCE(\lambdacls_{pred},cls_{truth})-\alpha\log(\lambda)\\&amp;\lambda=CIoU(box_{pred},box_{truth})\end{aligned}\]</span></li></ul><p>不同于TAL，这里只考虑将 <spanclass="math inline">\(d_{truth}\)</span> 和有<strong>smallestmetric</strong>的 <span class="math inline">\(top_k\)</span>个 <spanclass="math inline">\(d_{pred}\)</span>进行匹配。SimOTA不会去fix <spanclass="math inline">\(top_k\)</span>个 value，而是利用一个方式来估计<span class="math inline">\(top_k\)</span> for each <spanclass="math inline">\(d_{truth}\)</span>。实验表明：用动态的 <spanclass="math inline">\(top_k\)</span>会减慢训练过程，增加额外的计算开销。</p></li><li><p><strong>Loss Function</strong>：用两个loss function对应两个 labelassignment mechanisms。总的loss是由两部分： <spanclass="math display">\[\mathcal{L}=\mathcal{L}_{box}+\mathcal{L}_{cls}\]</span></p></li></ol><p>一个是bbox regression，一个是 loss for label classification，两个loss里有多个subcomponents。对于 <span class="math inline">\(d_{pred} \inN\)</span>， <span class="math inline">\(L_{box}\)</span> =0, <spanclass="math inline">\(cls_{truth}\)</span>也会完全为0.</p><ul><li><strong>TAL</strong>：Loss function为： <spanclass="math display">\[\mathcal{L}=\frac{\mathcal{L}_{box}+\mathcal{L}_{cls}}\omega\]</span></li></ul><p><spanclass="math display">\[\mathcal{L}_{box}=\delta(d_{truth})(\alphaCIoU(d_{pred},d_{truth})+\beta\mathcal{L}_{distribution})\\\mathcal{L}_{cls}=\gammaBCE(cls_{pred},cls_{truth})\]</span></p><p><span class="math inline">\(L_{distribution}\)</span>代表DistributionLoss function。</p><p><spanclass="math display">\[\begin{aligned}\delta(d_{i})&amp;=\sum_{p_{j}\incls_{i}}p_{j}\\\omega&amp;=\sum_{d_{pred}\in\mathcal{A}}\sum_{d_{i}\in\mathcal{M}(d_{pred})}\delta(d_{i})\end{aligned}\]</span></p><p><span class="math inline">\(\alpha，\belta,\gamma\)</span>是超参数，用来 scale components of the <spanclass="math inline">\(L\)</span> function。fix <spanclass="math inline">\(\alpha=7.5, \belta=1.5, \gamma= 0.5\)</span>。</p><ul><li><strong>SimOTA</strong>：loss function： <spanclass="math display">\[\mathcal{L}=\frac{\mathcal{L}_{box}+\mathcal{L}_{cls}}{|\mathcal{P}|}\]</span></li></ul><p><span class="math display">\[\mathcal{L}_{box}=\alphaCIoU(d_{pred},d_{truth})+\beta\mathcal{L}_{distribution}\\\mathcal{L}_{cls}=exp(cls_{t})|cls_{truth}-cls_{pred}|^{\nu}BCE(cls_{pred},cls_{truth})\]</span></p><p><span class="math inline">\(L_{cls}\)</span>是 generalized focal lossfunction，multiplied by a class balancing factor <spanclass="math inline">\(exp(cls_t)\)</span>。</p><p><span class="math display">\[p_i\incls_t=\begin{cases}class_ratio,&amp;p_{truth}\neq0\\1-class_ratio,&amp;p_{truth}=0\end{cases}\]</span></p><p><span class="math inline">\(class_ratio\)</span> 是class balancingfactor,classes have lower frequencies，<spanclass="math inline">\(class_{ratio}\)</span> will be higher。这里 <spanclass="math inline">\(\alpha=5.5,\belta=0.5, \gamma=0.5, \nu =0.5\)</span></p><ol start="7" type="1"><li>实验：在实验中，由于AVAv2.2是一个极度不平衡的dataset，因此，需要额外的方法来减小这个不平衡的影响，来提高overallmAP。解决这个问题的两个方法是：softLabels(qualified Loss)和inclusion ofa class balance term。<strong>对于频繁出现的common classes，<spanclass="math inline">\(class_{ratio}\)</span>会接近0.5, class balancetermexp(cls_{t})会基本不变，意味着如果预测错误的话，loss不会有很大的改变；对于很少出现的classes，它会接近0,模型没有正确预测的话，<spanclass="math inline">\(exp(cls_t)\)</span>会严重地惩罚。这会创造一个bias，能够帮助提高 less commonclasses的预测。</strong>另外，softlabels用在了减小模型overconfidence的影响，特别是出现的很频繁的classes**。<ul><li>对于label assignment one to many, 一个ground truth box会和多个multiple predicted boxes匹配，选择matching的boxes的数量可以被动态的估计或者predetermined。实验表明，选择<spanclass="math inline">\(k\)</span>个能够产生很好的结果，自动估计 <spanclass="math inline">\(top_k\)</span>导致额外增加 <spanclass="math inline">\(6%\)</span>的训练时间。这些结果倾向于将 <spanclass="math inline">\(top_k\)</span>视为一个超参数，而不是用来做优化的方法。</li><li>保留了<strong>Exponential MovingAverage(EMA)</strong>的模型的变体，来评估EMA的影响。结果展示了EMA在初始的epochs中，对模型的性能有很大的影响，在后面的epochs中有很小的影响，表明EMA帮助模型在早期的阶段快速收敛，在之后的训练过程，能提高mAPscore。</li></ul></li><li>看了YOWOv2和YOWOv3的图之后，看上去貌似是一样的，画的不一样，不知道是不是就换了个backbone，具体还是看看代码</li></ol><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/d02a8d30424422b99990cc394ca7d6526efc0a20/3-Figure2-1.png"alt="Structure" /> <span class="math inline">\(figure1^{[1]}\)</span>：overview architecture of YOWOv3</p><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/d02a8d30424422b99990cc394ca7d6526efc0a20/4-Figure3-1.png"alt="CFAM" /> <span class="math inline">\(figure2^{[2]}\)</span>：Overview of CFAM module</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;yowov3-an-efficient-and-generalized-framework-for-human-action-detection-and-recognition1&quot;&gt;YOWOv3:
An Efficient and Generalized Framework for Human Action Detection and
Recognition&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是Nguyen Dang Duc Manh, Duong Viet Hang等人。论文引用[1]:Dang,
Duc M et al. “YOWOv3: An Efficient and Generalized Framework for Human
Action Detection and Recognition.” (2024).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2024.Aug&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;one-stage detector&lt;/li&gt;
&lt;li&gt;different configurations to customie different model components&lt;/li&gt;
&lt;li&gt;efficient while reducing computational resource requirements&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;YOWOv3是YOWOv2的增强版，提供了更多的approach,用了不同的configurations来定制不同的model，YOWOv3比YOWOv2更好。&lt;/li&gt;
&lt;li&gt;STAD是计算机视觉中一个常见的任务，涉及到检测&lt;strong&gt;location(bbox),
timing(exact frame),and type(class of action)&lt;/strong&gt;，
需要对时间和空间特征进行建模。有很多的方法来解决STAD的问题，例如ViT，ViT的效果很好，但是计算量比较大。例如Hiera
model由超过600M的参数，VideoMAEv2由超过1B的参数，增加了训练的成本和消耗。为了解决STAD问题，同时最大程度减弱训练和推理时间的成本，有人提出用了YOWO方法，虽然可以做到实时，但是也有限制：不是一个efficient
model with low computational
requirements。框架的作者已经停止维护了，但是还有很多的问题。本文的contribution如下：
&lt;ul&gt;
&lt;li&gt;new lightweight framework for STAD&lt;/li&gt;
&lt;li&gt;efficient model&lt;/li&gt;
&lt;li&gt;multiple pretrained resources for application：creating a range of
pretrained resources spanning from lightweight to sophisticated models
to cater to diverse requirements for real-world applications。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>YOWOv2</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/26/YOWOv2/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/26/YOWOv2/</id>
    <published>2024-08-26T06:35:29.000Z</published>
    <updated>2024-09-01T02:15:01.524Z</updated>
    
    <content type="html"><![CDATA[<h3id="yowov2-a-stronger-yet-efficient-multi-level-detection-framework-for-real-time-stad1">YOWOv2:A Stronger yet Efficient Multi-level Detection Framework for Real-timeSTAD<sup>[1]</sup></h3><blockquote><p>作者是来自哈工大的 Jianhuan Yang和Kun Dai，论文引用[1]:Yang, Jianhuaand Kun Dai. “YOWOv2: A Stronger yet Efficient Multi-level DetectionFramework for Real-time Spatio-temporal Action Detection.” ArXivabs/2302.06848 (2023): n. pag.</p></blockquote><h3 id="time">Time</h3><ul><li>2023.Feb</li></ul><h3 id="key-words">Key Words</h3><ul><li>combined 2D CNN of diffferent size with 3D CNN</li><li>anchor-free mechanism</li><li>dynamic label assignment</li><li>multi-level detection structure</li></ul><h3 id="总结">总结</h3><ol type="1"><li>YOWOv2利用了3D backbone和2D backbone的优势，来做accurate actiondetection。设计了一个multi-level detectionpipeline来检测不同scales的actioninstances。为了实现这个目标，<strong>构建了一个 简单高效地2D backbonewith FPN，来提取不同level的classification features和regressionfeatures</strong>。对于 3D backbone，采用现有的3D CNN，通过结合3DCNN和不同size的2D CNN，设计了YOWOv2 family,包括:YOWOv2-Tiny，YOWOv2-Medium和YOWOv2-Large。同时引入了<strong>dynamiclabel assignmentstrategy</strong>和<strong>anchor-free</strong>机制，来使得YOWOv2和先进的模型架构一致。YOWOv2比YOWO好很多，同时能够保证实时检测。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>之前的工作中，由于3DCNN的网络的计算量比较大，实时性比较差；因此有人用一个参数共享的2DCNN网络，来提取spatial features frame byframe，然后把他们放到buffer中，在那之后，他们仅处理新的inputframe，将它的spaital feature和features in thebuffer一起形成spatio-temporal features for the finaldetection。然而，这样的pipeline，不能很好地model temporalassociation，实时的detection只能在RGBstreams的条件下实现，当用了光流的时候，尽管表现不错，但是速度下降了很多。之前的工作YOWO，效果很不错，但是也有两个不足。</p><ol type="1"><li><strong>YOWO是一个one-level detector,在一个low-level featuremap上进行最后的detection，损失了small actioninstances的性能。</strong></li><li><strong>YOWO是一个anchor-based 方法，有很多的anchor boxes with manyhyperparameters，例如数量、尺寸和anchor boxes的aspectratio。这些超参数必须仔细地设计，泛化性不够。</strong></li></ol><p>总的来说，设计一个实时的detection framework for spatio-temporalaction detection task仍然是一个挑战。 本文中，提出了一个新的实时的actiondetector，YOWOv2，<strong>YOWOv2包括一个3D backbone和multi-level的2Dbackbone</strong>，辛亏有multi-level 2D backbone withFPN，YOWOv2设计了一个<strong>multi-level detection pipeline来检测actioninstances of different scales</strong>.对于3D backbone，也用一个高效的3DCNN。另外，利用<strong>anchor-free mechanism</strong>，避免了anchorbox的缺点，因为anchor box去掉了，采用了一个<strong>dynamic labelassignmentstrategy</strong>,进一步提高了YOWOv2的多功能性。<strong>通过结合3Dbackbone和不同尺寸的2Dbackbones，构建了多个YOWOv2的模型，包括YOWOv2-Tiny，YOWOv2-Medium和YOWOv2-Large</strong>for the platforms with different computing power。</p><p>相比于YOWO，YOWOv2实现了更好的性能。另外，YOWOv2能够实时运行，相比于其它的实时的actiondetector，YOWOv2实现了更好的性能，contributions如下：</p><ul><li>YOWOv2是一个multi-level detection structure to detect small actioninstances。</li><li>YOWOv2是一个<strong>anchor-free</strong> detection pipeline</li><li>通过结合3D backbones和2D backbones of different sizes for theplatforms with different computing power。</li></ul></li><li><p>相关工作：STAD需要一个actiondetector来locate和identify当前帧中所有的instances。如何提取spatio-temporalfeatures对于精确的action detection是重要的。</p><ul><li>3D CNN-based：一些研究人员用3D CNN来设计actiodetectors，Girdhar用I3D来生产action regionproposals，然后用Transformer完成最后的detection；也有人用3D CNN来encoderinput video，然后用<strong>Transformer with the tuber queries for finaldetection</strong>。虽然3DCNN的方法很成功，但是需要大量的计算。<font color=red>实时性比较差</font></li><li>2D CNN-based：另外一种方式是将spatio-temporalassociations进行解耦，<strong>然后设计一个2D CNN-based action detectorsfor efficient detection</strong>。有人设计了一个one-stage detectionframework,<strong>ActionTubelet(ACT)</strong>：先用SSD来从videoclip中的每帧提取<strong>spatialfeatures</strong>然后stacked，再用一个<strong>detectionhead</strong>来处理stacked spatial features for the finaldetection。然后有人follow ACT的工作，设计了一个<strong>anchor-freeone-stage action detector MovingCenter</strong>。有人用自注意力增强了MOC，然而，这些方法的实时性是仅当输入时RGB的时候才可以，一旦加上了光流，尽管performancegain，但是速度是下降了。Moreover，高质量的光流需要离线获得，不满足在线的操作。</li></ul></li><li><p>Methodology： 给定 <spanclass="math inline">\(K\)</span>个frames的video clip <spanclass="math inline">\(V = \{I_{1},I_{2},\ldots,I_{K}\}\)</span> where<span class="math inline">\(I_K\)</span>是当前帧，YOWOv2用一个高效的3DCNN作为3D backbone来提取spatio-temporal features，<spanclass="math inline">\(F_{ST} \in\mathbb{R}^{\frac{H}{32}\times\frac{H}{32}\times C_{o_{2}}}\)</span>,YOWOv2的2D backbone是一个multi-level 2D CNN，用于输出解耦的multi-levelspatial features <span class="math inline">\(F_{cls} =\{F_{cls_i}\}_{i=1}^{3}\)</span> 和 <span class="math inline">\(F_{reg}= \{F_{reg_i}\}_{i=1}^3\mathrm{~of~}I_K\)</span>，where the <spanclass="math inline">\(F_{cls_{i}}\in\mathbb{R}^{\frac{H}{2^{i+2}}\times\frac{W}{2^{i+2}}\timesC_{o_{1}}}\)</span> 是分类的features，<spanclass="math inline">\(F_{reg_{i}} \in\mathbb{R}^{\frac{H}{2^{i+2}}\times\frac{W}{2^{i+2}}\timesC_{o_{1}}}\)</span>是回归的features。在这两个backbone之后，用了两个channelencoders on each feature map of level to integratefeatures。在这之后，两个额外并行的branches with two <spanclass="math inline">\(3 \times 3\)</span> conv layers followed thechannel encoders 来预测 <span class="math inline">\(Y_{cls_{i}}\in\mathbb{R}^{\frac{H}{2^{i+2}}\times\frac{W}{2^{i+2}}\timesN_{C}}\)</span> for classification。一个confidence branch加在了actionness confidence。</p><ul><li><strong>Design of YOWOv2</strong>：</li></ul><ol type="1"><li><font color=red>2D backbone</font>：2Dbackbone是用来抽取当前帧的multi-level spatialfeature。考虑性能和速度的balance，从先进的objectdetectors中得到了一些ideas。重新用了YOLOv7的backbone和FPN来节省时间，在FPN之后，加了一个额外的<spanclass="math inline">\(1 \times 1\)</span> conv layer 来压缩每个levelfeature map的channel number <span class="math inline">\(F_{s_i}\)</span>to <spanclass="math inline">\(C_o1\)</span>，默认设置为256。加了两个并行的brancheswith two <span class="math inline">\(3 \times 3\)</span> conv layers tooutput decoupled features。</li></ol><p><spanclass="math display">\[F_{cls_{i}}=f_{conv_{2}}^{1}\left(f_{conv_{1}}^{1}\left(F_{S_{i}}\right)\right)\\F_{reg_{i}}=f_{conv_{2}}^{2}\left(f_{conv_{1}}^{2}\left(F_{S_{i}}\right)\right)\]</span></p><p><span class="math inline">\(f^i_{convj}\)</span> 是第<spanclass="math inline">\(i\)</span>个branch的第<spanclass="math inline">\(j\)</span>个 <span class="math inline">\(3 \times3\)</span> conv layer。</p><p>在YOWOv2的框架中，2D backbone输出3个level的解耦的feature maps。 <spanclass="math inline">\(F_{cls} = \{F_{cls_{i}}\}_{i=1}^{3}\)</span> 和<span class="math inline">\(F_{reg} =\{F_{reg_{i}}\}_{i=1}^{3}\)</span>，为了方便，称2Dbackbone的为FreeYOLO。通过控制FreeYOLO的depth和width，设计了两个不同size的FreeYOLO，FreeYOLO-Tinyfor YOWOv2-Tiny，FreeYOLO-Large forYOWOv2-Medium和YOWOv2-Large。为了加速训练，在COCO上预训练带有一个额外<span class="math inline">\(1 \times 1\)</span> conv layers的2Dbackbone。</p><ol start="2" type="1"><li><font color=red>3D backbone</font>：3D backbone用来从videoclip中提取spatio-temporal features <spanclass="math inline">\(F_{ST}\)</span> for spatio-temporalassociation。采用高效的3D CNN来减小计算，保证实时检测，为了和decoupledspatial features融合，简单地upsample <spanclass="math inline">\(F_{ST}\)</span> 来得到 <spanclass="math inline">\(\{F_{STi}\}_{i=1}^{3}\)</span>，公式如下：</li></ol><p><spanclass="math display">\[\begin{aligned}&amp;F_{ST_{1}}=\mathrm{Upsample}_{4\times}\left(F_{ST}\right)\\&amp;F_{ST_{2}}=\mathrm{Upsample}_{2\times}\left(F_{ST}\right)\\&amp;F_{ST_{3}}=F_{ST}\end{aligned}\]</span></p><p>上采样操作是为了在空间维度上对齐 <span class="math inline">\(F_{ST_i}\in \mathbb{R}^{\frac{H}{2^{i+2}}\times\frac{H}{2^{i+2}}\timesC_{o_2}}\)</span> 和 <span class="math inline">\(F_{cls_i}\)</span>和<span class="math inline">\(F_{reg_i}\)</span>。</p><ul><li><font color=red>ChannelEncoder</font>：YOWO提出了ChannelEncoder，用来融合2D 和3D backbone出来的features。给定一个 <spanclass="math inline">\(F_{S} \in \mathbb{R}^{H_{o}\times W_{o}\timesC_{o_{1}}}\)</span> 和<span class="math inline">\(F_{ST} \in\mathbb{R}^{H_{o}\times W_{o}\times C_{o_{2}}}\)</span>，这个ChannelEncoder首先在channel dimension上进行concatenates，用两个conv layerfollowed a BN 和LeakyReLU来实现主要的channel integration： <spanclass="math display">\[F_f=f_{conv_2}\left(f_{conv_1}\left(\text{Concat}\left[F_S,F_{ST}\right]\right)\right)\]</span></li></ul><p><span class="math inline">\(F_{f} \in \mathbb{R}^{H_{o}\timesW_{o}\times C_{o_{3}}}\)</span>，Concat是要给channelconcatenation操作。然后 <spanclass="math inline">\(F_f\)</span>reshape成<spanclass="math inline">\(F_{f_{2}}\in\mathbb{R}^{C_{o_{3}}\timesH_{o}W_{o}}\)</span>，用来做之后的 self-attention mechanism inspired byDANet，这样包括不同levels的features能够完全的集成。</p><p><spanclass="math display">\[F_{f_{3}}=\text{Softmax}\left(F_{f_{2}}F_{f_{2}}^{T}\right)F_{f_{2}}\]</span></p><p>最后，<span class="math inline">\(F_{f_{3}} \in \mathbb{R}^{C_{o_{3}}\times H_{O}W_{o}}\)</span> 通过另一个conv layer被reshape成 <spanclass="math inline">\(F_{f} \in \mathbb{R}^{H_{o}\times W_{o}\timesC_{o_{3}}}\)</span>。</p><ul><li><font color=red>Decoupled fusion head</font>：YOWOv2中，2Dbackbone输出当前帧的decoupled spatial features，3D backbone输出的是videoclip上采样的 <spanclass="math inline">\(F_{ST}\)</span>。<strong>注意到，<spanclass="math inline">\(F_{cls_{i}}\)</span>和<spanclass="math inline">\(F_{reg_{i}}\)</span>包含不同的语义信息，因此需要将它们分别和<spanclass="math inline">\(F_{ST_{i}}\)</span>融合</strong>，因此设计了一个<strong>decoupled fusion head</strong> 来独立地融合 <spanclass="math inline">\(F_{ST_{i}}\)</span> into <spanclass="math inline">\(F_{cls_{i}}\)</span>，<spanclass="math inline">\(F_{reg_{i}}\)</span>。</li></ul><p><span class="math display">\[F_{cls_{i}}^{f}=\text{ChannelEncoder}(F_{cls_{i}},F_{ST_{i}})\\F_{reg_{i}}^{f}=\text{ChannelEncoder}(F_{reg_{i}},F_{ST_{i}})\]</span></p><p>在feature aggregation之后，用两个并行的branches on each level来做最后的detection，设计很简单，一个是分类的branch，一个是boxregression branch。</p><p>对于分类branch，输出分类的预测 <spanclass="math inline">\(Y_{cls_{i}}\)</span>, <spanclass="math inline">\(Y_{cls_{i}}\)</span> 表示 <spanclass="math inline">\(Y_{cls_{i}}\)</span>上面的每个空间位置的actioninstances的概率。 <span class="math inline">\(N_c\)</span>是actionclasses的数量。Taking <spanclass="math inline">\(F^f_{cls_{i}}\)</span>，分支应用两个 <spanclass="math inline">\(3 \times 3\)</span> conv layers，每个有 <spanclass="math inline">\(C\)</span>个filters，followed by SiLUactivations。最后，一个 conv layer with <spanclass="math inline">\(N_c\)</span> filters和sigmoid activations来输出<span class="math inline">\(N_c\)</span>个binary predictions per spatialposition。</p><p>对于box regression 分支，输出box regression prediction <spanclass="math inline">\(Y_{reg_{i}}\)</span>，<spanclass="math inline">\(Y_{reg_{i}}\)</span>代表每个空间位置的4个relativeoffsets。另外，一个额外的 $1 $ conv layer with 1filter加在了这个分支上for actioness confidence prediction，<spanclass="math inline">\(Y_{conf_{i}}\)</span>，注意到，在每个spatialposition，没有anchor box，<strong>YOWOv2是 anchor-free</strong>。</p><ul><li><strong>Loss assignment</strong>: 因为YOWOv2是anchor-free的actiondetector without any anchor boxes，multi-lelve labelassignment重要。最近，dynamic labelassignment在目标检测领域很成功，受YOLOX的启发，用<strong>SimOAT</strong>for the label assignment of YOWOv2。具体地，计算所有predictedbboxes和ground truths之间的cost。每个ground truth分配了<spanclass="math inline">\(top_k\)</span>个预测的bboxes with leastcost，<span class="math inline">\(k\)</span>是由预测的bboxes和targetbboes之间的IoU决定的。</li></ul><p><spanclass="math display">\[c_{ij}\left(\hat{a}_i,a_j,\hat{b}_i,b_j\right)=L_{cls}(\hat{a}_i,a_j)+\gammaL_{seg}(\hat{b}_i,b_j)\]</span></p><p><span class="math inline">\(\hat{\alpha}_i\)</span>和 <spanclass="math inline">\(\alpha_j\)</span>是分类的预测和target，<spanclass="math inline">\(\hat{b}_i\)</span>和 <spanclass="math inline">\(b_j\)</span>是回归的预测和target。<spanclass="math inline">\(\gamma\)</span>是cost平衡系数。</p><ul><li><strong>Loss function</strong>：<spanclass="math display">\[\begin{aligned}L(\{a_{x,y}\},\{b_{x,y}\},\{c_{x,y}\})&amp;=\frac{1}{N_{pos}}\sum_{x,y}L_{conf}(\hat{c}_{x,y},c_{x,y}) \\&amp;+\frac{1}{N_{pos}}\sum_{x,y}\mathbb{I}_{\{\hat{a}_{x,y}&gt;0\}}L_{cls}(\hat{a}_{x,y},a_{x,y})\\&amp;+\frac{\lambda}{N_{pos}}\sum_{x,y}\mathbb{I}_{\{\hat{a}_{x,y}&gt;0\}}L_{reg}(\hat{b}_{x,y},b_{x,y})\end{aligned}\]</span></li></ul><p><span class="math inline">\(L_cls\)</span>是binarycross-entropy，<spanclass="math inline">\(L_{reg}\)</span>是GIoU损失，<spanclass="math inline">\(a_{x,y}\)</span>，<spanclass="math inline">\(b_{x,y}\)</span>和<spanclass="math inline">\(c_{x,y}\)</span>分别是分类的预测、回归的预测和confidenceprediction。<span class="math inline">\(\hat{a}_{x,y}\)</span>，<spanclass="math inline">\(\hat{b}_{x,y}\)</span>和<spanclass="math inline">\(\hat{c}_{x,y}\)</span> 是groundtruths。<spanclass="math inline">\(\mathbb{I}_{\{\hat{a}_{x,y}&gt;0\}}\)</span>是indicatorfunction，<span class="math inline">\(\hat{a}_{x,y}\)</span> &gt;0的时候为1，否则为0。<spanclass="math inline">\(N_{pos}\)</span>是positive的预测的数量。<spanclass="math inline">\(\lambda\)</span>是损失的平衡系数，在实验中为5.</p></li><li><p>实验：实验中表明，<strong>decoupled featurefusion</strong>是有必要的，因为<strong>categorical 和regressivefeatures的语义信息不一样</strong>；3DCNN和光流的方法都会导致计算量过大，很难保证实时性。</p></li></ol><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/5f039c1410a764ad673dbb3336d360fad9e4e9e6/2-Figure1-1.png"alt="Structure" /> <span class="math inline">\(Fig. \ 1^{[1]}\)</span>.Overview of YOWOv2. YOWOv2 uses upsampling operation to align thespatio-temporal features output by the 3D backbone with the spatialfeatures of each level output by the 2D bakcbone and uses the Decoupledfusion head to achieve the fusion of the two features on each level.Finally, YOWOv2 outputs the multi-level confidence predictions,classification predictions, and regression predictions respectively.</p><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/5f039c1410a764ad673dbb3336d360fad9e4e9e6/3-Figure2-1.png"alt="channel encoder" /> <span class="math inline">\(Fig. \2^{[1]}\)</span>. Overview of ChannelEncoder. It contains the channelfusion and channel self-attention mechanism, which are both used to fuse2D and 3D features.</p><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/5f039c1410a764ad673dbb3336d360fad9e4e9e6/4-Figure3-1.png"alt="Coupled fusion head" /> <span class="math inline">\(Fig. \3^{[1]}\)</span>. Coupled fusion head. In the coupled head, the spatialfeatures from the 2D backbone is also coupled which means that theparallel 3 × 3 conv layers after the FPN are removed.</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;yowov2-a-stronger-yet-efficient-multi-level-detection-framework-for-real-time-stad1&quot;&gt;YOWOv2:
A Stronger yet Efficient Multi-level Detection Framework for Real-time
STAD&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自哈工大的 Jianhuan Yang和Kun Dai，论文引用[1]:Yang, Jianhua
and Kun Dai. “YOWOv2: A Stronger yet Efficient Multi-level Detection
Framework for Real-time Spatio-temporal Action Detection.” ArXiv
abs/2302.06848 (2023): n. pag.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2023.Feb&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;combined 2D CNN of diffferent size with 3D CNN&lt;/li&gt;
&lt;li&gt;anchor-free mechanism&lt;/li&gt;
&lt;li&gt;dynamic label assignment&lt;/li&gt;
&lt;li&gt;multi-level detection structure&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;YOWOv2利用了3D backbone和2D backbone的优势，来做accurate action
detection。设计了一个multi-level detection
pipeline来检测不同scales的action
instances。为了实现这个目标，&lt;strong&gt;构建了一个 简单高效地2D backbone
with FPN，来提取不同level的classification features和regression
features&lt;/strong&gt;。对于 3D backbone，采用现有的3D CNN，通过结合3D
CNN和不同size的2D CNN，设计了YOWOv2 family,
包括:YOWOv2-Tiny，YOWOv2-Medium和YOWOv2-Large。同时引入了&lt;strong&gt;dynamic
label assignment
strategy&lt;/strong&gt;和&lt;strong&gt;anchor-free&lt;/strong&gt;机制，来使得YOWOv2和先进的模型架构一致。YOWOv2比YOWO好很多，同时能够保证实时检测。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>TAAD</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/TAAD/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/TAAD/</id>
    <published>2024-08-24T08:53:52.000Z</published>
    <updated>2024-08-30T03:11:10.636Z</updated>
    
    <content type="html"><![CDATA[<h3id="spatio-temporal-action-detection-under-large-motion1">Spatio-TemporalAction Detection Under Large Motion<sup>[1]</sup></h3><blockquote><p>作者是来自ETHZ的Gurkirt Singh, Vasileios Choutas, Suman Saha, FisherYu和Luc Van Gool。论文引用[1]:Singh, Gurkirt et al. “Spatio-TemporalAction Detection Under Large Motion.” 2023 IEEE/CVF Winter Conference onApplications of Computer Vision (WACV) (2022): 5998-6007.</p></blockquote><h3 id="time">Time</h3><ul><li>2022.Oct</li></ul><h3 id="key-words">Key Words</h3><ul><li>track information for feature aggregation rather than tube fromproposals</li><li>3 motion categories: large motion、medium motion、small motion</li></ul><h3 id="总结">总结</h3><ol type="1"><li>当前的STAD的tubedetection的方法经常将一个给定的<strong>keyframe</strong>上的bboxproposal扩展成一个<strong>3D temporalcuboid</strong>，然后从邻近帧进行poolfeatures。如果actor的位置或者shape表现出了large 2D motion和variabilitythrough frames，这样的pooling不能够积累有意义的spaito-temporalfeatures。在这个工作中，作者旨在研究<strong>cuboid-aware featureaggregation in action detection under largeaction</strong>。进一步，提出了在<strong>largemotion</strong>的情况下，通过<strong>tracking actors和进行temporalfeature aggregation along the respective tracks</strong>增强actorfeature representation，定义了在不同的固定的time scales下的<strong>actormotion的IoU</strong>。有large motion的action会随着时间导致lowerIoU，slower actions会随着时间维持higher IoU。作者发现<strong>track-awarefeature aggregation持续地实现了很大的提升in actiondetection</strong>。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>目前的很多工作聚焦于通过复杂的上下文建模和更大的backbone，或者利用光流stream，来提升actiondetection的性能。上述方法用了cuboid-aware temporal pooling for featureaggregation。在这个工作中，旨在研究actioninstance的不同角度的情况下的cuboid-aware action detection。<strong>largeobject motion</strong>有不同的原因：例如fast camera motion，fastaction，body shape deformation due to pose change, or mixed camera andaction motions。所有的这些原因会造成sub-optimal featureaggregation，导致action classification的错误。</p></li><li><p>作者将actions分成了3类：largemotion、medium-motion和small-motion。这个区分是基于同一个actor overtime的boxes的IoU,可以用actors的ground truthtubes来计算。研究cuboid-aware的基线方法在不同motioncategories上的表现，不用context features或者long-termfeatures之类的。因为large-motion在很短的时间窗口中发生的很快。<strong>large-motioncases, IoU会比较小，因此一个3D cuboid-aware featureextractor不能获取actor location上的features throughtoutaction</strong>。为了解决large-motion，提出了随着时间来<strong>track theactor</strong>，用 <strong>Track-of-InterestAlign(TOI-Align)</strong>来extract features，resulting in <strong>TrackAware Action Detector(TAAD)</strong>。Further，研究了TOI-Alignedfeatures上的不同类型的 feature aggregation modules for TAAD网络。</p><p>为了这个目的，有以下的contributions：</p><ul><li>是第一个用evaluaiton metrics for each type ofmotion，来研究large-motion action detection。</li><li>提出了用 <strong>tube/track-aware feature aggregationmodules</strong>来处理largemotion，这种类型的module实现了很好的提升。</li></ul></li><li><p>Related work: Action recognition models提供了很强的videorepresentation models；action detection是一个在largemotion下的理解actions的问题。主要关注于STAD 问题，这里一个<strong>actioninstance被定义为随着时间的一系列的linked bboxes</strong>。最近的onlineactiondetection的方法的性能直追离线的STAD的方法。MultiSports数据集有更多的fine-grainedactionclass；有多个actors在同一个视频里执行不同类型的action。另外，MultiSports是一个<strong>denselyannotated</strong>。 很多的方法关注于keyframe，基于action detection onAVA，long-term feature banks起到了重要作用，获得了一些temporalcontext，但是没有actors之间的temporalassociations；另外也有人研究了actors和object之间的interactions。这些以上的方法都是用<strong>cuboid-awarepooling for local feature aggregation</strong>，作者们发现，当motion isquick和large，这不是最理想的方法。作者这里用SlowFast作为基线方法。</p><p><strong>Weinzaepfel</strong>是首个用<strong>tracking for actiondetection</strong>的工作，用一个tracker来解决tube generationpart的<strong>linking</strong>问题，给定tracks中的bboxesproposals，可以on a frame-by-frame basis做actionclassification。作者提出通过<strong>pooling features from within entiretracks</strong>来处理action detection。</p></li><li><p>Methodology：提出了处理large motions的方法，称之为Track AwareAction Detector(TAAD)，在视频中trackactor，同时，用了一个神经网络designed for videorecognition，来从每个clip中提取特征。用track boxes和video features，poolper-frame features with a RoI-Align operation，之后，<strong>TemporalFeature Aggregation module</strong>得到per-trackfeatures，计算单个feature vector，这里classifier预测最后的actionlabel。</p><ul><li><p><font color=red>Baseline ActionDetector</font>：选择Slowfast作为videobackbone，原因是它相对于大型的transformer模型仍然有竞争力 on the task ofSTAD；另外，Slowfast比其它的transformer的方法更高效。而且提供了不同temporalscales的features，有不同的temporalscales是重要的，特别是作者旨在处理fast/large motions,where a smallerscale是必须的；最后Slofast是MultiSports和UCF24datasets默认的backbone。用基于Slowfast的架构的pySlowFast with aResNet-50来执行baseline。首先，增加background frame,作为训练actiondetector的额外的negative samples。接下来，用一个multiclassclassifier来代替multi-label，switching from a binary cross entropy perclass to a cross entropy loss(CE-loss)。最后，加一个<strong>downward FPNblock</strong></p></li><li><p><font color=red>Tracker</font>：用YOLOv5-DeepSort的classagnostic版本作为tracker，这个是基于YOLOv5和TrochReID，fine-tuneYOlOv5的medium size作为detection model for personclasses。一个预训练的OsNet-x0-25被用作ReID模型。一个high recall、smallnumber of association的tracker对于提高action tubedetection的性能是重要的。<strong>微调detector</strong>也是重要的一步。<strong>Tracker可以被用作bboxesproposal filtering module</strong>,有时候detector产生多个high scoringdetections，有些会导致falsepositives,这些detections不和任何的tracks相匹配，因为它们时序上不是一致的。tracks产生的proposals能够在测试的时候用上。</p></li><li><p><font color=red>Temporal Feature Aggregation</font>：</p></li></ul><ol type="1"><li><strong>Track-of-Interest Align(TOI-Align)</strong>：SlowFast videobackbone处理输入的clip，产生 <span class="math inline">\(T \times H\times W\)</span> feature tensor，然而trackers返回一个 <spanclass="math inline">\(N_t \times T \times4\)</span>的array，包含物体附近的boxes。RoI-Align将这两个arrays作为输入，得到一个featurearray <span class="math inline">\(N_t \times T \times H \timesW\)</span>，one feature tube per track，在track的length小于inputclip的情况下，在时序上复制最后的available bbox。</li><li><strong>Featureaggregation</strong>：为了预测keyframe中的bbox的label，需要aggregatefeatures across time andspace。首先，在TOI-Align提取出来的features上，在空间维度上做一个averagepooling，然后执行Temporal Feature Aggregation的一个变体。<ul><li>Max-pooling over temporal axes(MaxPoo)</li><li>A sequence of temporal conv(TCN)</li><li>A temporal variant of Atrous Spatial PyramidPooling(ASPP)，修改Detectron2中的ASPP，用1D conv代替2D也尝试了ConvNeXt和VideoSwin的temporalversion，然而，这些导致不稳定的训练，即使调整学习率和其它的超参数，实验中，仅用了temporalconv for TCN module的一个layer。</li></ul></li></ol><ul><li><font color=red>Tube Construction</font>：<strong>Video-level tubedetection要求从per-frame detections中构建actiontubes。这个过程分为两步：首先将proposals连接起来形成tubehypotheses；然后trim这些proposals，得到有action的部分。可以将这两部视为trackingstep加上一个temporal action detection step</strong>。大多数的<strong>action tube detectionmethod</strong>在第一步用一个贪心的proposal连接算法；<strong>由于TAAD已经有tracks，所以不需要linkingstep</strong>。action tracks的temporal trimming是通过<strong>labelsmoothingoptimisation</strong>来做的，之前的很多工作都用到了，具体地，用了<strong>class-wisetemporal trimming</strong>。</li></ul></li><li><p>实验：</p></li><li><p>讨论和结论：在实验中发现，TAAD用<strong>tracking information forfeature aggregation，而不是从proposalboxes得到的tube</strong>,提高了性能，这不意味着没有了提高的空间，作者的方法对于<strong>tracker的性能比较敏感</strong>，因为这是pipeline的第一步。用更好的SOTA的tracker和persondetector，可以进一步提高性能。通过在<strong>TAAD中加上spatial/actorcontext modelling, long-term temporal context 或者一个transformer heador backbone</strong>，也能提高性能。 关于motion分类的定义可以说是不精确，不同于MS COCO中的object size类别，motion类别不容易定义。除了普遍的复杂的相机运动和quick actormotion外，必须特别注意错误标记。</p></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/978129084bd486157f3cc0997204554956a7cca3/4-Figure4-1.png"alt="Pipleline of TAAD" /><figcaption aria-hidden="true">Pipleline of TAAD</figcaption></figure><p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Proposed TrackAware Action Detector (TAAD): Given an input clip with T frames, weextract features using a video recognition network and Nt per-actortracks from a tracker. The TOI-Align operation extracts per-trackfeatures from the entire video sequence, using an RoI-Align operationand the track boxes, returning a <span class="math inline">\(N_t × T ×C\)</span> feature array. Next, the Temporal Feature Aggregation (TFA)module aggregates the features along the temporal dimension and passesthe resulting <span class="math inline">\(N_t × C\)</span> array to theaction classifier that predicts the action label.</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;spatio-temporal-action-detection-under-large-motion1&quot;&gt;Spatio-Temporal
Action Detection Under Large Motion&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自ETHZ的Gurkirt Singh, Vasileios Choutas, Suman Saha, Fisher
Yu和Luc Van Gool。论文引用[1]:Singh, Gurkirt et al. “Spatio-Temporal
Action Detection Under Large Motion.” 2023 IEEE/CVF Winter Conference on
Applications of Computer Vision (WACV) (2022): 5998-6007.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2022.Oct&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;track information for feature aggregation rather than tube from
proposals&lt;/li&gt;
&lt;li&gt;3 motion categories: large motion、medium motion、small motion&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;当前的STAD的tube
detection的方法经常将一个给定的&lt;strong&gt;keyframe&lt;/strong&gt;上的bbox
proposal扩展成一个&lt;strong&gt;3D temporal
cuboid&lt;/strong&gt;，然后从邻近帧进行pool
features。如果actor的位置或者shape表现出了large 2D motion和variability
through frames，这样的pooling不能够积累有意义的spaito-temporal
features。在这个工作中，作者旨在研究&lt;strong&gt;cuboid-aware feature
aggregation in action detection under large
action&lt;/strong&gt;。进一步，提出了在&lt;strong&gt;large
motion&lt;/strong&gt;的情况下，通过&lt;strong&gt;tracking actors和进行temporal
feature aggregation along the respective tracks&lt;/strong&gt;增强actor
feature representation，定义了在不同的固定的time scales下的&lt;strong&gt;actor
motion的IoU&lt;/strong&gt;。有large motion的action会随着时间导致lower
IoU，slower actions会随着时间维持higher IoU。作者发现&lt;strong&gt;track-aware
feature aggregation持续地实现了很大的提升in action
detection&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>TubeR</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/TubeR/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/TubeR/</id>
    <published>2024-08-24T08:53:46.000Z</published>
    <updated>2024-08-26T09:28:55.720Z</updated>
    
    <content type="html"><![CDATA[<h3 id="tuber-tubelet-transformer-for-video-action-detection1">TubeR:Tubelet Transformer for Video Action Detection<sup>[1]</sup></h3><blockquote><p>作者是来自阿姆斯特丹大学、罗格斯大学和AWS AI Labs的JiaojiaoZhao、Yanyi Zhang等人。论文引用[1]:Zhao, Jiaojiao et al. “TubeR: TubeletTransformer for Video Action Detection.” 2022 IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR) (2021): 13588-13597.</p></blockquote><h3 id="time">Time</h3><ul><li>2021.April</li></ul><h3 id="key-words">Key Words</h3><ul><li>tubelet queries, tubelet-attention</li><li>in sequence-to-sequence manner</li><li>scales well to longer video clips</li><li>end-to-end without person detectors, anchors or proposals</li></ul><h3 id="总结">总结</h3><ol type="1"><li>不同于现有的依赖于离线检测器或者人工设计的actor-positionalhypotheses like proposals or anchors，提出了一个通过同时进行actionlocalization和recognition from a singlerepresentation，直接检测视频里的actiontubelet的方法。TubeR学习一系列的tubelet queries，利用tubelet-attentionmodule来model video clip里的动态的spatio-tempralnature。相比于用actor-positional hypotheses in the spatio-temporalspace，它能够有效的强化模型的能力。对于包含transitional states或者scenechanges的视频，提出了一个context aware classificationhead，来利用short-term和long-term context to strengthen actionclassification，和一个action switch regression head来检测精确的时序上的行为范围。TubeR直接产生不同长度的actiontubelets，对于长的视频clips，也能保持一个比较好的结果。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>Actiondetection是一个复杂的任务，要求逐帧的任务的定位、将检测到的personinstances连接成actiontubes、预测action的类别。STAD中两种路径很流行：frame-level和video-level。<strong>frame-level的detection在每一帧上独立地进行检测和分类，然后将per-framedetections连接起来，形成连贯的actiontubes</strong>；为了弥补时序信息的缺失，一些方法简单地重复2D proposals或者离线的人物检测 over time，来得到时空特征。<strong>tubelet-leveldetection，直接生成spatio-temporal volumes from a videoclip，来获得连贯和动态的natures of actions。通常预测actionlocalization和classification jointly over spatio-temporalhypotheses</strong>。像 3D cuboid proposals。然而， 这些 3Dcuboids仅能够 capture a short period of time, when spatial location of aperson changes as soon as they move or due to cameramotion。Ideally，这些模型能够用灵活的spatio-temporal tubelets，能够trackthe person over a longer time。但是，large configuration space of such aparameterization限制了之前的方法只能用shortcuboids。这里，作者提出了一个tubelet-level的detectionapproach，<strong>能够同时定位和识别action tubelets in a flexiblemanner，使得tubelets能够随着时间改变size和location</strong>。这使得系统能够利用更长的tubelets，在更长的时间上汇聚人物和它们的行为的视觉信息。</p><p>从NLP中的sequence-to-sequence获得启发，特别是机器翻译和它在目标检测上的应用，DETR。DETR是一个frame-level的actiondetection。这里，用decoderqueries来表示整个视频序列上的人物和它们的行为，不限制tubelets是一个固定的cuboids。</p><p>提出了<font color=red>tubelet-transformer</font>：称之为<strong>TubeR for localizing and recognizing actions from a singlerepresentation</strong>。基于DETR的框架，TubeR学习一系列的 tubeletqueries，来从spatio-temporal video representation中pull action-specifictubelet-level features。TubeR的包括特别的 <strong>spatial and temporaltubelet attention</strong>，使得tubelets能够随着时间在它们的spatiallocation 和scale上没有限制。克服了之前对于cuboidsd限制。TubeR随着时间在一个 tubelet里 回归bboxes。考虑到tubelets之间的时序关联。汇聚visual features over thetubelet to classifyactions。这个涉及表现很好，但是并没有比用离线人物检测器的方法好很多。猜想是因为querybased features缺少全局上下文，only look at a singleperson的话，很难对涉及到的relationships 行为例如：listening-to或者talking-to进行分类。因此，提出了一个 <strong>contextaware classification head</strong>， along with the tubelet feature,利用完整的clipfeatures，分类头能够得出上下文信息。这个设计能够使得网络将persontubelet和完整的scene context(where tubeletappears)关联起来。这个设计的限制就是 *context feature仅能够从 tubelet占据的same clip中得到。包含long term contextualfeatures对于最后的行为分类很重要。因此受到要锁和存储tubelet附近的视频内容的contextual features的启发，引入了memorysystem。用相同的feature injection策略，将long term contextual memory给到分类头。</p><p>主要的贡献如下：</p><ol type="1"><li>提出来TubeR：一个tubelet-level的transformer 框架 for human actiondetection</li><li>tubelet query和attention basedformulation能够产生任意位置和尺寸的tubelets。</li><li>context aware classification head能够汇聚short-term和long-term上下文信息。</li></ol></li><li><p>相关工作：</p><ul><li><strong>frame-level action detection</strong>：用2D positionalhypothese(anchors) 或者离线的person detector on a keyframe来定位actors，然后更多地关注提高actionrecognition。通过利用光流分支来包含temporal patterns。其它的用 3DconvNet来获取时序信息来识别行为。不同于frame-level的方法，面向tubelet-levelvideo actiondetection，用一个统一的configuration，来进行定位和识别。</li><li><strong>Tubelet-level action detection</strong>：通过将tubelet作为一个representation unit来 detect actions变得流行了，有人重复 2Danchors per-frame来pooling ROI features，然后stack frame-wisefeatures来预测行为类别。有人依赖严格设计的 3D cuboidproposal，前者直接俄检测tubelets，后者逐步第refines 3D cuboid proposalsacross time。除了box/cuboid anchors，也有人通过centerposition假设，来检测tubeletinstances。基于假设的方法来处理长视频clips有很多困难。通过学习tubeletqueries的子集，来表示tubelets的动态nature。将action detection taskreformulate程一个 sequence-to-sequence学习的问题，在一个tubelet里显式地model temporal correlations。</li><li><strong>Transformer-based action detection</strong>：Girdhar提出来一个video action transformer network for detectingactions。用region-proposal-network forlocalization,通过汇聚actors附近的时空上下文信息，提高actionrecognition.</li></ul></li><li><p>TubeR：TubeR的输入是一个video clip，直接输出一个<strong>tubelet：a sequence of bboxes and the actionlabel</strong>，TubeR是受 DETR的启发，但是将transformer架构 reformulatefor sequence-to-sequence modeling in video。 给定一个video clip $IR^{T_{in} H W C} <span class="math inline">\(，\)</span>T_{in}, H, W,C$分别表示帧数，height,width和channel， TubeR首先用一个 3Dbackbone来提取 video feature <spanclass="math inline">\(F_{\mathrm{b}}\in\mathbb{R}^{T^{\prime}\timesH^{\prime}\times W^{\prime}\times C^{\prime}}\)</span>，<spanclass="math inline">\(T&#39;\)</span> 表示 temporal dimension， <spanclass="math inline">\(C&#39;\)</span>是 featuredimension。用一个transformer的encoder-decoder来transform视频的特征 intoa set of tubelet-specific feature <spanclass="math inline">\(F_{\mathrm{tub}}\in\mathbb{R}^{N\timesT_{\mathrm{out}}\times C^{\prime}}\)</span>，<spanclass="math inline">\(N\)</span>是 tubelets的数量。为了处理长的videoclips，用temporal 下采样，使得 $ T_{out} &lt; T' &lt; T_{in}$，减小memory的 requirement。TubeR 产生了稀疏的sparse，对于短的videoclips，去掉时序下采样，使得 <span class="math inline">\(T_{out} &lt;T&#39; &lt; T_{in}\)</span>，results in dense tubelets。Tubelet回归和associated action classification能够用一个separated task heads同时实现：</p><p><spanclass="math display">\[y_{\mathrm{coor}}=f(F_{\mathrm{tub}});y_{\mathrm{class}}=g(F_{\mathrm{tub}}),\]</span></p><p><span class="math inline">\(f\)</span>表示 tubelet 回归头， <spanclass="math inline">\(y_{coor} \in \mathbb{R}^{N\timesT_{\mathrm{out}}\times4}\)</span> 表示<spanclass="math inline">\(N\)</span>个 tubelets的坐标，each of which isacross <span class="math inline">\(T_{out}\)</span> frames(或者 <spanclass="math inline">\(T_{out}\)</span> sampled frames for longclips)。<span class="math inline">\(g\)</span>表示行为分类头，<spanclass="math inline">\(y_{\mathrm{class}}\in\mathbb{R}^{N\timesL}\)</span>表示 <span class="math inline">\(N\)</span>个 tubelets with<span class="math inline">\(L\)</span> 个可能的labels的行为分类。</p><ul><li><font color=red>TubeREncoder</font>：不同于普通的transformer的encoder，TubeR encoder用于处理3D 时空中的信息。每个 encoder layer由 self-attentionlayer(SA)、两个normalization layer和一个 FFN组成。core attentionlayers的公式如下：</li></ul><p><spanclass="math display">\[F_\mathrm{en}=\mathrm{Encoder}(F_\mathrm{b}),\]</span><spanclass="math display">\[\mathrm{SA}(F_\mathrm{b})=\mathrm{softmax}(\frac{\sigma_q(F_\mathrm{b})\times\sigma_k(F_\mathrm{b})^T}{\sqrt{C^{\prime}}})\times\sigma_v(F_\mathrm{b}),\]</span><spanclass="math display">\[\sigma(*)=\mathrm{Linear}(*)+\mathrm{Emb}_{\mathrm{pos}},\]</span></p><p><span class="math inline">\(F_b\)</span>是 backbone feature,<spanclass="math inline">\(F_{en} \in R^{T&#39;H&#39;W&#39; \timesC&#39;}\)</span>，<span class="math inline">\(C&#39;\)</span>表示dimensional encoded feature embedding。<spanclass="math inline">\(\sigma(*)\)</span> 是线性变换加上 positionalembedding。<span class="math inline">\(Emb_{pos}\)</span>是 3Dpositional embedding。optional temporal down-sampling 能够用在backbonefeatures上，来shrink 输入的sequence length to transformer for bettermemory efficiency。</p><ul><li><p><font color=red>TubeR Decoder</font>：</p><ul><li><p><strong>tubelet query</strong>：基于anchor假设来直接检测tubelets是相当有挑战的。tubelet space along thespatio-temporal dimension相比于single frame bbox space来说是巨大的。考虑FasterRCNN，requires for each position in a feature map with spatialsize <span class="math inline">\(H^{\prime}\times W^{\prime},K(=9)\)</span> anchors，总共有 <spanclass="math inline">\(KH&#39;W&#39;\)</span>个anchors。对于一个across<span class="math inline">\(T_{out}\)</span> frames的tubelet来说，需要<spanclass="math inline">\(KH&#39;W&#39;^{T_{out}}\)</span>个anchors，来保持同样的samplingin space-time。为了减小tubelet space，一些方法通过忽略短的videoclip中的action的空间位移，用 3Dcuboids来近似tubelets。然而，视频clip越长， 3Dcuboids代表的tubelet的精度越低。提出了学习tubelet queries小的子集，<span class="math inline">\(Q{=}\{Q_{1},...,Q_{N}\}\)</span>, <spanclass="math inline">\(N\)</span>是queries的数量。 第 <spanclass="math inline">\(i\)</span>个 tubelet query <spanclass="math inline">\(Q_{i}=\{q_{i,1},...,q_{i,T_{\mathrm{out}}}\}\)</span>包含 <span class="math inline">\(T_{out}\)</span> box query embeddings<span class="math inline">\(q_{i,t} \in R^{C&#39;}\)</span> across <spanclass="math inline">\(T_{out}\)</span> frames。学习一个tubeletquery表示dynamics of a tubelet, 而不是手工设计的 3D anchors。初始化boxembeddings identically for a tubelet query。</p></li><li><p><strong>tubelet attention</strong>：为了model tubeletqueries内的relations，提出来一个 tubelet-attention (TA)module，包含连个self-attention layers。首先有一个 <em>spatialself-attention layer</em>来处理一帧内的box queryembeddings的空间relations。这个layer的intuition是识别actions 受益于interactions between actors，或者between actos and objects in the sameframe。接下来有 <strong>temporal self-attentin layer</strong>来models同一个tubelet里的box query embeddings acrosstiem的correlations。这一层促使 TubeR query 去trackactors，然后产生action tubelets，聚焦于single actors而不是一个fixed areain the frame。TubeR decoder把tubelet attention module用在了 tubeletqueries <span class="math inline">\(Q\)</span>上，来产生 tubelet queryfeature <spanclass="math inline">\(F_{\mathfrak{a}}\in\mathbb{R}^{N\timesT_{\mathrm{out}}\times C^{\prime}}\)</span>：</p><p><span class="math inline">\(F_q = TA(Q)\)</span></p></li><li><p><strong>Decoder</strong>：decoder包含一个 tubele-attentionmodule和一个 cross-attention (CA) layer，用来decode tubelet-specificfeature <span class="math inline">\(F_{tub}\)</span> from <spanclass="math inline">\(F_{en}\)</span> to <spanclass="math inline">\(F_q\)</span>：</p></li></ul><p><spanclass="math display">\[\mathrm{CA}(F_{q},F_{\mathrm{en}})=\mathrm{softmax}(\frac{F_{q}\times\sigma_{k}(F_{\mathrm{en}})^{T}}{\sqrt{C^{\prime}}})\times\sigma_{v}(F_{\mathrm{en}}),\\F_{\mathrm{tub}}=\mathrm{Decoder}(F_{q},F_{\mathrm{en}}).\]</span></p><p><span class="math inline">\(F_\mathrm{tub}\in\mathbb{R}^{N\timesT_\mathrm{out}\times C^{\prime}}\)</span> 是tubelet specificfeatures。有temporal pooling的时候，<span class="math inline">\(T_{out}&lt; T_{in}\)</span>，TubeR产生 <strong>sparse tubelets</strong>，对于<span class="math inline">\(T_{out} = T{in}\)</span>，TubeR产生 densetubelets。</p></li></ul><p><font color=red>Task-Specific Heads</font>：对于每个tubelet，bbox和action classification可以用独立的task-specificheads来处理。这样的设计最大化的减小了计算量。</p><ul><li><strong>Context aware classificationhead</strong>：这个分类用一个简单的linear project就能实现。 <spanclass="math display">\[y_{\mathrm{class}}=\mathrm{Linear_c}(F_{\mathrm{tub}}),\]</span></li></ul><p><span class="math inline">\(y_{class} \in R^{N \timesL}\)</span>表示在 <spanclass="math inline">\(L\)</span>个可能的label上的分类的分数。one foreach tubelet。</p><ol type="1"><li><em>Short-term context head</em>:对于理解sequences，context是重要的。进一步提出利用spatio-temporal videocontext来帮助理解sequence。query the action specific feature <spanclass="math inline">\(F_{tub}\)</span> from some context feature <spanclass="math inline">\(F_{context}\)</span> to strengthen <spanclass="math inline">\(F_{tub}\)</span>，得到了 feature <spanclass="math inline">\(F_{c}\in R^{N \timesC&#39;}\)</span>，用于最后的分类： <spanclass="math display">\[F_\text{c}=\text{CA}(\text{Pool}_t(F_\text{tub}),\text{SA}(F_\text{context}))+\text{Pool}_t(F_\text{tub}).Eq.9\]</span></li></ol><p>这里设置 <span class="math inline">\(F_{context} = F_b\)</span> forutilizing the short-term context in the backbone feature。称之为<strong>Short-term context head</strong>，<spanclass="math inline">\(F_{context}\)</span> 首先用一个自注意力层，然后cross-attenion layer utilizes <spanclass="math inline">\(F_{tub}\)</span> to query from <spanclass="math inline">\(F_{context}\)</span>。<spanclass="math inline">\(F_{c}\)</span>经过线性层，用于最后的分类。</p><ol start="2" type="1"><li><em>Long-term context head</em>：为了利用long-range的时序信息，但是在有限的memory下，采用两阶段的decoder for long-termcontext compression。</li></ol><p><spanclass="math display">\[\mathrm{Emb}_{\mathrm{long}}=\mathrm{Decoder}(\mathrm{Emn}_{n1},\mathrm{Decoder}(\mathrm{Emb}_{n0},F_{\mathrm{long}}).\]</span></p><p>long-term context <spanclass="math inline">\(F_{\mathrm{long}}\quad\in\quad\mathbb{R}^{T_{\mathrm{long}}\timesH^{\prime}W^{\prime}\times C^{\prime}}\)</span> 是一个buffer，包含从<spanclass="math inline">\(2W\)</span>个在时间上concatenated的相邻的clips抽取出来的backbonefeature。为了将long-term video feature buffer压缩到 embedding <spanclass="math inline">\(Emb_{long}\)</span> with a lower temporaldimension，用了两个 stacked decoders with token <spanclass="math inline">\(Emn_{n0}\)</span> 和 <spanclass="math inline">\(Emn_{n1}\)</span>。首先用一个压缩的token <spanclass="math inline">\(Emb_{n0} (n0 &lt; T_{long})\)</span> to query<spanclass="math inline">\(F_{long}\)</span>中重要的信息，得到一个temporaldimension 为 <spanclass="math inline">\(n0\)</span>的中间压缩embedding。然后，进一步利用另外一个压缩的token<span class="math inline">\(Emb_{n1} (n1 &lt; n0)\)</span> to query from中间压缩的embedding，然后得到最后的压缩embedding <spanclass="math inline">\(Emb_{long}\)</span>。<spanclass="math inline">\(Emb_{long}\)</span>包含long-term的视频信息，但是有着 lower temporal dimension <spanclass="math inline">\(n1\)</span>，然后，对 <spanclass="math inline">\(F_b\)</span>和 <spanclass="math inline">\(Emb_{long}\)</span>采用cross-attentionlayer，来得到long-term context feature <spanclass="math inline">\(F_{\mathrm{lt}}\in\mathbb{R}^{T^{\prime}\timesH^{\prime}\times\bar{W}^{\prime}\times C^{\prime}}\)</span>：</p><p><spanclass="math display">\[F_{\mathrm{lt}}=\mathrm{CA}(F_{\mathrm{b}},\mathrm{Emb}_{\mathrm{long}}),\]</span></p><p>设置 <span class="math inline">\(F_{context} = F_{lt} inEq.9\)</span>，来利用 long-term context for classification。</p><p><font color=red>Action Switch regression head</font> <spanclass="math inline">\(T_{out}\)</span> bboxes in a tubelet是用一个 FClayer同时进行回归。</p><p><spanclass="math display">\[y_{\mathrm{coor}}=\mathrm{Linear}_{\mathrm{b}}(F_{\mathrm{tub}}),\]</span></p><p><span class="math inline">\(y_{\mathrm{coor}}\in\mathbb{R}^{N\timesT_{\mathrm{out}}\times4}\)</span>， <spanclass="math inline">\(N\)</span>是 action tubelet的数量， <spanclass="math inline">\(T_{out}\)</span>是一个action tubelet的temporallength。为了去掉tubelet里的non-action boxes。进一步用 FC layer来决定 abox 是否描述了tubelet里actor的行为。称之为 action switch。这个actionswitch 使得能够产生action tubelets witha more precise temporalextent。<span class="math inline">\(T_{out}\)</span> predicted boxes ina tubelet的概率是： <spanclass="math display">\[y_\mathrm{switch}=\mathrm{Linear}_\mathrm{s}(F_\mathrm{tub}),\]</span></p><p><span class="math inline">\(y_\mathrm{switch}\in\mathbb{R}^{N\timesT_\mathrm{out}}\)</span>，对于每个预测的tubelet, each of its <spanclass="math inline">\(T_{out}\)</span> bboxes 包含一个action switchscore。</p><p><font color=red> Losses</font>：4个loss的线性组合是： <spanclass="math display">\[\mathcal{L}=\lambda_{1}\mathcal{L}_{\mathrm{switch}}(y_{\mathrm{switch}},Y_{\mathrm{switch}})+\lambda_{2}\mathcal{L}_{\mathrm{class}}(y_{\mathrm{class}},Y_{\mathrm{class}})\\+\lambda_{3}\mathcal{L}_{\mathrm{box}}(y_{\mathrm{coor}},Y_{\mathrm{coor}})+\lambda_{4}\mathcal{L}_{\mathrm{iou}}(y_{\mathrm{coor}},Y_{\mathrm{coor}}),\]</span></p><p><span class="math inline">\(y\)</span>是模型的输出，<spanclass="math inline">\(Y\)</span>表示ground truth，action switch loss<span class="math inline">\(L_{switch}\)</span> 是一个binarycross-entropy loss，<span class="math inline">\(L_{class}\)</span> crossentropy loss，<span class="math inline">\(L_{box}\)</span> 和 <spanclass="math inline">\(L_{iou}\)</span> 表示per-frame bboxes matchingerror。当 <span class="math inline">\(T_{out} &lt; T_{in}\)</span>，tubelet是sparse，coordinate ground truth <spanclass="math inline">\(Y_{coor}\)</span>是来自对应的时序下采样的framesequence。用 匈牙利匹配，根据经验，设置参数 <spanclass="math inline">\(\lambda_{1}=1, \lambda_{2}=5, \lambda_{3}=2,\lambda_{4}=2\)</span>。</p></li><li><p>消融实验：</p><ul><li><strong>benefit of tubelet queries</strong>：在实验中发现了tubeletquery sets的好处，每个query set是由 <spanclass="math inline">\(T_{out}\)</span> per-frame query embeddings组成，能够在各自的frame上预测spatial location of the action。将其和single query embedding which represents a whole tubelet and must regress<span class="math inline">\(T_{out}\)</span> box locations for allframes in the clip.进行对比。结果是要好一些，证明了modeling actiondetection as a sequence-to-sequencetask，能够有效地利用transformer的架构。</li><li><strong>effect of tubelet attention</strong>：tubeletattention相比于典型的self-attention，能够节省memory。</li><li><strong>benefic of action switch</strong>：actionswitch能够精确地判断action的temporal start and end。没有actionswitch，TubeR会将transitional states误分类为actions。</li><li><strong>effect of short and long term contexthead</strong>：在AVA数据集上有很好的性能提升，网络能够<strong>seeingfull context of the clip</strong>。</li></ul></li><li><p>局限：</p><ul><li>3D backbone会占用很大的memory和计算量，限制了在长视频上应用TubeR。近期的工作是将transformer的encoder用于videoembedding，会占用较少的memory。</li><li>如果in one pass处理一个长视频，需要足够的queries来cover视频中per-person的不同的最多的行为数量。这会造成在自注意力层中，需要大量的queries，造成memory问题。一个可能的解决方法是产生persontubelets而不是 actiontubelets。因此当一个新的action发生的时候，不需要splittubelets。对于每个person instance，只需要一个query。</li></ul></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/e36c35f3b3d898898a1b63817ce70397555cef76/1-Figure1-1.png"alt="tube" /><figcaption aria-hidden="true">tube</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/e36c35f3b3d898898a1b63817ce70397555cef76/3-Figure2-1.png"alt="structure of TubeR" /><figcaption aria-hidden="true">structure of TubeR</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;tuber-tubelet-transformer-for-video-action-detection1&quot;&gt;TubeR:
Tubelet Transformer for Video Action Detection&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自阿姆斯特丹大学、罗格斯大学和AWS AI Labs的Jiaojiao
Zhao、Yanyi Zhang等人。论文引用[1]:Zhao, Jiaojiao et al. “TubeR: Tubelet
Transformer for Video Action Detection.” 2022 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (2021): 13588-13597.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2021.April&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;tubelet queries, tubelet-attention&lt;/li&gt;
&lt;li&gt;in sequence-to-sequence manner&lt;/li&gt;
&lt;li&gt;scales well to longer video clips&lt;/li&gt;
&lt;li&gt;end-to-end without person detectors, anchors or proposals&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;不同于现有的依赖于离线检测器或者人工设计的actor-positional
hypotheses like proposals or anchors，提出了一个通过同时进行action
localization和recognition from a single
representation，直接检测视频里的action
tubelet的方法。TubeR学习一系列的tubelet queries，利用tubelet-attention
module来model video clip里的动态的spatio-tempral
nature。相比于用actor-positional hypotheses in the spatio-temporal
space，它能够有效的强化模型的能力。对于包含transitional states或者scene
changes的视频，提出了一个context aware classification
head，来利用short-term和long-term context to strengthen action
classification，和一个action switch regression head
来检测精确的时序上的行为范围。TubeR直接产生不同长度的action
tubelets，对于长的视频clips，也能保持一个比较好的结果。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>EVAD</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/EVAD/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/EVAD/</id>
    <published>2024-08-24T08:51:19.000Z</published>
    <updated>2024-08-27T07:18:15.529Z</updated>
    
    <content type="html"><![CDATA[<h3id="efficient-video-action-detection-with-token-dropout-and-context-refinement1">EfficientVideo Action Detection with Token Dropout and ContextRefinement<sup>[1]</sup></h3><blockquote><p>作者是来自nju、蚂蚁集团、复旦和上海AI Lab的Lei Chen、ZhanTong、Yibing Song等人。论文引用[1]:Chen, Lei et al. “Efficient VideoAction Detection with Token Dropout and Context Refinement.” 2023IEEE/CVF International Conference on Computer Vision (ICCV) (2023):10354-10365.</p></blockquote><h3 id="time">Time</h3><ul><li>2023.Aug</li></ul><h3 id="key-words">Key Words</h3><ul><li>spatiotemporal token dropout</li><li>maintain all tokens in keyframe representing scene context</li><li>select tokens from other frames representing actor motions</li><li>drop out irrelavant tokens.</li></ul><h3 id="总结">总结</h3><ol type="1"><li>视频流clips with large-scale vieo tokens 阻止了ViTs for efficientrecognition，特别是在video actiondetection领域，这是需要大量的时空representations来精确地actoridentification。这篇工作，提出了端到端的框架 <strong>for efficient videoaction detection(EVAD) based on vanillaViTs</strong>。EVAD包含两个为视频行为检测的特殊设计。首先：提出来时空tokendropout from a keyframe-centric perspective. 在一个video clip中，mainall tokens from its keyframe，保留其它帧中和actormotions相关的tokens。第二：通过利用剩余的tokens，refine scene contextfor better recognizing actor identities。actiondetector中的RoI扩展到时间域。获得的时空actor identity representationsare refined via scene context in a decoder with the attentionmechanism。这两个设计使得EVAD高效的同时保持精度。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>image patches视为ViT输入的tokens for自注意力的计算。当识别一个video clip的时候，tokens来自每个frame，形成大规模的input for ViTs，这些videotokens会在训练和推理的时候引入很多计算，特别是在计算自注意力的时候。有一些尝试来减少visiontokens for fast computations。对于video action detection任务来说，平衡精度和效率是个挑战。这是因为在VAD中，需要定位每一帧中的actors，视频序列中的temporalmotion需要保持 for consistent identification。同时，scene context应该被保留以区别其它actors。大量的表示actor motions和scenecontext的video tokens会保持VAD的精度。这篇文章中，保留表示actormotions和scene context的video tokens，同时dropping out不相关的tokens。<strong>基于视频clips的时序连贯性，从keyframe-centric的角度，提出来时空tokendropout</strong>。<strong>对于每个视频clip,选择代表scenecontext的keyframe，这里的所有tokens将被maintained。同时，从其它代表actormotions的帧中选择tokens。另外，drop out 这个clip中的剩余的videotokens</strong>。</p><p><font color=red>通过一个keyframe-centric token pruning module withthe ViT encoder backbone，实现spatiotemporal tokendropout。这个keyframe要么是均匀采样得到的，要么人为在videoclips中定义的。默认是选择input clip中带有box annotations的middleframe</font>。另外，提取由这个keyframe增强的attention map。这个attentionmap在non-keyframe中知道tokedropout。在定位之后，需要分类每个定位好的bbox for actoridentification。当video tokens在non-keyframes中是不完整的时候，分类的性能在bbox regions (where tokenshave been droppedout)中是较差的。然而，视频的内在时序一致性使得能够refine bothactor和scene context from the remaining videotokens。扩展时域上的localized bboxes for RoIAlign，来获得与actormotion相关的tokenfeatures。然后，引入一个decoder，通过这个clip中的剩余videotokens的指导，来refine actor features。这个decoder concatenatesactor和token features，进行自注意力操作来产生丰富的actor features forbetter identification。在token dropout之后，退化的actionclassification能够用剩余的video tokens for contextrefinement来恢复。这个恢复的性能和用整个video tokens for actionclassification是一样的。通过这个context refinement，用减少了的videotokens保持了VAD的性能。</p></li><li><p>相关工作:</p><ul><li><strong>Spatio-temporal ActionDetection</strong>：当前的SOTA方法采用两个分开的backbones、2个stagespipelines。例如2D backbone for actor localization on keyframes，3Dbackbone for video featureextraction.之前的方法通过端到端的方式训练这两个backbone，来简化pipeline。这个会导致很高的复杂度和优化困难。最近的方法用统一的一个backbone来进行actiondetection。 VAT是一个transformer风格的actiondetector，来汇聚目标actor附近的时空上下文。WOO和TubeR是query-based的actiondetectors，来预测actor bboxes和actionclasses。STMixer是一个但阶段的query-based detector，来adaptively samplediscriminativefeatures。几个新的基于transformer的方法，用ViT的变种backbone，用两阶段的pipeline得到了一个比较好的结果。</li><li><strong>Spatio-temporal Redundancy</strong>：<ol type="1"><li>SpatialRedundancy：<strong>DynamicViT</strong>观察到，精确的图像识别是基于最富有信息的tokens的子集，设计了一个dynamictokensparsification的框架，来剪掉多余的tokens。<strong>EViT</strong>计算了classtoken to each token的attentiveness，确定了top-k tokens using theattentiveness value。<strong>ATS</strong>引入了一个differentiableadaptive token sampler for adaptively sampling 重要的tokens based onimage content。</li><li>Spatio-temporalredundancy：由于视频数据集的高冗余性，有很多的研究关注于开发高效的视频识别。最近的方法展示了高效的schemesspecialized for videotransformers。<strong>MAR</strong>提出了一个人为设计的masking策略，来丢弃 部分patches，减少videotokens。<strong>K-centered</strong>提出了patch-basedsampling，超越了传统的frame-basedsampling。<strong>STTS</strong>用transformer在时间和空间上sequentially选择tokens。</li></ol>为了和这些方法对比，考虑keyframe和邻近帧的correlation,来drop out冗余的tokens，然后执行端到端的基于transformer的action detector。</li></ul></li><li><p>Method：EVAD使encoder with token pruning来去掉多余的tokens，decoder来refine actor spatiotemporalfeatures。和<strong>WOO</strong>设置相同，利用encoder中的keyframe的多个中间的spatialfeature maps for actor localization，最后一个encoderlayer输出的spatiotemporal feature map 用来做action classification。</p><ul><li><strong>Keyframe-centric TokenPruning</strong>：相邻帧有着相似语义信息的video是高度冗余的，使得用一个比较高的dropoutrate on video transformers来做tokenpruning是可能的。这里，将所有的spatiotemporal tokens分成keyframetokens和non-keyframe tokens。keyframe中，保留所有的tokens for accurateactor localization。</li></ul><ol type="1"><li><font color=red>Non-keyframe token pruning</font>：用<strong>EViT</strong>中的方法，预先计算一个attentionmap来表示每个token的重要性，不需要额外的learnableparameters和计算开销。首先，average attentionmap的<em>num_heads</em>维度，来得到一个 <span class="math inline">\(N\times N\)</span>的matrix，表示tokens之间的<strong>attentiveness</strong>。因为没有classification token forvideo-level recognition，所以计算每个token的平均重要性分数 <spanclass="math inline">\(I_{j}=\frac{1}{N}\sum_{i=1}^{N}attn(i,j)\)</span>。然后从<span class="math inline">\(N_2\)</span> non-keyframetokens中，通过重要性分数的降序排列，找到 top(<spanclass="math inline">\(N \times \rho - N_1\)</span>) tokens，这里<spanclass="math inline">\(N, N_1, N_2\)</span>分别表示所有的tokens、keyframetokens和non-keyframe tokens。<span class="math inline">\(\rho\)</span>表示 token keepingrate，通常情况下，keyframe包含当前样本的最精确的语义信息，其它帧会带来信息bias。guidedby keyframe进行token pruning是符合实际的。为了这个目的，在acquiringattention map和计算non-keyframe token重要之间插入一个 <strong>KeyframeAttentiveness Enhancement</strong> step。用一个greater weight value tokeyframe queries，保留和keyframetokens高关联的tokens。每个token的重要性分数是这样更新的：</li></ol><p><span class="math display">\[I_j=\frac{1}{N}\sum_{i=1}^N\begin{cases} w_{tf} \cdot attn(i,j),&amp;i \in(0,N_1)\\\\attn(i,j),&amp;i \in (N_1,N)\end{cases}\]</span></p><p>假设first <span class="math inline">\(N_1\)</span>tokens属于keyframe，weigth value <spanclass="math inline">\(w_{kf}\)</span>是一个超参数。丢掉仅和non-keyframes 有highresponse的tokens，这些可能不是高质量的tokens。仅当non-keyframe变成了previousor next samples的keyframe的时候，这些highly responsivetokens才是高质量的。通过dropout这些冗余的tokens，进一步减少tokens的数量；做完tokenpruning之后，将这些保留的tokens给到之后的FFN。</p><p>第一次token pruning是在encoder layers 的1/3开始，之后，每个 1/4的total layers进行一次 token pruning，丢弃冗余的tokens，保留effectiveones。</p><ul><li><strong>Video Action Detection</strong>：</li></ul><ol type="1"><li><p><font color=red>Actor localization branch</font> 得到了keyframetokens，就能得到多个完整的keyframe feature maps。然后对这些featuremaps进行上采样或者下采样，来从palin ViT中产生hierarchicalfeatures。受Sparse RCNN的启发，引入了query-based actor localizationhead，来检测keyframe中的actors。actor localization branch的输出是 <spanclass="math inline">\(n\)</span>个 prediction boxes in thekeyframe和对应的actor confidence scores。</p></li><li><p><font color=red>Action Classification branch</font>不同于传统的特征提取，EVAD产生<spanclass="math inline">\(M\)</span>个离散的video tokens。需要恢复videofeature map的时空结构，然后进行location-related操作例如RoIAlign。初始化blank feature map shape as <spanclass="math inline">\((T/2,H/16,W/16)\)</span>，用保留的tokens来填充这个featuremap according to 它们对应的时空位置，剩余的用0进行pad。</p></li></ol><p>然后，用localization branch产生的boxes，通过3D RoIAlign来提取actorRoI features for subsequent action prediction。由于actormovement或者相机的变化，actor的空间位置是逐帧变化的，用keyframe box for3D RoIAlign不能得到partial actor feature deviated from thebox。直接扩展scope of the box来cover整个motiontrajectory，可能会引入背景或者其它的干扰信息，对actor featurerepresentation不利。然而，在EVAD featureextraction阶段，<strong>视频中的干扰项会被逐步去掉，因此可以扩展scope ofbox来增加deviated feature</strong>. <strong>观察发现：</strong>直接用vision classification的token pruning方法能减少计算中的tokens的数量，但是会对最后的检测性能有负面的影响。Videoaction detection要localizing和classifying actions of allactors，但是token pruning算法会导致时空上不连贯的actorfeatures**。在encoder中，pair-wise self-attention能够modeling globaldependency among tokens，actor regions内的dropouttokens的语义信息能够被incorporated into some preservedtokens，因此能够从保留的video tokens中恢复去掉的actorfeatures。为了这个目的，设计了一个<font color=red>context refinementdecoder</font>来refine actor的spatiotemporalrepresentation。具体地说：concatenate <spanclass="math inline">\(M\)</span>个video tokens的 <spanclass="math inline">\(n\)</span>个actor RoIfeatures，然后送到deocder中，guiding by 保留的tokens，actor features canenrich themselves with actor represntation and motion information fromother frames。没有token pruning，decoder会被用来作为relational modelingmodules，来得到inter-actor和actor-context的interaction information。</p><p>decoder输出的<span class="math inline">\(n\)</span>个refined actorfeatures are retrieved，然后通过一个classification layer，做最后的actionprediction。</p></li><li><p>实验：</p><ul><li><strong>RoI extension</strong>：由于人的largemotion，从keyframe中得到的box不能cover整个motiontrajectory，Intuitively，通过适当地扩展box scope来解决这个问题。pruningmechanism能够消除extension带来的interference information。结合了RoIextension的pruning能够在一定程度上消除human movements的影响。</li><li>EVAD能够实现实时的推理in an end-to-end manner。</li><li>之前流行的model是VideoMAE，是一个两阶段的model，需要一个离线的persondetector来pre-compute personproposals。现在EVAD，用同样的预训练的backbone，能够获得和VideoMAE相当的性能。和其它端到端的模型例如<strong>WOO</strong>和<strong>TubeR</strong>，用明显的性能提升；比基于CNN的model有更快的推理速度,more friendly to real-time action detection。比STMixer的性能略微好一点，<strong>STMixer是最近的端到端的模型，设计了一个decoder来采样discriminativefeatures。EVAD is deivsed for efficient video featureextraction，进一步结合这两个或许会得到更好的检测性能</strong>。</li></ul></li><li><p>Conclusion：受videosequence的transformer的大量的计算开销和视频检测中高度冗余时空信息的的启发，通过<strong>droping out spatiotemporal tokens and refining scene context toenable efficient transformer-based actiondetection</strong>,设计了EVAD的方法。 EVAD的局限是：它需要<strong>retraining once to take the benefits of reduced computations andfaster inference from removing redundancy</strong>。一个潜在的方法是探索transformer-adaptive token pruning algorithms。另外，follow 端到端的框架<strong>WOO</strong>来验证<strong>EVAD的效率和有效性</strong>，但是<strong>WOO</strong>是一个两阶段的pipeline，sequentially执行actor localization和action classificationmodules。在未来的工作中，旨在将这两个modules集成到一个 unifiedhead，能够减小通过 detector head的推理时间，能够amplifyEVAD的高效的特点。</p></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/f3ea68b1948c43518259dc22dc121b2c9730103a/3-Figure2-1.png"alt="Structure" /><figcaption aria-hidden="true">Structure</figcaption></figure><p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Pipeline ofEVAD. Our detector consists of three parts: an encoder withkeyframe-centric token pruning for efficient video feature extraction, aquery-based localization branch using multiscale features of thekeyframe for actor boxes prediction, and a classification branchconducting actor spatiotemporal feature refinement and relationalmodeling between actor RoI features and compact context tokens from ViTencoder.</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/f3ea68b1948c43518259dc22dc121b2c9730103a/4-Figure3-1.png"alt="Keyframe-centric token pruning" /><figcaption aria-hidden="true">Keyframe-centric tokenpruning</figcaption></figure><p><span class="math inline">\(Figure \ 2^{[1]}\)</span>: Structure ofkeyframe-centric token pruning.We conduct token pruning within theoriginal encoder layer.Specifically, we calculate the importance scoresof non-keyframe tokens using a keyframe-enhanced attention map.Then, wepreserve the top-k important non-keyframe tokens concatenated withkeyframe tokens as the results</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;efficient-video-action-detection-with-token-dropout-and-context-refinement1&quot;&gt;Efficient
Video Action Detection with Token Dropout and Context
Refinement&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自nju、蚂蚁集团、复旦和上海AI Lab的Lei Chen、Zhan
Tong、Yibing Song等人。论文引用[1]:Chen, Lei et al. “Efficient Video
Action Detection with Token Dropout and Context Refinement.” 2023
IEEE/CVF International Conference on Computer Vision (ICCV) (2023):
10354-10365.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2023.Aug&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;spatiotemporal token dropout&lt;/li&gt;
&lt;li&gt;maintain all tokens in keyframe representing scene context&lt;/li&gt;
&lt;li&gt;select tokens from other frames representing actor motions&lt;/li&gt;
&lt;li&gt;drop out irrelavant tokens.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;视频流clips with large-scale vieo tokens 阻止了ViTs for efficient
recognition，特别是在video action
detection领域，这是需要大量的时空representations来精确地actor
identification。这篇工作，提出了端到端的框架 &lt;strong&gt;for efficient video
action detection(EVAD) based on vanilla
ViTs&lt;/strong&gt;。EVAD包含两个为视频行为检测的特殊设计。首先：提出来时空token
dropout from a keyframe-centric perspective. 在一个video clip中，main
all tokens from its keyframe，保留其它帧中和actor
motions相关的tokens。第二：通过利用剩余的tokens，refine scene context
for better recognizing actor identities。action
detector中的RoI扩展到时间域。获得的时空actor identity representations
are refined via scene context in a decoder with the attention
mechanism。这两个设计使得EVAD高效的同时保持精度。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>YOWO</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/YOWO/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/YOWO/</id>
    <published>2024-08-24T08:51:02.000Z</published>
    <updated>2024-08-27T07:15:14.850Z</updated>
    
    <content type="html"><![CDATA[<h4id="you-only-watch-once-a-unified-cnn-architecture-for-real-time-spatiotemporal-action-localization1">YouOnly Watch Once: A Unified CNN Architecture for Real-Time SpatiotemporalAction Localization<sup>[1]</sup></h4><blockquote><p>作者是来自Technical Univ of Munich的Okan Kopuklu, Xiangyu Wei,Gerhard Rigoll。论文引用[1]:Köpüklü, Okan et al. “You Only Watch Once: AUnified CNN Architecture for Real-Time Spatiotemporal ActionLocalization.” ArXiv abs/1911.06644 (2019): n. pag.</p></blockquote><h4 id="time">Time</h4><ul><li>2019.Nov.15(v1)</li><li>2021.Oct.18(v5)</li></ul><h4 id="key-words">Key Words</h4><ul><li>single-stage with two branches</li></ul><h4 id="总结">总结</h4><ol type="1"><li>当前的网络抽取时序信息和keyframe的空间信息是用两个分开的网络，然后用一个额外的mechanism来融合得到detections。YOWO是一个单阶段的架构，有两个分支，来同时抽取当前的时序和空间信息，预测bboxes和action的概率 directly from video clips in oneevaluation。因为架构是统一的，因此可以端到端的优化。YOWO架构速度快，能够做到在16-framesinput clips上做到 34 frames-per-second，62 frames-per-second on 8-framesinput clips。是当前在STAD任务上最快的架构。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>和静态图像里的目标检测相比，时序信息很重要，受目标检测FasterRCNN的启发，SOTA的工作将经典两阶段的网络架构扩展到actiondetection，第一阶段产生proposals，在第二阶段进行分类和定位的refinement，然而，两阶段的STAD任务有3个主要的缺点：(1):action tube是由bboxes acrossframes组成的，它的产生比2Dcase更复杂和耗时。分类的性能极度依赖这些Proposals，然而detectedbboxes可能对于后续的分类任务是sub-optimal；(2)actionproposals只关注视频里的人物的features，忽略人和背景中的其它特征，而这这能够提供对于actionprediction相当重要的上下文信息。(3)训练RPN网络和分类网络是分开的，不能保证找到全局最优。只有局部最优from the combination of two stages can be found.训练的成本比单阶段的高，因此花费很多时间和存储。</p></li><li><p>YOWO克服了上述提到的缺点，YOWO的本能的idea是来自人类视觉认知系统，为了理解视频中的人物的行为，需要<strong>将当前帧的信息2Dfeatures from key frame</strong>与之前记忆里获得的知识(3D features fromclip)相关联，之后，两种features融合到一起，提供一个合理的结论。YOWO架构是一个单阶段的、有两个分支的网络。一个分支<strong>提取keyframe的spatial features via a 2D-CNN，另一个分支models spatiotemporalfeatures of the clip consisting of previous frames via a 3DCNN</strong>。YOWO是一个<strong>causal架构(因果架构)，就是没有利用future frames</strong>，能够operate onlineon incoming video streams。为了aggregate 2D CNN和3D CNN的featuressmoothly, 用了一个channel fusion和attention机制，get the utmost out ofinter-channel dependencies。最后，用融合的特征，产生frame-leveldetections，用一个<strong>linking 算法</strong>来产生action tubes.YOWO不局限于RGB的模态，其它的例如光流也是可以的；任何一个CNN的架构根据实时性能的要求，都可以用。YOWOoperates with maximum 16 frames input，因为short cliplengths对于实现STAD人物的实时性是必要的。然而，small clipsize是时序信息累计的限制因素。因此，利用<strong>long-term featurebank</strong>，通过训练好的3DCNN从整个视频中提取非重叠的8帧片段的特征。在推理的时候，averaged 3Dfeatures centering the key-frame.</p></li><li><p>主要贡献如下：</p><ul><li>提出了在视频流里的单阶段的STAD框架，能够端到端的训练。实现了在3DCNN和2D CNN上特征的bboxes 回归，同时，这两个特征对于彼此是互补的 forfinal bboxes 回归和分类。用了<strong>channel attention</strong>来汇聚两个branches的特征。实验证明：channel-wise attention机制，modelsinter-channel relationship within the concatenated featuremaps，提高了性能。</li></ul></li><li><p>Related work: 为了考虑时序信息，有twe-streamCNN来提取分别提取空间和时间特征，然后汇聚到一起；这样的工作大部分是基于光流的，很耗时和耗计算资源。然后就是3DCNN，用它来提取时空特征。为了resource efficiency，一些工作用2DCNN来学习2D 特征，然后用一个3DCNN来将它们融合到一起，学习时间特征。Attention是一个有效的mechanism来capturelong-range dependencies，用在了CNNs中来尝试提高图像分类的性能。Attentionmechanism在<strong>spatial-wise和channel-wise</strong>来执行。<strong>spatialattention解决inter-saptial relationship among features，channelattention增强最有意义的channels，弱化其它的</strong>。作为一个channel-wiseattention block，Squeeze-and-Excitationmoduel对于提高CNN的性能有益。另一方面，对于视频分类人物，non-localblock考虑时空信息，来学习acrossframes的特征的dependencies。可以视为自注意力策略。不同于之前的工作，YOWO只使用clip一次，检测keyframe里的对应的actions。为了避免光流的复杂计算，用keyframe的2Dfeatures和clip的3D features。之后，两种类型的features用<strong>attentionmechanism</strong>融合在一起，就能够考虑到丰富的上下文信息。</p></li><li><p>YOWO的架构主要分为4个部分：<strong>3D CNNbranch</strong>、<strong>2D CNN branch</strong>、<strong>CFAM</strong>和 <strong>bbox regression parts</strong>.</p><ul><li><font color=red>3DCNN</font>：因为上下文信息对于人类行为理解很重要，因此用3DCNN来提取时空特征。3DCNN能够获得运动信息(在空间和时间维度做卷积)，基本的3D CNN架构这里用的是3D-ResNext-101，对于所有的3DCNN架构，在最后的卷积层之后的所有层都会被丢弃。3D网络的输入是一个videoclip。是由时间上连续的视频帧组成，shape为 <span class="math inline">\(C\times D \times H \times W\)</span>，最后一个3DResNext-101的输出的feature map的shape为 <spanclass="math inline">\(C&#39; \times D&#39; \times H&#39; \timesW&#39;\)</span>，<span class="math inline">\(C=3, D&#39;=1, H&#39; =\frac{H}{32}, W&#39; = \frac{W}{32}\)</span>，<spanclass="math inline">\(D\)</span> 是输入的帧数。输出特征图的depth维度减小到1，以至于 output volume可以squeezed to <spanclass="math inline">\(C&#39; \times H&#39; \timesW&#39;\)</span>，为了和2D-CNN的输出匹配。</li><li><font color=red>2D CNN</font>：为了解决空间定位问题，keyframe的2D特征被extracted in parallel，用Darknet-19作为2DCNN的基本架构，因为它在精度和效率上取得了平衡。key frame with the shape<span class="math inline">\(C \times H \times W\)</span> 是输入clip的<strong>most recentframe</strong>，因此不需要一个额外的dataloader，Darknet-19的输出特征图的shape为<span class="math inline">\(C&#39;&#39; \times H&#39; \timesW&#39;\)</span>，<span class="math inline">\(C= 3\)</span>，<spanclass="math inline">\(C&#39;&#39;\)</span> 是输出的channels，<spanclass="math inline">\(H&#39;= \frac{H}{32}, W&#39; =\frac{W}{32}\)</span>，和3DCNN的情况类似。YOWO的另一个重要特性是，它的2D CNN和3DCNN的分支能够被任意的CNN架构代替，使其更灵活。需要注意的是：<strong>虽然YOWO有两个分支，但是它是一个统一的架构，能够端到端的训练</strong>。</li><li><font color=red>Feature aggregation: Channel Fusion and AttentionMechanism(CFAM) </font>：让3D和2D网络的输出有相同的shape in the last twodimensions，以至于两个featuremaps能够简单地融合。用concatenation来融合两个featuremaps。因此，融合的feature map能够 <strong>encoders both motion andappearanceinformatin</strong>，然后这个融合的特征输到CFAM模块里，这个模块是基于<font color=red>Gram matrix</font>来映射 <strong>inter-channeldependencies</strong>。虽然<strong>Grammatrix</strong>最初是用来做<strong>styletransfer</strong>，最近用在了segmentationtask，这样一个注意力机制对于来自不同sources的feature的融合是有益的。能够提高性能。concatenatedfeature map <span class="math inline">\(A \in R^{(C&#39; + C&#39;&#39;)\times H \times W}\)</span>，可以被视为3D 和2D 信息的一个 <strong>abruptcombination</strong>，这忽略了它们之间的<strong>interrelationship</strong>。因此，将<spanclass="math inline">\(A\)</span>输给两个conv layers来产生新的特征图<span class="math inline">\(B \in R^{C \times H&#39; \timesW&#39;}\)</span>，之后，在特征图<spanclass="math inline">\(B\)</span>上进行一些操作。$F R^{C N} $是 featuremap <span class="math inline">\(B\)</span>reshape之后的tensor， <spanclass="math inline">\(N=H \timesW\)</span>，意味着每个channel中的features被 <strong>vectorized to onedimension</strong>。然后 对 <span class="math inline">\(F \in R^{C\times N}\)</span> 和它的转置 <span class="math inline">\(F^T \in R^{N\times C}\)</span> 进行 矩阵乘法，来得到 <strong>Gram Matrix</strong><span class="math inline">\(G \in R^{C \timesC}\)</span>，这个矩阵能够表明不同channel之间的<strong>correlations</strong>。</li></ul><p><spanclass="math display">\[\begin{array}{rcl}\mathbf{G}&amp;=&amp;\mathbf{F}\cdot\mathbf{F}^\mathrm{T}&amp;with&amp;G_{ij}&amp;=&amp;\sum_{k=1}^{N}F_{ik}\cdotF_{jk}\end{array}\]</span></p><p><strong>Gram matrix G</strong>中的每个元素 <spanclass="math inline">\(G_{ij}\)</span>代表 <strong>vectorize feature mapsi and j</strong> 之间的<strong>inner product</strong>。</p><p>在计算完Gram matrix之后，用一个softmax layer来得到 <strong>channelattention map M</strong>, <span class="math inline">\(M \in R^{C \timesC}\)</span>，<span class="math inline">\(M_{ij}\)</span>表示<spanclass="math inline">\(j^{th}\)</span> channel对 <spanclass="math inline">\(i^{th}\)</span> channel之间的的影响。因此，<spanclass="math inline">\(M\)</span> summaries 给定featuremap的features的<strong>inter-channel dependency</strong>。为了performimpact of attention map to original features，对<spanclass="math inline">\(M\)</span>和<spanclass="math inline">\(F\)</span>做一个矩阵乘法，将结果shape到3维空间<span class="math inline">\(R^{C \times H \timesW}\)</span>，这将和输入的tensor有相同的shape。</p><p><span class="math display">\[\mathbf{F}^{\prime} =\mathbf{M}\cdot\mathbf{F}\\\mathbf{F}^{\prime}\in\mathbb{R}^{C\timesN}\xrightarrow{reshape}\mathbf{F}^{\prime\prime}\in\mathbb{R}^{C\timesH\times W}\]</span></p><p>channel attention module的输出 <span class="math inline">\(C \in R^{C\times H \times W}\)</span>是 <spanclass="math inline">\(F&#39;&#39;\)</span>和初始输入特征图 <spanclass="math inline">\(B\)</span> 的的结合，with a trainable scalarparameter <span class="math inline">\(\alpha\)</span>，用<strong>element-wise sum operation</strong>，<spanclass="math inline">\(\alpha\)</span> <strong>learns a weight from0</strong>。 <span class="math display">\[\mathbf{C} =\alpha\cdot\mathbf{F}^{\prime\prime}+\mathbf{B}\]</span></p><p>该方程表明：每个channel的最后的特征是 <strong>weighted sum of thefeatures of all channels and original features</strong>，这是对featuremaps之间的long-range的semantics dependencies的建模。最后 <spanclass="math display">\[\mathbf{C}\in\mathbb{R}^{C\times H^{\prime}\timesW^{\prime}}\]</span> 给到了两个conv layer，来得到 <em>CFAM module</em>的输出特征图 <spanclass="math display">\[\mathbf{D}\in\mathbb{R}^{C^{*}\timesH^{\prime}\times W^{\prime}}\]</span>，在CFAM模块的开始和结束的2 个convlayer很重要，因为它们 <strong>mix the features from differentdistributions</strong>，没有这些convlayers，CFAM模块的性能提升会有限。</p><p>这样的一个架构 promote feature representativeness in terms of<strong>inter-dependencies amongchannels</strong>，因此来自不同branches的features能够被reasonably andsmoothly汇聚到一起。另外，<strong>Gram matrix</strong> 考虑整个 featuremap。两个flattened feature vectors的<strong>点乘</strong> 展示了<strong>它们之间的relationinformation</strong>。一个比较大的product表明两个 channels的features<strong>more correlated</strong>，smallerproduct表明它们彼此不一样。对于一个给定的channel，<strong>allocate moreweights to other channels which are much correlated and have more impactto it</strong>。通过这种机制，上下文的关系被<strong>emphasized，features discriminability is enhanced</strong>。</p><ul><li><font color=red> Bounding box regression</font>：followYOLO相同的guidelines for bbox regression。最后一个conv layer with <spanclass="math inline">\(1 \times 1\)</span> kernels用来产生<strong>desired number of output channels。对于每个grid cel in <spanclass="math inline">\(H&#39; \times W&#39;\)</span>。用 k-means方法在对应的datasets上选择5个prior anchors, with </strong>NumCls classconditional action scores, 4 coordinates and confidencescore**。YOWO的最后的输出的size是 <spanclass="math inline">\([(5\times(NumCls+5))\times H&#39;\timesW&#39;]\)</span>。bboxes的回归然后基于这些anchors进行refined。</li></ul></li><li><p>在训练和测试阶段的输入分辨率都为 <span class="math inline">\(224\times 224\)</span>，用不同的分辨率进行multi-scaletraining在实验中没有发现有性能的提高。损失函数和原始的YOLOv2的网络中类似，除了这个采用了smooth L1 Loss with beta=1 for localization，</p><p><spanclass="math display">\[L_{1,smooth}(x,y)=\begin{cases}0.5(x-y)^2&amp;if|x-y|&lt;1\\\\|x-y|-0.5&amp;otherwise\end{cases}\]</span></p><p><span class="math inline">\(x,y\)</span>分别指prediction和groundtruth。L1 loss相比于MSE loss，对outliers不那么敏感，能够在某些情况阻止梯度爆炸。用MSE loss forconfidence scores.</p><p><span class="math display">\[L_{MSE}(x,y)=(x-y)^2\]</span></p><p>最后的detection loss是 individual coordiante losses forx,y,width,height和confidence score loss，</p><p><spanclass="math display">\[L_D=L_x+L_y+L_w+L_h+L_{conf}\]</span></p><p>用focal loss for classification:</p><p><span class="math display">\[L_{focal}(x,y)=y(1-x)^\gammalog(x)+(1-y)x^\gamma log(1-x)\]</span></p><p>x是 softmaxed network prediction, <span class="math inline">\(y \in{0,1}\)</span> is grouth truth class label。<spanclass="math inline">\(\gamma\)</span>是modulating factor，reduce loss ofsamples with high confidence(easy samples), increase the loss of sampleswith low confidence(hardsamples)。AVA数据集是一个多标签数据集，每个人执行一个poseaction和多个human-human or human-object interaction actions。因此，用<strong>softmax to pose classes and sigmoid to the interactionactions</strong>。另外，AVA是一个不平衡的数据集，modulating factor <spanclass="math inline">\(\gamma\)</span>不足以处理数据集的不平衡问题。因此用了一个focal loss的 <spanclass="math inline">\(\alpha -balanced variant\)</span>，对于 <spanclass="math inline">\(\alpha\)</span>，<strong>we have used exponentialof class sampleratios</strong>。最后的YOWO用的loss是检测的loss和分类的loss的和。</p><p><span class="math display">\[L_{final}=\lambdaL_D+L_{Cls}\]</span></p><p>这里 <span class="math inline">\(\lambda =0.5\)</span>在实验中表现最好。</p></li><li><p>分别初始化3D 和2D CNN网络：用在Kinetics上预训练的models来初始化3D CNN，用在PASCAL VOC上预训练的models来初始化2D CNN。虽然架构是由2DCNN和 3D CNN组成。这些参数能够一起更新。选择 <strong>mini-batch SGD withmomentum and weight decay</strong>来优化loss function。学习率初始为0.0001。在训练的时候，由于J-HMDB-21的样本数量少，冻结所有的 3D convnet的参数，因此收敛会更快，减小过拟合的风险。另外，在训练中用一些数据增强的方式例如flipping,random scaling等。</p></li><li><p><strong>linking strategy</strong>：在得到了frame-level的 actiondetection之后，下一步是将这些检测到的 bboxes 连接起来，构建<strong>action tubes in the whole video</strong>。利用连接算法，来找到最优的video-level action detections。 假设 <spanclass="math inline">\(R_t\)</span>和 <spanclass="math inline">\(R_{t+1}\)</span>是连续帧 <spanclass="math inline">\(t\)</span> 和 <spanclass="math inline">\(t+1\)</span>的两个区域。linking score for anaction class <span class="math inline">\(c\)</span> 定义为： <spanclass="math display">\[\begin{aligned}s_{c}(R_{t},R_{t+1})&amp;=\quad\psi(ov)\cdot[s_{c}(R_{t})+s_{c}(R_{t+1})\\&amp;+\alpha\cdots_{c}(R_{t})\cdot s_{c}(R_{t+1})\\&amp;+\beta\cdotov(R_{t},R_{t+1})]\end{aligned}\]</span></p><p><span class="math inline">\(s_{c}(R_{t})\)</span>和<spanclass="math inline">\(s_{c}(R_{t+1})\)</span> 是regions <spanclass="math inline">\(R_t\)</span>和 <spanclass="math inline">\(R_{t+1}\)</span>的 class specific socres。<spanclass="math inline">\(ov\)</span>是两个区域的IoU，如果overlap存在，则<spanclass="math inline">\(\psi(ov)\)</span>为1，否则为0.在linkingscore的定义上增加了一个额外的项：<span class="math inline">\(\alpha\cdots_{c}(R_{t})\cdots_{c}(R_{t+1})\)</span>。将两个连续帧的剧烈变化考虑进来了，能够提高videodetections的性能。在计算出所有的linking scores之后，用 <strong>Viterbialgorithm</strong>来找到最有的路径，生成 action tubes。</p></li><li><p><strong>long-term featurebank</strong>：虽然YOWO的推理是在线的和因果的 with small clipsize，但是16帧的输入限制了 temporal information required for actionunderstanding。因此，利用long-term featureback(LFB)，这个包含了不同的timestamps的来自3DCNN的features。在推理时，3D features centering the keyframe areaveraeged and resulting feature map用作输入，给到CFAM block，<strong>LFBfeatures are extracted for non-overlapping 8-frames clips using thepretrained 3D ResNeXt-101 backbone</strong>。用 8 features centering thekey-frame。因此在推理的时候，利用了总共64帧的数据。LFB增加了actionclassificatin的性能，类似于difference between clip accuracy and videoccuracy in video datasets。然后，LFB会导致一个非因果的架构，因为 future3D features在推理的时候用到了。</p></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/08c936517b8e8b9fcab9a544c735be2909fab3ee/8-Figure2-1.png"alt="YOWO" /><figcaption aria-hidden="true">YOWO</figcaption></figure><p><span class="math inline">\(Figure \ \ 1^{[1]}\)</span>: The YOWOarchitecture. An input clip and corresponding key frame is fed to a 3DCNN and 2D-CNN to produce output feature volumes of <spanclass="math inline">\([C&#39;&#39; × H&#39; × W&#39;]\)</span> and <spanclass="math inline">\([C&#39; × H&#39; × W&#39;]\)</span>,respectively.These output volumes are fed to channel fusion and attention mechanism(CFAM) for a smooth feature aggregation. Finally, one last conv layer isused to adjust the channel number for final bounding boxpredictions.</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/08c936517b8e8b9fcab9a544c735be2909fab3ee/9-Figure3-1.png"alt="Channel Fusion" /><figcaption aria-hidden="true">Channel Fusion</figcaption></figure><p><span class="math inline">\(Figure \ \ 2^{[1]}\)</span>: Channelfusion and attention mechanism for aggregating output feature mapscoming from 2D-CNN and 3D-CNN branches</p>]]></content>
    
    
    <summary type="html">&lt;h4
id=&quot;you-only-watch-once-a-unified-cnn-architecture-for-real-time-spatiotemporal-action-localization1&quot;&gt;You
Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal
Action Localization&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自Technical Univ of Munich的Okan Kopuklu, Xiangyu Wei,
Gerhard Rigoll。论文引用[1]:Köpüklü, Okan et al. “You Only Watch Once: A
Unified CNN Architecture for Real-Time Spatiotemporal Action
Localization.” ArXiv abs/1911.06644 (2019): n. pag.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;time&quot;&gt;Time&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;2019.Nov.15(v1)&lt;/li&gt;
&lt;li&gt;2021.Oct.18(v5)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;key-words&quot;&gt;Key Words&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;single-stage with two branches&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;总结&quot;&gt;总结&lt;/h4&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;当前的网络抽取时序信息和keyframe的空间信息是用两个分开的网络，然后用一个额外的mechanism来融合得到detections。YOWO是一个单阶段的架构，有两个分支，来同时抽取当前的时序和空间信息，预测bboxes和action
的概率 directly from video clips in one
evaluation。因为架构是统一的，因此可以端到端的优化。YOWO架构速度快，能够做到在16-frames
input clips上做到 34 frames-per-second，62 frames-per-second on 8-frames
input clips。是当前在STAD任务上最快的架构。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Holistic Interaction Transformer</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/Holistic-Interaction-Transformer/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/Holistic-Interaction-Transformer/</id>
    <published>2024-08-24T01:24:04.000Z</published>
    <updated>2024-08-27T07:12:05.312Z</updated>
    
    <content type="html"><![CDATA[<h4id="holistic-interaction-transformer-network-for-action-detection1">HolisticInteraction Transformer Network for Action Detection<sup>[1]</sup></h4><blockquote><p>作者是来自国立清华大学和微软AI的Gueter Josmy Faure, Min-HungChen和Shang-Hong Lai.论文引用[1]:Faure, Gueter Josmy et al. “HolisticInteraction Transformer Network for Action Detection.” 2023 IEEE/CVFWinter Conference on Applications of Computer Vision (WACV) (2022):3329-3339.</p></blockquote><h3 id="time">Time</h3><ul><li>2022.Nov.18</li></ul><h3 id="key-words">Key Words</h3><ul><li>bi-modal structure</li><li>combine different interactions</li></ul><h3 id="总结">总结</h3><ol type="1"><li>行为是关于我们如何与环境互动的，包括其他人、物体和我们自己。作者提出了一个新的多模态的<strong>HolisticInteraction Transformer Network(HIT)</strong>，利用大量被忽略的、但是对人类行为重要的手部和姿态信息。HIT网络是一个全面的bi-modal框架，由一个RGBstream和pose stream组成。每个stream独立地建模person、object和handinteractions，对于每个子网络，用了一个<strong>Intra-Modality Aggregationmodule(IMA)</strong>，选择性地融合个体的交互。从每个模态的得到的features然后用一个<strong>AttentiveFusion Mechanism(AFM)</strong>进行融合，最后，从temporalcontext中提取cues，用cached memory来更好地分类存在的行为。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>一个合理的时空行为检测的框架旨在正确地label帧里的每个人。应该在相邻帧之间keep a link来更好地理解带有连续性特点的活动例如："open"、"close"。最近，更多鲁棒的工作来考虑spatialentities之间的关系，因为如果两个人在同一帧，他们大概率会和彼此互动。然而，仅用personfeatures是不足以获得object-related action。其他人try to understand不仅是帧里的人物之间的关系，而且还有他们周围的物体。这些方法有两个主要的缺点：(1)他们仅依赖于highdetectionconfidence的目标，可能会导致忽略重要的没有检测到的目标；(2)这些模型努力检测没有出现在frame里的和目标相关的actions。例如：考虑到行为："pointto (an object)",actor所指向的object可能不在当前帧中。</p></li><li><p>HIT网络用细粒度的上下文，包括personpose，hands，objects，来构建一个<font color=red> bi-modal interactionstructure </font>，每个modality包括3个主要的components：<font color=red>person interaction </font>，<font color=red> object interaction</font>，<font color=red>hand interaction</font>，每个component学习有价值的local actionpatterns。在从相邻帧学习时序信息之前，用<font color=red>Attentive FusionMechanism </font>结合不同模态的信息，来帮助更好地检测发生在当前帧中的行为。主要的contributions有：</p><ul><li>提出了新的框架、<strong>结合RGB、pose和hand features for actiondetection</strong></li><li>引入了一个<strong>bi-modal HIT网络，结合不同的interaction in anintuitive and meaningful way</strong></li><li>提出了一个<strong>Attentive FusionModuel</strong>，作为一个选择性地过滤器，保持每个modal中信息量最多的features，<strong>Intra-ModalityAggregator</strong>用来学习modalities内有用的actionrepresentations.</li></ul></li><li><p>Related work:</p><ul><li><p>spatio-temporal actiondetection，不同于将整个视频分为1类，需要在空间上和时间上检测到行为。很多近期的spatio-temporalaction detection上的工作是用3D CNN作为backbone来提取视频特征，然后用ROIpooling或者ROI align来crop person features from videofeatures，这么做，抛弃了视频中潜在的有用信息。</p></li><li><p>时空行为检测任务实际是一个<strong>交互建模</strong>的任务，大多数的行为是在和环境互动。很多研究是用<strong>attentionmechanism</strong>，有人提出了<strong>Temporal RelationNetwork(TRN)</strong>，学习帧间的依赖，或者说，interaction betweenentities from相邻帧。其它的方法进一步，不仅建模temporal，也建模spatialinteractions between different entities from the same frame.然而，选择什么entities来model interactions也是因model而异的，不只用humanfeatures，也有人用background information来model interactions between theperson in the frame and context. 选择crop persons'features，但是不抛弃剩下的backgroundfeatures，这样提供了丰富的信息，但是，可能会引入很多噪音。也有人尝试bemore selective about the features to use. 有人首先pass the video framesthrough an object detecotr, crop both the object and personfeatures，model它们的interactions。interaction的额外的层，提供了更好的representationsthan 单独的human interaction modeling models，能够helps with classesrelated to objects such as "work on acomputer"，然而，当目标太小没有检测到或者没有出现在frame中的时候，这些方法会fallshort。</p></li><li><p>很多最近的action detection frameworks仅用RGBfeatures，也有人用光流来获取motion，有人用inception-likemodel来concatenates RGB和flow features at the <font color=red> Mixed4blayer </font>，然而也有人用I3D 网络来分别得到RGB和flowfeatures，然后在actionclassifier之前，concatenate两个模态。作者提出的bi-modal方法，用了<font color=red>visual and skeleton-based features</font>，每个modality计算一系列的interactions includingperson、object、and hands before being fused。一个temporal interactionmodule用在fused features上，来学习global information regardingneighboring frames.</p></li></ul></li><li><p>Methods: HIT由<strong>RGB和posesubnetwork组成</strong>,每个旨在学习persions interactions with theirsurroundings by focusing on the key entities that drive most of ouractions。在融合了两个sub-networks的输出之后，进一步通过<font color=red>lookingat cached features from past and future frames，model how actions evolveintime</font>。这样全面的活动的理解方案能够帮助实现更好的行为检测性能。几个步骤：<strong>entityselection</strong>、<strong>RGB modality</strong>、<strong>posemodality</strong>、<strong>Attentive FusionModule(AFM)</strong>、<strong>Temporal Interaction Module</strong>.</p><ul><li><strong>entity selection</strong>: HIT由两个mirroring modalitieswith distinct modules组成，来学习不同类型的interactions。Humanactions大部分基于它们的pose、hand movements和interactions with theirsurroundings。基于这些观察，选择human poses 和handsbboxes作为模型的entities along with object and personbboxes。用<strong>Detectron</strong> for human posedetection，然后create a bbox 包围location of the person's hands.用<font color=red> Faster-RCNN</font> 来计算<strong>object bboxproposals</strong>。 Video feature extractor是一个 3D CNN backbone。poseencoder是一个轻量的<font color=red>spatialtransformer</font>，用ROIAlign 来trim videofeatures，来抽取person、hands和object features。</li><li><strong>RGB branch</strong>：RGBbranch包含3个components，每个包含一系列的操作，来学习目标person的特定的信息。这个object和handsinteraction modules model person-object and person-handsinteraction。person interaction moduel学习当前帧之间的persons的interaction。在每个interaction unit的heart，是要给<font color=red>cross-atttentincomputation</font>，这个<strong>query是target person(or the output ofthe previous unit), key and value are derived from the objects, or handsfeatures,depending on which moduele we are at</strong>.<font color=blue> it is like asking "how can these particular featureshelp detect what the target person is doing?"</font>，公式如下： <spanclass="math display">\[F_{rgb}=(A(\mathcal{P})\to z_{r}\toA(\mathcal{O})\to z_{r}\to A(\mathcal{H})\toz_{r})\\A(*)=softmax(\frac{w_q(\widetilde{P})\timesw_k(*)}{\sqrt{d_r}})\times w_v(*)\\z_{r}=\sum_{b}A(b)\timessoftmax(\theta_{b}),b\in(\widetilde{P},\mathcal{O},\mathcal{H},\mathcal{M})\]</span></li></ul><p><span class="math inline">\(d_r\)</span> 代表RGB features的dimensionchannel,<font color=red> <span class="math inline">\(w_q\)</span>、<spanclass="math inline">\(w_k\)</span>、<spanclass="math inline">\(w_v\)</span> project their inputs into query,keyand value</font>，<em>A(*)</em>是cross-attention机制，only takes personfeatures as input when computing person interaction <spanclass="math inline">\(A(P)\)</span>，对于hand interaction(objectsinteraction):只有两个sets的输入：output of <spanclass="math inline">\(z_r\)</span> which serves as query(<spanclass="math inline">\(\bar{P}\)</span>)、hands features(object features)from which we obtain the key and values.</p><p><span class="math inline">\(Z_r\)</span>是所有interactionmodules的加权和，包括termporal interaction module <spanclass="math inline">\(TI\)</span>，<spanclass="math inline">\(Z_r\)</span>很重要，首先，它允许网络aggregate尽可能多的信息；另外可学习的参数<spanclass="math inline">\(\theta\)</span>帮助过滤不同sets的features，hand-pickingthe best each of them has to offer while discarding noisy andunimportant information。</p><ul><li><strong>Pose branch</strong>：pose model类似于它的RGBcounterpart，reuses most of its outputs。首先通过一个轻量的transformerencoder <span class="math inline">\(f\)</span>来抽取pose features <spanclass="math inline">\(K&#39;\)</span> <spanclass="math display">\[\mathcal{K}^{\prime}=f(\mathcal{K})\]</span></li></ul><p>然后通过mirroring RGB modality的不同的constituents来计算 <spanclass="math inline">\(F_pose\)</span>，然后reusing 对应的outputs，<spanclass="math inline">\(P&#39;\)</span>、<spanclass="math inline">\(O&#39;\)</span>、<spanclass="math inline">\(H&#39;\)</span>是对应的outputs of <spanclass="math inline">\(A(P),A(O),A(H)\)</span>。 <spanclass="math display">\[F_{pose}=(A(\mathcal{K}^{\prime},\mathcal{P}^{\prime})\toz_{p}\to A(\mathcal{O}^{\prime})\to z_{p}\to A(\mathcal{H}^{\prime})\toz_{p})\\A(\mathcal{K}^{\prime},\mathcal{P}^{\prime})=softmax(\frac{w_{q}(\mathcal{K}^{\prime})\timesw_{k}(\mathcal{P}^{\prime})}{\sqrt{d_{p}}})\timesw_{v}(\mathcal{P}^{\prime})\]</span></p><p><spanclass="math inline">\(A(\mathcal{K}^{\prime},\mathcal{P}^{\prime})\)</span>计算cross-attentionbetween pose features <spanclass="math inline">\(K&#39;\)</span>和增强的person interaction features<span class="math inline">\(P&#39;\)</span>，这样的cross-modal blendenforces the pose features by focusing on the key correspondingattributes of the RGB features。其它的components，<spanclass="math inline">\(A(O&#39;)\)</span>和<spanclass="math inline">\(A(H&#39;)\)</span>对 <spanclass="math inline">\(Z_p\)</span>做一个线性的projection as query whiletheir key-value pairs stem from <span class="math inline">\(A(O) andA(H)\)</span>，<spanclass="math inline">\(Z_p\)</span>是要给intra-modality aggregationcomponent for the pose model,类似于 <spanclass="math inline">\(Z_r\)</span>，它过滤和汇聚了每个interactionmodule的信息。</p><ul><li><p>Attentive Fusion Module(AFM)：在进入action分类器之前，RGB和posestream需要结合到一个set of features。出于这个目的，提出了AttentionFusion Module，用<font color=red>channel-wise concatenation of twofeatures sets followed by self-attention for featurerefinement</font>。通过用project matrix <spanclass="math inline">\(\Theta_{fused}\)</span>来减小输出特征的值。 <spanclass="math display">\[F_{fused}=\Theta_{fused}(SelfAttention(F_{rgb},F_{pose}))\]</span></p></li><li><p>Temporal Interaction Unit：fusion module之后，就是一个temporalinteraction block <span class="math inline">\(TI\)</span>，humanactions是连续发生的，因此，long-term context对于理解行为是重要的，alongwith <span class="math inline">\(F_fused\)</span>，这个Module receives压缩的 memory data <span class="math inline">\(M\)</span> with length<span class="math inline">\(2S+1\)</span>，memory cache包含了videobackbone得到的person features. <spanclass="math inline">\(F_fused\)</span> inquirs <spanclass="math inline">\(M\)</span> as to which of the neighboring framescontains informative features, 然后absorb。<spanclass="math inline">\(TI\)</span>是另一个 cross-attention module where<span class="math inline">\(F_fused\)</span>是query,memory <spanclass="math inline">\(M\)</span>两个不同的projections构成了key-valuepairs. <spanclass="math display">\[F_{cls}=TI(F_{fused},\mathcal{M})\]</span>最后，分类头 <span class="math inline">\(g\)</span>是有两个 feed-forwardlayers with relu activation 和output layer组成的， <spanclass="math display">\[\hat{y}=g(F_{cls})\]</span></p></li></ul></li><li><p>实验：</p></li></ol><ul><li><p>person和object detector:从数据集中的每个视频抽取keyframes，然后用detected person bbox from(YOWO[16]) fo inference。作为一个object detector，用FasterRCNN withResNet-50-FPN作为backbone，模型是在ImageNet上进行的预训练，然后再MSCOCO上微调。</p></li><li><p>Keypoints Detection and Processing：对于keypoints detection，采用<span class="math inline">\(Detectron\)</span>中的posemodel，用在ImageNet for object detection上预训练 and fine-tuned onMSCOCO keypoints using precomputed RPN proposals的ResNet-50-FPN，targetdataset中的每个keyframe通过model，输出17个keypoints for each detectedperson corresponding to the COCO format.进一步后处理这些检测到的posecoordinates，因此match GT person bboxes(during training) and bboxes from<a href="during%20testing">16</a>。对于person handslocation，仅对人的手腕的部位的keypoints感兴趣，因此，对这个keypoints做一个bboxes，来highlight人物的hands和everything in between。</p></li><li><p>用SlowFast网络作为视频的backbone</p></li><li><p><strong>Limitations</strong>: 该框架依赖于离线的detector和poseestimator，因此detector和poseestimator的精度可能会对这个方法有影响。similar-lookingclasses例如："throw"和"catch"看起来很像；第二个是部分的occlusion。</p></li></ul><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/ea3763cbc7bc7b502dd50fe0a54b7d06d8d8163b/3-Figure2-1.png"alt="HIT Network" /><figcaption aria-hidden="true">HIT Network</figcaption></figure><p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Overview ofour HIT Network. On top of our RGB stream is a 3D CNN backbone which weuse to extract video features. Our pose encoder is a spatial transformermodel. We parallelly compute rich local information from bothsub-networks using person, hands, and object features. We then combinethe learned features using an attentive fusion module before modelingtheir interaction with the global context</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/ea3763cbc7bc7b502dd50fe0a54b7d06d8d8163b/3-Figure3-1.png"alt="Illustration of the Interaction module" /><figcaption aria-hidden="true">Illustration of the Interactionmodule</figcaption></figure><p><span class="math inline">\(Figure \ 2^{[1]}\)</span>: Illustrationof the Interaction module. ∗ refers to the module-specific inputs whilePe refers to the person features in <spanclass="math inline">\(A(P)\)</span> or the output of the module thatcomes before <span class="math inline">\(A(∗)\)</span></p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/ea3763cbc7bc7b502dd50fe0a54b7d06d8d8163b/3-Figure4-1.png"alt="Illustration of the Intra-Modality" /><figcaption aria-hidden="true">Illustration of theIntra-Modality</figcaption></figure><p><span class="math inline">\(Figure \ 3^{[1]}\)</span>: Illustrationof the Intra-Modality Aggregator. Features from one unit to the next arefirst augmented with contextual cues then filtered</p>]]></content>
    
    
    <summary type="html">&lt;h4
id=&quot;holistic-interaction-transformer-network-for-action-detection1&quot;&gt;Holistic
Interaction Transformer Network for Action Detection&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自国立清华大学和微软AI的Gueter Josmy Faure, Min-Hung
Chen和Shang-Hong Lai.论文引用[1]:Faure, Gueter Josmy et al. “Holistic
Interaction Transformer Network for Action Detection.” 2023 IEEE/CVF
Winter Conference on Applications of Computer Vision (WACV) (2022):
3329-3339.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2022.Nov.18&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;bi-modal structure&lt;/li&gt;
&lt;li&gt;combine different interactions&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;行为是关于我们如何与环境互动的，包括其他人、物体和我们自己。作者提出了一个新的多模态的&lt;strong&gt;Holistic
Interaction Transformer Network
(HIT)&lt;/strong&gt;，利用大量被忽略的、但是对人类行为重要的手部和姿态信息。HIT网络是一个全面的bi-modal框架，由一个RGB
stream和pose stream组成。每个stream独立地建模person、object和hand
interactions，对于每个子网络，用了一个&lt;strong&gt;Intra-Modality Aggregation
module(IMA)&lt;/strong&gt;，选择性地融合个体的交互。从每个模态的得到的features然后用一个&lt;strong&gt;Attentive
Fusion Mechanism(AFM)&lt;/strong&gt;进行融合，最后，从temporal
context中提取cues，用cached memory来更好地分类存在的行为。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Video Understanding</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/20/Video-Understanding/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/20/Video-Understanding/</id>
    <published>2024-08-20T06:12:07.000Z</published>
    <updated>2024-08-23T08:38:57.194Z</updated>
    
    <content type="html"><![CDATA[<h4id="视频理解及分析的计算机视觉任务">视频理解及分析的计算机视觉任务</h4><ol type="1"><li><p>之前看的时候，不管是论文还是一些博客，感觉都不是很清晰和全面，大家的定义不全面，特别是英文的名称上，这里写一下我的理解：</p></li><li><p>几个任务：</p><ul><li><strong>行为识别(Action Recognition)</strong>:实质是对视频的分类任务，可以类别图像领域的分类任务</li><li><strong>时序动作定位(Temporal Action Localization)</strong>:在时间上对视频进行分类，给出动作的起止时间和类别</li><li><strong>时空行为检测(Spatio-Temporal Action Detection)</strong>:不仅识别出动作出现的<strong>区间</strong>和<strong>类别</strong>，还要在空间范围内用一个boundingbox标记处目标的<strong>位置</strong>。</li><li><strong>还有人提出了时空动作定位(Spatio-temporal Actionlocalization)</strong>：和上一个是一样的</li><li>Action Detection在Paperswithcode上的定义： aims to find both whereand when an action occurs within a video clip and classify what theaction is taking place. Typically results are given in the form of<strong>action tublets</strong>, which are action bounding boxes linkedacross time in the video. <em>This is related to temporal localization,which seeks to identify the start and end frame of an action, and actionrecognition, which seeks only to classify which action is taking placeand typically assumes a trimmed video.</em></li><li>论文里还提到了<strong>temporal action segmentation</strong>：针对细粒度的actions和videos with dense occurrence of actions to predictaction label labels at every frame of the video.</li></ul></li><li><p>时空行为检测的算法：之前的论文都是都是基于行为识别(actionrecognition)的，很多都是基于早期的Slowfast的那个检测的方式：需要一个额外的检测器，实现行为检测。也就是在行为识别的基础上，再进行时空行为检测。但这并不是我理想中的方式，所以很多行为识别的算法，在AVA上也能上榜；最近看VideoMAE看了之后，就一直在看这个，没有去看看其它的。</p></li><li><p>Action Detection数据集：</p><ul><li>J-HMDB</li><li>UCF101-24</li><li>MultiSports</li><li>AVA</li><li>其中，JHMDB和UCF101-24是密集标注数据集(每一帧都标注，25fps)，这类数据集每个视频只有一个动作，大部分视频是单人做一些语义简单的重复动作；AVA为代表的稀疏标注数据集(隔一段时间标注一帧，1fps)，没有给出明确的动作边界</li></ul></li></ol><span id="more"></span><h4id="deep-learnign-based-action-detection-in-untrimmed-videos-a-survey">DeepLearnign-based Action Detection in Untrimmed Videos: A Survey</h4><blockquote><p>作者是来自纽约城市大学的Elahe Vahdani and YingliTian,论文引用[1]:Vahdani, Elahe and Yingli Tian. “Deep Learning-BasedAction Detection in Untrimmed Videos: A Survey.” IEEE Transactions onPattern Analysis and Machine Intelligence 45 (2021): 4302-4320.</p></blockquote><ol type="1"><li><p>很多action recognition 算法是在untrimmedvideo里，真实世界中的视频大部分是漫长的和untrimmed with sparse segmentsof interest. temporal activitydetection在没有剪辑的视频里的任务是定位行为的时间边界和分类行为类别。spatio-temporalactiondetection：action在temporal和spatial维度上都进行定位，还需识别行为的类别。因为长的未修剪的视频的标注费时费力，所以<strong>action detection with limited supervision</strong>是一个重要的研究方向。</p></li><li><p><strong>Temporal Action Detection</strong> 旨在 在untrimmedvideo里找到精确的时间边界和行为实例的label。依赖于训练集的标注的availability，可以分为：</p><ul><li>全监督 action detection: 时间边界和labels of action instances areavailable</li><li>弱监督action detection：only the video-level labels of actioninstances are available, the order of action labels can be provided ornot.</li><li>unsupervised action detection: no annotations for actioninstances</li><li>semi-supervised action detection: 数据被划分为小的子集 <spanclass="math inline">\(S_1\)</span> 和大的子集 <spanclass="math inline">\(S_2\)</span>，<spanclass="math inline">\(S_1\)</span> 中的视频是全标注的，<spanclass="math inline">\(S_2\)</span>中的视频没有标注(as infully-supervised)或者only annotated with video-level labels(asweakly-supervised)</li><li>self-supervised action detection:用一个代理任务从数据中抽取信息，然后用于提高性能，例如一般的自监督预训练，然后有监督微调。</li><li>Temporal action detection：或者说temporal action localization,思路和图像中的目标检测类似，会用到 proposals、RoIpooling这些类似的思路。</li></ul></li><li><p>Untrimmedvideos通常很长，由于计算资源的限制，很难直接把整个视频给到visual encoder来提取特征。通常的做法是把视频划分为相同大小的temporal intervals called<em>snippets</em>，然后对每个snippet都用visual encoder。</p></li><li><p><strong>Spatio-temporal Action Detection</strong>:有frame-levelaction detection和clip-level action detection</p><ul><li><strong>frame-level action detection</strong>:早期的方式是基于滑动窗口的一些扩展方法，要求一些很强的假设：例如cuboidshape，一个actor的跨帧的固定的空间范围。图像的目标检测启发了识别人类行为at frame level; 第一阶段，通过region proposal 或者 densely sampledanchors 产生action proposals，然后第二阶段proposals用于actionclassification和localization refinement. 在检测frames里的actionregions之后，一些方法，用光流来获取运动信息，<strong>用linkingalgorithm来连接frame-level bounding box into spatio-temporal actiontubes</strong>；有人用dynamic programming approach来连接 resultingper-frame detection，这个costfunction是基于boxes的检测分数和连续帧之间的重叠；也有人用tracking-by-detection方法来代替linkingalgorithm；另外一组是依赖于actionness measure, 例如pixel-wiseprobability of containing any action.为了估计actionness，它们用low-level cues例如光流；通过thresholding theactionness scores来抽取action tubes，这个输出是action的roughlocalization。这些方法的主要的缺点是没有完全利用视频的时序信息，检测是在每一帧上独立做的，<strong>有效的时序建模是很重要，因为当时序上下文信息是可用的时候，大量的actions才是可识别的。</strong></li><li><strong>Clip-level action detection</strong>: 通过在cliplevel执行action detection来利用时序信息。Kalogeiton提出了action tubeletdetector(ACT-detector)，输入为一系列的frames，输出action来别和回归的tubelets：系列的带有associatedscores的bounding box。tubelets被连接，来构造actiontubes。Gu等人进一步通过用longer clips和利用I3D pre-trained on thelarge-scale video dataset，展示了时序信息的重要性。为了生成actionproposals，把2D region proposals扩展到3D，假定spatialextent在一个clip内是固定的。随着时间的推移，用较大空间唯一的actiontubes将要违反假设，特别是当clip是很长，涉及actors或者camera的快速移动。</li><li><strong>modeling spatio-temporal dependencies</strong>:理解人类行为要求理解它们身边的人和物体。一些方法用图结构的网络、注意力机制来汇集视频中的物体和人的上下文信息。时空关系通过多层图结构的自注意力来学习，这个能够连接连续clips的entities，因此考虑long-rangespatial and temporal dependencies。</li><li>Metrics for Spatio-temporal action detection:<strong>frame-AP</strong>: measures the area under the precision-recallcurve of the detections for each frame. frame中的IoU大于某个阈值且actionlabel是正确的，则detection 是正确的。<strong>video-AP</strong>: measuresthe area under the precision-recall curve of the action tubespredictions。如果整个视频帧中，mean per frame IoU大于某个阈值且actionlabel预测正确，则tube 是正确的.</li></ul></li></ol><h4id="a-survey-on-deep-learning-based-spatio-temporal-action-detection"><strong>ASurvey on Deep Learning-based Spatio-temporal ActionDetection</strong></h4><blockquote><p>作者是来自浙大和蚂蚁集团的Peng Wang, Fanwei Zeng和YuntaoQian，论文引用[2]:Wang, Peng et al. “A Survey on Deep Learning-basedSpatio-temporal Action Detection.” ArXiv abs/2308.01618 (2023): n.pag.</p></blockquote><ol type="1"><li><p>Spatio-temporal action detection(STAD)旨在对视频中出现的行为进行分类，然后在空间和时间上进行定位。传统的STAD方式涉及到了滑动窗口，例如deformablepart models, branch and bound approach.模型主要划分为2类：frame-level和clip-level；<em>frame-level</em>预测 2Dbounding box for a frame; <em>clip-level</em>预测 3D spatio-temporaltubelets for a clip.</p></li><li><p><strong>Frame-level</strong>:目标检测做的很成功，研究人员将目标检测的模型泛化到STAD领域，直接的思路是：把STAD in video视为 2D image检测的集合。具体地说，在每一帧上用action detector来检测得到<strong>frame-level 2D bounding box</strong>。然后用<strong>linking ortracking算法</strong>关联这些frame-level detection results，生成<strong>3D action proposals</strong>。作者从<strong>Temporalcontext</strong>、<strong>3D CNN</strong>、<strong>High efficiency andreal-time speed</strong>、<strong>Visual RelationModeling</strong>这几个角度给出了相关的算法。</p><p>有些借鉴了RCNN、FasterRCNN的思路，用了RPN网络，然后用两个分支分别处理RGB和光流；然后融合外观和运动信息，Linkedup里得到 class-specific action tubes. 也有基于actionnessmaps的方法。<strong>actionness是指在图像的特定位置包含一般的actioninstance的可能性</strong>。上述这些STAD方法独立的对待frame，忽视了时序上下文关系。为了克服这个问题，有人提出了<strong>cascadeproposal and location anticipationmodel(CPLA)</strong>的方法，能够推理发生在两帧之间的运动趋势。用<em>frame <span class="math inline">\(I_t\)</span> 上检测到的bbox来推理<span class="math inline">\(I_{t+k}\)</span> frame上对应的 bbox。<spanclass="math inline">\(k\)</span>是 anticipation gap</em>.除了通过光流来获取视频里的运动特性之外，可以用 <font color=red> 3D CNN</font>来从多个相邻帧提取运动信息。后续的还有用<strong>X3D</strong>网络、<strong>ACDnet</strong>、将<strong>光流和RGB</strong>嵌入到一个单流网络中，利用光流来modulateRGB特征、用<strong>SSD</strong>作为检测器、借鉴YOLO的<strong>YOWO</strong>：3DCNN来提取时空信息，2Dmodel来提取空间信息、<strong>WOO</strong>：单个统一的网络，只用一个backbone来做actorlocalization和actionclassification、<strong>SE-STAD</strong>用FCOS作为目标检测器、<strong>EVAD</strong>用ViTs，通过dropout non-kkeyframe tokens减小计算开销，refine scenencontext来增强模型性能。</p></li><li><p>frame-level的方法没有完全利用时序的信息，将视频帧视为独立的图像，因此提出了clip-level的STAD方法，将一系列的frames作为输入，直接输出检测到的<font color=red>tubelet proposals(short sequence of bounding boxes)</font></p></li><li><p><strong>Clip-level</strong>: 输入一个video clip，模型输出一个3Dspatio-temporal tubelet proposals。3D tubelet proposals是由一系列的bboxes that tightly bound the actions of interest形成。然后这些tubelet proposals在successiveclips连接在一起，形成完整的action tubes。作者从几个<strong>Largemotion</strong>、<strong>Progressive learning</strong>、<strong>Anchorfree</strong>、<strong>Visual RelationModeling</strong>这几个角度给出了相关的算法。为了克服 3D anchors的 fixedspatial exntet的问题，有人提出了 <font color=red> two-framemicor-tubes</font>的方法。为了避免3D cuboid anchor，也有人提出了<font color=red> 通过frame-level actor detection，然后将detectedbboxes连接起来形成class-independent action tubelets,然后给到temporalunderstanding module来做行为分类 </font>。还有<strong>sparse-to-dense</strong>的方法。在<em>progressivelearning</em>方面，通过progressive learning 方法，反复修正proposalstowards actions over a fewsteps。有人提出了<strong>PCSC</strong>框架，以迭代的方式，用一个stream(RGB/Flow)里的regionproposals和features来帮助另一个stream(RGB/Flow)提高action localizationresults。计算anchor是一个比较费劲的事情，提出了一些<strong>anchor-free</strong>的方法：有人把每个actioninstance 视为moving points的轨迹。</p><ul><li><strong>MovingCenterDetector(MOC-detector)</strong>，它由3个branches组成：center-branch forinstance center detection and action recognition; movement branch formovement estimation;box branch for spatial extent detection.</li><li><strong>VideoCapsuleNet</strong>：用3D conv along withcapsules来学习必要的语义信息 for action detection and recognition。有一个定位的component，利用capsules得到的action representation for apixel-wise localization of actions.</li><li><strong>TubeR</strong>：直接检测视频里的actiontubelet，同时执行action localization和recognition from a singlerepresentation. 设计了一个tubelet-attention moduel 来model dynamicspatio-temporal nature of a video clip. TubeR学习了tubeletqueries的集合，输出actio tubelets.</li></ul></li></ol><p>在<strong>Visual Relation Modeling</strong>方面，clip-level 的visualrelations也被探索了，来增强STAD模型；有人提出了 long short-term relationnetwork(LSTR)，获取short-term 和long-term relations in videos。具体地说： LSTR先产生3D bboxes(tubelets) in eachvideo。然后通过<strong>spatio-temporal attention mechanism</strong> ineach clip 来建模human-context interactions。推理long-term temporaldynamics across video clips via <em>graph ConvNet in a cascadedmanner</em>。 actor tubelets和objectproposals的特征然后被用于构建关系图，建模human-object manipulations andhuman-human interaction actions。</p><ol start="4" type="1"><li><p><strong>Linking up the Detection Results</strong>:actions会持续一段时间，通常跨很多帧和clips。在frame-level或者clip-level检测结果得到之后，很多方法用一个<strong>linkingalgorithm</strong>来detections across frames orclips连接起来，形成video-level的action tubes。</p><ul><li><strong>linking up frame-level detection boxes</strong>：第一个frame-level action detection linking算法是由<strong>Gkioxari</strong>提出的，他们假设两个相邻regionproposals(bboxes)的空间范围有很好的重叠，且scores很高，有很大的可能性belinked。计算两个region proposals的linking score的公式为： <spanclass="math display">\[s_c(R_t,R_{t+1})=s_c(R_t)+s_c(R_{t+1})+\lambda\cdotov(R_t,R_{t+1}), Eq.(1)\]</span></li></ul><p><span class="math inline">\(s_c\)</span>(R_i)是region proposalR_i的class specific score，<spanclass="math inline">\(ov(R_i,R_j)\)</span> 是<spanclass="math inline">\(R_i\)</span>和<spanclass="math inline">\(R_j\)</span>的 IoU(overlap)。<spanclass="math inline">\(\lambda\)</span>是一个超参数，对IoU项进行加权，有些模型输出的bbox是带有actionnessscores，这里就用actionness scores代替class-specificscores。计算出所有的linking scores之后，<font color=red>最优的path</font>通过这个来搜索： <span class="math display">\[\barR_c^*=\underset{\barR}{\text{argmax}}\frac{1}{T}\sum_{t=1}^{T-1}s_c(R_t,R_{t+1}),Eq.(2)\]</span></p><p><span class="math inline">\(\bar{R}_{c} =[R_{1},R_{2},\ldots,R_{T}]\)</span> 是action class <spanclass="math inline">\(c\)</span>的一系列的linked region.通过维特比算法来解这个优化问题。找到最有的path之后，region proposals in<span class="math inline">\(\bar{R}_{c}\)</span> 从 set of regionproposals中去掉，然后再继续解该方程，直到set of regionproposals是空的。从Eq.(2)中计算得到的path被称为 <font color=red> actiontube</font>。 action tube <spanclass="math inline">\(\bar{R}_{c}\)</span> 定义为：<spanclass="math inline">\(S_{c}(\bar{R}_{c})=\frac1T\sum_{t=1}^{T-1}s_{c}(R_{t},R_{t+1}).\)</span></p><ul><li>基于Gkioxari的思路，Peng等人提出了在 Eq.(1)增加一个阈值函数，linkingscore between two region proposals变成了： <spanclass="math display">\[s_{c}(R_{t},R_{t+1})=s_{c}(R_{t})+s_{c}(R_{t+1})+\lambda\cdotov(R_{t},R_{t+1})\cdot\psi(ov)\]</span></li></ul><p><span class="math inline">\(\psi(ov)\)</span>是一个阈值函数，当<spanclass="math inline">\(ov\)</span>大于<spanclass="math inline">\(\tau\)</span>时，<spanclass="math inline">\(\psi(ov)=1\)</span>，否则<spanclass="math inline">\(\psi(ov)=0\)</span>。Peng在实验中发现，有了这个阈值函数，linkingscore比之前更好了更robust了。Kopuklu进一步扩展了这个linkingscore的定义：</p><p><span class="math display">\[\begin{aligned}s_{c}\left(R_{t},R_{t+1}\right)=&amp;\psi(ov)\cdot[s_{c}\left(R_{t}\right)+s_{c}\left(R_{t+1}\right) \\&amp;+\alpha\cdot s_{c}\left(R_{t}\right)\cdot s_{c}\left(R_{t+1}\right)\\&amp;+\beta\cdot ov\left(R_{t},R_{t+1}\right)] ,\end{aligned}\]</span></p><p>其中<span class="math inline">\(\alpha\)</span>和<spanclass="math inline">\(\beta\)</span>是超参数，<spanclass="math inline">\(\alpha \cdot s_c(R_t)\cdots_c(R_{t+1})\)</span>项将两个连续帧之间的dramatic change考虑进去，提高video detection的性能</p><ul><li>在<strong>Temporal trimming</strong>中，上述的linking算法得到了action tubes 横跨整个video duration，然而，human actions通常只占很小一部分。为了决定一个actioninstance的<strong>时间范围</strong>。有一些temporaltrimming的工作。Saha限制了consecutive proposals来得到smooth actionnessscores。通过动态规划解一个energymaximization的问题。Peng等人依赖一个高校的maximum subarray方法：给定一个video-level action tube <spanclass="math inline">\(\bar{R}\)</span>, 它的理想的时间范围是从frame<span class="math inline">\(s\)</span> to frame <spanclass="math inline">\(e\)</span>，满足下列公式：</li></ul><p><spanclass="math display">\[s_{c}(\bar{R}_{(s,e)}^{\star})=\underset{(s,e)}{\operatorname*{argmax}}\{\frac{1}{L_{(s,e)}}\sum_{i=s}^{e}s_{c}(R_{i})-\lambda\frac{|L_{(s,e)}-L_{c}|}{L_{c}}\},\]</span></p><p><span class="math inline">\(L_{(s,e)}\)</span>是 action tube的长度，<span class="math inline">\(L_c\)</span>是 class <spanclass="math inline">\(c\)</span>在训练集中的平均时长。</p><ul><li><p>在<strong>online action tubegeneration</strong>中，在视频的第一帧，用 <spanclass="math inline">\(n\)</span>个detected bboxes来初始化 <spanclass="math inline">\(n\)</span> action tubes for each class <spanclass="math inline">\(c\)</span>。然后，actiontubes通过增加frame中的box扩大或者在<spanclass="math inline">\(k\)</span>个连续帧之后没有匹配的boxes，就会终结。最后，每个更新的tube通过执行binarylabeling using a online Viterbi算法来进行temporally trimmed。</p></li><li><p><strong>Linking up Clip-level DetectionResults</strong>:clip-level tubelet linking算法旨在 <strong>associate asequence of clip-level tubelets into video-level actiontubes</strong>。它们通常是从frame-levelbox中得到。一个tubelet内的内容应该获取一个action，在任何两个连续的clips连接的tubelets应该有一个大的<font color=red> temporal overlap</font>，因此，他们定义tubelet's的linking score是这样的： <spanclass="math display">\[S=\frac{1}{m}\sum_{i=1}^{m}Actionness_{i}+\frac{1}{m-1}\sum_{j=1}^{m-1}Overlap_{j,j+1}\]</span></p></li></ul><p><span class="math inline">\(Actionness_i\)</span> 表示 第<spanclass="math inline">\(i\)</span>个clip的tubelet的actionness score。<spanclass="math inline">\(Overlap_{j,j+1}\)</span>表示来自第<spanclass="math inline">\(j\)</span>和第<spanclass="math inline">\(j+1\)</span>个clip的两个proposals的Overlap。<spanclass="math inline">\(m\)</span>是videoclips的总数，两个tubelets之间的overlap是基于<strong>第<spanclass="math inline">\(j\)</span>个tubelet的最后一帧和<spanclass="math inline">\(j+1\)</span>个tubelet的第一帧</strong>来计算的。在计算出了tubelets'分数之后；另外，有人把frame linking的算法扩展到tubelet linking来构建action tubes。核心的idea是这样的： -初始化：在视频的第一帧，对每个tubelet开始一个new link，这里a link 指asequence of linked tubelets - Linking: 给定一个new frame <spanclass="math inline">\(f\)</span>，扩展存在的links with one o the tubeletcandidates starting at this frame. 选择tubeletcandidate的标准如下：没有被其它links选择；有最高的actionscore；和要被扩展的link的overlap高于给定的阈值。 -终止：对于一个存在的Link，如果这个标注在<spanclass="math inline">\(K\)</span>个连续帧之后没有被满足，这个link就会终止，<spanclass="math inline">\(K\)</span>是一个给定的超参数。</p><p>由于它的简单和高效，tubelet linking算法被很多最近的工作采用。</p><p>在<strong>temporal trimming</strong>方面。tubelet linking算法，初始和终止决定了actiontubes的时间范围，有人发现它不能彻底的解决transition state产生的temporallocation error。定义为ambiguous states，但不属于targetactions。为了解决这个问题，有人提出了transition-awareclassifier：能够区分transitional states和realactions；后续也有人通过引入一个action switch regressionhead：决定一个boxprediction是否描述了一个执行actions的actor。这个regressionhead给出了一个tubelet每个bbox的action switchscore。如果这个score高于给定的阈值，这个box就包含这个action。这个actionswitch regression head能够有效减小transitional states的误分类。</p></li><li><p>数据集：STAD中经常用到的数据集有：</p><ul><li><strong>Weizmann</strong>：在一个统一的背景中用一个静态相机记录，包含90个videoclips grouped 10 action classes, performed by 9 diffferentsubjects，每个video clip 包含多个单一行为的实例，空间分辨率为$180$，每个clip是从1-5s。</li><li><strong>CMU Crowded Videos</strong>：包含5个actions，每个action有5个training videos和48个testvideos。所有的video被缩放，空间分辨率为<span class="math inline">\(120\times 160\)</span>，testvideos是5-37秒(166-1115帧)，这个数据集是在一个凌乱的和动态的环境中记录的，以至于这个数据集上的actiondetection更加有挑战性。数据集是denselyannotated，提供时间和空间坐标(x,y,height,width,start,end andframes)。</li><li><strong>MSR Action I andII</strong>：是微软研究组弄的，II是I的扩展，Action I包含62个actioninstances in 16个video sequences。II包含203 instances in 54videos，每个video包含不同个体执行的多个actions。所有的视频是32-76秒，每个actioninstance的时间和空间标注是提供的。包含3个action 类别。</li><li><strong>J-HMDB</strong>：是joint-annotatedHMDB数据集，HMDB包含5个action categories，每个category包含至少101个视频片段，数据集包含6849个视频片段，分布在51个actioncategories中。J-HMDB包含从HMDB数据集中宣导的21类视频，选择的视频涉及单个任务的动作，每个actionclass有36-55个clips，每个clip包含15-40帧，总共928个clips，每个clip被裁剪了，第一帧和最后一帧对应一个action的开始和终止。frame的分辨率是<span class="math inline">\(320 \times 240\)</span>, frame rate 是30fps。</li><li><strong>UCF Sports</strong>：包含体育领域的10个actions，所有的视频包含相机运动和复杂背景，包含150个clips，每个clip的framerate是10 fps，空间分辨率是 <span class="math inline">\(480 \times360\)</span> to <span class="math inline">\(720 \times576\)</span>，持续时间是 2.2 - 14.4秒，平均6.39秒。</li><li><strong>UCF101-24</strong>：UCF101的数据来自Youtube，包含101行为类别，总共13320个视频。对于行为检测任务，包含24个行为类别的3207个视频子集提供了密集标注，这个子集称为UCF101-24，不同于UCFSports和J-HMDB，视频是被剪过的，UCF101-24是没有被剪过的。</li><li><strong>THUMOS andMultiTHUMOS</strong>：THMOS系列数据集包含4个数据集：THUMOS13，THUMOS14,THUMOS15和MultiTHUMOS，所有的视频是来自UCF101，THUMOS数据集包含24个actionclasses，视频的时长从几秒到几分钟不等。数据集包含13000个被剪过的视频，超过1000个没有剪过的视频，超过2500个negativesamplevideo。这些视频可能包含none、one、或者单个行为或者多个行为的实例。MultiTHUMOS是一个THUMOS的增强的版本，是一个dense、multi-class、frame-wiselabeled video dataset with 400 videos of 30hours和65个类别的38690个标注。平均每帧有1.5个标注，每个视频10.5个行为类别。</li><li><strong>AVA</strong>：来自Youtube的430个movies，每个movie提供了第15到30分钟的这个clip，每个clip分成897个重叠3s的segmentswith a stride of 1second。对于每个segment，中间帧被选为keyframe，在每个keyframe，每个人都用bbox和actions标注，430个movies分成235个training，64个validation和131test movies，差不多是55:15:30的比例，包含80个原子行为，60个actions用来evaluation。</li><li><strong>MultiSports</strong>：这个视频来自Youtube上奥林匹克和世界杯的竞赛，包含4个运动，66个行为类别，每个运动800个clips，共3200个clips；包含37701个actioninstances with902k个bboxes，每个行为类别的instance从3个到3477个不等，显示了自然的长尾分布。每个视频被多个行为类别的多个实例标注，视频的平均长度是750帧，每个行为的segment比较短，平均24帧。</li></ul></li><li><p>评估指标：主要是两个：frame mAP和video-mAP</p><ul><li><strong>Frame mAP</strong>: area under the PR curve of bboxdetections at each frame.<strong>如果和GTbbox的IoU大于给定的阈值且actionlabel是正确的，则detection是对的，阈值设为0.5</strong>。Frame-mAP能够独立于linkingstrategy来比较检测精度。</li><li><strong>Video-mAP</strong>：area under PR curve of action tubepredictions。<strong>如果和GT tube的IoU大于给定的阈值且actionlabel是正确的，则tubedetection是对的</strong>，两个tubes之间的IoU被定义为时序上的IoU，<strong>multipliedby the average of the IoU between boxes averaged over all overlappingframes</strong>。 video-mAP的阈值通常设为0.2、0.5、0.75，and0.5:0.95。对应于average video-mAP for thresholds with step 0.05 in thisrange.然而frame-mAP衡量的是单帧里的分类和空间检测的能力，video-mAP能够进一步评估时序检测的能力。</li></ul></li><li><p>未来的方向：</p><ul><li><strong>Lable-efficient learning forSTAD</strong>：STAD需要密集的标注，然而密集的标注是昂贵的。</li><li><strong>Online real-timeSTAD</strong>：STAD有很多的在线的应用，必须基于过去的数据来给出当前帧的预测。这要求模型必须是轻量和高效的，还有很长的路。</li><li><strong>STAD under largemotion</strong>：在真实场景中，很多行为由于fast actordisplacement，camera motion,actions有很大的motion。</li><li><strong>Multimodal learning for STAD</strong>：actionvideo包含多个模态，包括视觉、声音甚至语言，因此，通过多模态学习，有潜力实现比单个模态更好的检测精度。另一方面，actions可以通过多种传感器得到，例如深度相机，红外相机，Lidar等，STAD或者可以从多个模态数据中学到的融合表征受益。</li><li><strong>Diffusion models forSTAD</strong>：扩散模型作为一类生成模型，从 sample in随机分布开始，通过逐步地去噪恢复样本数据。尽管它们属于生成模型，它们对于表征的感知任务(例如目标检测和时序动作定位)，表现有效,输入随机的spatialboxes(temporal proposals)，基于扩散的模型能够精确地产生目标框(actionproposals)，自从STAD视为目标检测和时序动作定位(temporal actionlocation)和结合体，有一些工作展示了利用diffusionmodels来解决STAD任务。</li></ul></li></ol><h4 id="参考链接">参考链接：</h4><ul><li>https://0809zheng.github.io/2021/07/15/stad.html：时空行为检测的比较好的介绍</li></ul><p>一些STAD相关的解释和算法图示：</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/2217f65a3fade532f079dbefe1230b47063417b7/8-Figure7-1.png"alt="Spatio-Temporal Action Detection Task" /><figcaption aria-hidden="true">Spatio-Temporal Action DetectionTask</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/2-Figure1-1.png"alt="Spatio-temporal action detection" /><figcaption aria-hidden="true">Spatio-temporal actiondetection</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/2-Table1-1.png"alt="Comparision" /><figcaption aria-hidden="true">Comparision</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/2-Figure2-1.png"alt="illustration" /><figcaption aria-hidden="true">illustration</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/3-Figure3-1.png"alt="Taxonomy of STAD models" /><figcaption aria-hidden="true">Taxonomy of STAD models</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/3-Figure4-1.png"alt="RPN for STAD" /><figcaption aria-hidden="true">RPN for STAD</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/4-Figure5-1.png"alt="I3D for STAD" /><figcaption aria-hidden="true">I3D for STAD</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/4-Figure6-1.png"alt="YOWO" /><figcaption aria-hidden="true">YOWO</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/5-Figure7-1.png"alt="ARCN" /><figcaption aria-hidden="true">ARCN</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/6-Figure8-1.png"alt="CFAD" /><figcaption aria-hidden="true">CFAD</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/10-Table2-1.png"alt="STAD datasets" /><figcaption aria-hidden="true">STAD datasets</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/12-Table3-1.png"alt="Performance on J-HMDB and UCF101-24" /><figcaption aria-hidden="true">Performance on J-HMDB andUCF101-24</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/13-Table4-1.png"alt="Performance on UCF Sports and MultiSports datasets" /><figcaption aria-hidden="true">Performance on UCF Sports and MultiSportsdatasets</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/13-Table5-1.png"alt="Performance on AVA" /><figcaption aria-hidden="true">Performance on AVA</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;h4
id=&quot;视频理解及分析的计算机视觉任务&quot;&gt;视频理解及分析的计算机视觉任务&lt;/h4&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;之前看的时候，不管是论文还是一些博客，感觉都不是很清晰和全面，大家的定义不全面，特别是英文的名称上，这里写一下我的理解：&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;几个任务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;行为识别(Action Recognition)&lt;/strong&gt;:
实质是对视频的分类任务，可以类别图像领域的分类任务&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时序动作定位(Temporal Action Localization)&lt;/strong&gt;:
在时间上对视频进行分类，给出动作的起止时间和类别&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时空行为检测(Spatio-Temporal Action Detection)&lt;/strong&gt;:
不仅识别出动作出现的&lt;strong&gt;区间&lt;/strong&gt;和&lt;strong&gt;类别&lt;/strong&gt;，还要在空间范围内用一个bounding
box标记处目标的&lt;strong&gt;位置&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;还有人提出了时空动作定位(Spatio-temporal Action
localization)&lt;/strong&gt;：和上一个是一样的&lt;/li&gt;
&lt;li&gt;Action Detection在Paperswithcode上的定义： aims to find both where
and when an action occurs within a video clip and classify what the
action is taking place. Typically results are given in the form of
&lt;strong&gt;action tublets&lt;/strong&gt;, which are action bounding boxes linked
across time in the video. &lt;em&gt;This is related to temporal localization,
which seeks to identify the start and end frame of an action, and action
recognition, which seeks only to classify which action is taking place
and typically assumes a trimmed video.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;论文里还提到了&lt;strong&gt;temporal action segmentation&lt;/strong&gt;：
针对细粒度的actions和videos with dense occurrence of actions to predict
action label labels at every frame of the video.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;时空行为检测的算法：之前的论文都是都是基于行为识别(action
recognition)的，很多都是基于早期的Slowfast的那个检测的方式：需要一个额外的检测器，实现行为检测。也就是在行为识别的基础上，再进行时空行为检测。但这并不是我理想中的方式，所以很多行为识别的算法，在AVA上也能上榜；最近看VideoMAE看了之后，就一直在看这个，没有去看看其它的。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Action Detection数据集：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;J-HMDB&lt;/li&gt;
&lt;li&gt;UCF101-24&lt;/li&gt;
&lt;li&gt;MultiSports&lt;/li&gt;
&lt;li&gt;AVA&lt;/li&gt;
&lt;li&gt;其中，JHMDB和UCF101-24是密集标注数据集(每一帧都标注，25fps)，这类数据集每个视频只有一个动作，大部分视频是单人做一些语义简单的重复动作；AVA为代表的稀疏标注数据集(隔一段时间标注一帧，1fps)，没有给出明确的动作边界&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Video Understanding" scheme="https://young-eng.github.io/YoungBlogs/tags/Video-Understanding/"/>
    
  </entry>
  
</feed>
