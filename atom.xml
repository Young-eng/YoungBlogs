<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Young&#39;s Blog</title>
  
  
  <link href="https://young-eng.github.io/YoungBlogs/atom.xml" rel="self"/>
  
  <link href="https://young-eng.github.io/YoungBlogs/"/>
  <updated>2024-09-04T06:26:40.865Z</updated>
  <id>https://young-eng.github.io/YoungBlogs/</id>
  
  <author>
    <name>Young</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Docker配置及使用</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/09/04/Docker%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/09/04/Docker%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8/</id>
    <published>2024-09-04T06:22:14.000Z</published>
    <updated>2024-09-04T06:26:40.865Z</updated>
    
    <content type="html"><![CDATA[<h3 id="docker的配置及使用">Docker的配置及使用</h3><ol type="1"><li><p>windows和linux安装docker的方式有点不一样，但也不复杂，主要的地方在于需要弄一个registry_mirror，虽然不知道还有没有效，当然，能科学上网的话就方便很多了。</p></li><li></li></ol><span id="more"></span><h3 id="参考链接">参考链接：</h3><ul><li><p><code>https://www.ruanyifeng.com/blog/2018/02/docker-tutorial.html</code>，阮一峰老师的博客</p></li><li><p><code>https://docker-practice.github.io/zh-cn/image/pull.html</code>,DockerPractice</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;docker的配置及使用&quot;&gt;Docker的配置及使用&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;windows和linux安装docker的方式有点不一样，但也不复杂，主要的地方在于需要弄一个registry_mirror，虽然不知道还有没有效，当然，能科学上网的话就方便很多了。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Tools" scheme="https://young-eng.github.io/YoungBlogs/categories/Tools/"/>
    
    
    <category term="Docker" scheme="https://young-eng.github.io/YoungBlogs/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>SLAM 学习记录</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/09/03/SLAM-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/09/03/SLAM-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</id>
    <published>2024-09-03T08:25:10.000Z</published>
    <updated>2024-09-03T08:41:57.170Z</updated>
    
    <content type="html"><![CDATA[<h3 id="slam介绍">SLAM介绍</h3><ol type="1"><li>SLAM: <strong>S</strong>imultaneous <strong>L</strong>ocalization<strong>a</strong>nd<strong>M</strong>apping，翻译为“即时定位与建图”，是指<strong>搭载特定传感器的主体，在没有环境先验信息的情况下，于运动过程中建立环境的模型，同时估计自己的运动，如果这里的传感器主要为相机，那就称为“视觉SLAM”</strong>。</li></ol><h3 id="visual-slam">Visual SLAM</h3><ol type="1"><li>经典视觉SLAM的框架主要有几个步骤：<ul><li><strong>传感器信息读取</strong>：在视觉SLAM中主要为相机图像信息的读取和预处理；如果在机器人中，可能还有码盘、IMU等传感器信息的读取和同步。</li><li><strong>前端视觉里程计(Visual Odometry,VO)</strong>：视觉里程计的任务是估算相邻图像间相机的运动，以及局部地图的样子，VO又称为前端(FrontEnd)。</li><li><strong>后端(非线性)优化(Optimization)</strong>：后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行优化，得到全局一致的轨迹和地图，由于接在VO之后，又称为后端(BackEnd)。</li><li><strong>回环检测(Loop ClosureDetection)</strong>：回环检测判断机器人是否到达过先前的位置，如果检测到回环，它会把信息提供给后端进行处理。</li><li><strong>建图(Mapping)</strong>。它是根据估计的轨迹，建立与任务要求对应的地图。</li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;slam介绍&quot;&gt;SLAM介绍&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;SLAM: &lt;strong&gt;S&lt;/strong&gt;imultaneous &lt;strong&gt;L&lt;/strong&gt;ocalization
&lt;strong&gt;a&lt;/strong&gt;nd
&lt;st</summary>
      
    
    
    
    <category term="Learning" scheme="https://young-eng.github.io/YoungBlogs/categories/Learning/"/>
    
    
    <category term="SLAM" scheme="https://young-eng.github.io/YoungBlogs/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>WOO</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/27/WOO/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/27/WOO/</id>
    <published>2024-08-27T07:23:26.000Z</published>
    <updated>2024-08-29T05:58:17.880Z</updated>
    
    <content type="html"><![CDATA[<h3id="watch-only-oncean-end-to-end-video-action-detection-framework1">WatchOnly Once：An End-to-end Video Action DetectionFramework<sup>[1]</sup></h3><blockquote><p>作者是来自港大的罗平老师组的Shoufa Chen、Peize Sun、EnzeXie等人。论文引用[1]:Chen, Shoufa et al. “Watch Only Once: An End-to-EndVideo Action Detection Framework.” 2021 IEEE/CVF InternationalConference on Computer Vision (ICCV) (2021): 8158-8167.</p></blockquote><h3 id="time">Time</h3><ul><li>2021.Oct</li></ul><h3 id="key-words">Key Words</h3><ul><li>end-to-end unified network</li><li>task-specific features</li></ul><h3 id="总结">总结</h3><ol type="1"><li>提出了一个端到端的pipeline for video actiondetection。<strong>当前的方法要么是将video action detection这个任务解耦成action localization和actionclassification这两个分离的阶段，要么在一个阶段里训练两个separatedmodels</strong>。相比之下，作者的方法将actor localization和actionclassification弄在了一个网络里。通过统一backbone网络，去掉很多认为的手工components，整个pipeline被简化了。<strong>WOO</strong>用一个unifiedvideo backbone来提取features for actor location 和actionlocalization,另外，引入了<font color=red>spatial-temporal actionembeddings</font>，设计了一个 spatial-temporal fusionmodule来得到更多的含有丰富信息的discriminative features，提升了actionclassification的性能。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>Video action detection 包含 <strong>actor bboxlocalization</strong>和 <strong>action typeclassification</strong>，当前方法的复杂性来自于actorlocalization和action classification之间的基本的困境。thatis，*用一个single key frame is "positive" for actor localization，但是"negative" for actionclassification，然而用多个frames有相反的影响。这是因为actionlocalization需要一个2D的检测模型同预测video clip的key frame上的actorbbox。在这个阶段，考虑clip中的相邻帧会带来额外的计算和存储成本，相比之下，actionclassification严重依赖于一个 3D video model来提取videosequence中的时序的信息，单帧图像有很少的temporal motion representationfor action classification。</p><p>之前提出了两种可能的替代方法来解决这个困境。第一个是用离线的persondetector，用来产生actor proposals，不是和actionclassification一起训练的，然后一个独立的video model用这些actorproposals和raw frames作为输入来预测action classes。单独的一个persondetector是已经足够复杂了，which is pre-trained on the ImageNet and COCOhuman keypoint detection，and further fine-tuned on the target actiondetection dataset。这个方法比较复杂，计算成本高，需要两个separatemodels和两个训练阶段。更进一步，<strong>separate optimization on twosub-problems leads to a sub-optimal solution</strong>。</p><p>第二种类型的方法，是将actor detection和action classificationmodels一起在单个trainingstage中进行联合训练。虽然训练的pipeline在某种程度上是简化了，两个模型仍然需要独立的从rawimages中提取特征。因此，整个框架仍然有很高的computation和memorycost。</p><p><strong>一个很自然的问题是：有没有可能设计一个简单的unified的网络来解决actorlocalization和action classification in a single end-to-endmodel</strong>。</p><p>本文提出了 <strong>Watch OnlyOnce(WOO)</strong>的框架，WOO直接预测actors的bbox和action classes from avideo clip。"watch" the clip onlyonce，就能够预测actor的位置和action的类别。方法主要包含3个keycomponents：<font color=red>unified backbone、a spatial-temporal actionembedding、a spatial-temporal knowledge fusion mechanism</font>。</p><p>首先，设计了一个简单的有效的module，使得单个backbone能偶提供task-specificfeature maps for actor localization head 和action classificationhead。这个module是轻量的，能够用来isolate keyframe features from allframes in the early stage of thebackbone。这个动机是，当model走深的时候，keyframe能够得到moreinteraction with neighboringframes，提出的module能够很容易地插入到现有的backbone中，例如I3D,X3D等。</p><p><strong>另外，注意到，同一个的架构tends to behave well for actorlocalization，但是对于action classification是有限的。这个困难是actiondetection主要lies in action classification。因此，怀疑单个backbone forboth tasks是否会bias towards localization，然后找到一个undesiredsolution，因此对actionclassification的性能造成了影响。基于这个观察，提出了spatial和tempoalaction embedding和interaction mechanism between them，来使得actionclassification features在spatial 和temporal perspectives更加的discriminative。</strong></p><p>第三，提出了一个spatial-temporal fusion module来汇聚spatial和temporalknowledge，这个spatial properties例如shape和pose，temporalproperties例如dynamic motion和temporal scale ofaction,结合在一起，通过spatial-temporal fusion module，来生成actionfeatures for actio classification。</p><p>主要贡献如下：</p><ol type="1"><li><p>提出了一个端到端的框架for video action detection，给定一个videoclip作为输入，能够直接产生bboxes 和action classes，不需要独立的persondetector(这个是在之前的工作中必不可少的).</p></li><li><p>提出了一个spatial-temporal embedding，一个embedding interactionmechanism，能够提高features的discriminativeness for actionclassificatin。一个spatial-temporal fusionmodule来进一步从spatial和temporal来汇聚features。</p></li></ol></li><li><p>相关工作：</p><ul><li><p><font color=red>two-stage, twobackbone</font>：当前的STAD的SOTA模型通常用了两阶段的pipeline，用了两个backbone。这些方法简单地把STAD任务划分成actorlocalization和action classification。具体地说，在第一阶段，在COCOkeypoint上预训练一个model，然后再目标STADdataset上进行微调；第二阶段，用video clip的keyframe作为第一阶段中得到的detection model的输入，来预测actorbboxes。然后将video clip和actor bboxes作为3Dbackbone的输入，来提取RoI区域的特征 for action classprediction。<strong>这些方法有很高的复杂度和很低的效率，因为sequentialtraining stage和separatemodel架构。另外，在两个独立的阶段的独立的优化可能导致一个sub-optimal的结果</strong>。</p></li><li><p><font color=red>One-stage,two-backbone</font>：<strong>YOWO和ACRN通过同时训练2D的actor检测网络和3D video model，来简化网络。然而，这里仍然有两个separatemodels来优化</strong>。<strong>以YOWO为例，它包含一个在Kinetics上预训练的一个3D model和 YOLO pre-trained on PASCAL VOC上的 2Dmodel，有很高的计算量和memoryburden</strong>。是虽然这个pipeline在一定程度上简化了。</p></li></ul><p>相比于这些方法，WOOrefreshing的简单：给定一个videoclip，直接预测actor bboxes和对应的action classes。</p><ul><li><p><font color=red>End-to-end objectdetection</font>：最近的端到端的目标检测框架，不需要任何手工设计的过程例如NMS，直接输出预测，实现了很好的性能。在这些工作中，DETR可以被视为端到端的目标检测方法，它采用了globalattention mechanism和双向matching between predictions和ground truthobjects。DETR抛弃了NMS步骤，实现了很好的性能。它在小目标上性能不太好，相比主流的检测器需要更长的训练时间。为了解决上述的问题，提出了<strong>Deformable-DETR</strong>，来将每个objectquery 限制在 a small set of crucial sampling points around referencepoints，而不是all points in the featuremap。Deformable-DETR是高效和快速收敛的；Sparse RCNN利用一组稀疏的learnedobject proposals，以iterative way来进行分类和定位。SparseRCNN和well-established的检测器相比，展示了精度、实时和训练收敛。在本次工作中，主要采用<strong>SparseRCNN的检测头来做定位</strong>。</p></li><li><p><font color=red>Attention mechanism for actionrecognition</font>：对于language相关的任务，注意力机制是一个流行的概念。对于actiorecognition，Non-local网络利用自注意力来得到不同时间或者空间features的<strong>dependencies</strong>。使得注意力机制applicablefor action classification，有人利用non-local block作为一个long-termfeature bank operator，使得video models能得到long-terminformation，提高了action detection的性能。</p></li></ul></li><li><p>Methods: <spanclass="math inline">\(X\quad\in\quad\mathbb{R}^{C\times T\times H\timesW}\)</span> 是一个layer的输入的spatial-temporal featuremap。跟随前人的工作，in this work，将key frame放在videoclip的中间，<span class="math inline">\(X_{t=\lfloorT/2\rfloor}\in\quad\mathbb{R}^{C\times H\timesW}\)</span>表示<strong>keyframe feature map</strong>。</p><ul><li><strong>union backbone</strong>：在之前的video backbone中，key framefeatures将会和相邻的frame features通过temporalpooling或者3D卷积(temporal kernel size 大于1，会给keyframe特征带来意想不到的disturbance)进行interact。为了克服这个问题，video backbone设计的时候，将keyframe和temporal interaction之前的早期的网络的features隔离开。</li></ul><p>和之前的backbon Slowfast(将spatial stride of res5设为1，用a dilationof 2 for its filters，来增加spatial resolution of res5 by <spanclass="math inline">\(2 \times\)</span>)不同，作者去掉了res5中的dialtedconv，采用FPN module来做<strong>keyframe</strong>特征提取。FPNmodule用res2,res3,res4,res5的输出的<strong>keyframefeature</strong>作为输入。进一步用FPN的输出的特征 for actorlocalization,res5输出的特征来做actionclassification。为了这个目的，一个统一的actionbackbone用来提供task-relevant features。</p><p>以上的设计有几个好处，首先，actor localizationhead采用层次化的feature representation作为sourcefeatures。对于目标检测是有好处的。第二，用于actor localization的keyframefeatures通过FPN结构，和所有的video frames的features隔离开，starting atthe early stage of thebackbone。这能减少邻近帧的干扰，因为keyframe会随着model的深入，和邻近帧有更多的interaction。第三，相比于存在的两个backbone的网络(用独立的backbone)for actor localization，作者仅用一个轻量的FPN module that tasks imagefeatures as input，减少了参数和FLOPS。</p><ul><li><p><strong>Acotr Localization Head</strong>：受最近的SparseRCNN的启发，设计了一个端到端的actor detection head for actorlocalization,detection head在得到hierarchical features fromFPN之后，能够预测bbox和对应的scores indicating model's confidence on thebox containing an actor。另外，person detector利用 set prediction lossfor optimal bipartite matching between prediciton和ground truth attrainingstage，在验证的阶段不需要post-process。不同于two-backbone的方法，不许哟啊额外的预训练，因为persondetector和action classifier共享一个backbone。</p></li><li><p><strong>Action Classification Head</strong>：给定有persondetector生成的 <span class="math inline">\(N\)</span>个actor proposalboxes。用RoIAlign来提取每个box的spatial和temporalfeatures，这两种类型的features然后融合，得到最终的action classprediction。细节如下：</p><ol type="1"><li><p><strong>Spatial Action Features</strong>：<spanclass="math inline">\(X_{5} \in \mathbb{R}^{C\times T\times H\timesW}\)</span>表示 res5得到的feature，在时间维度上进行一个<strong>globalaverage pooling</strong>，得到一个spatial feature map，<spanclass="math inline">\(f^{s} \in \mathbb{R}^{C\times1\times H\timesW}\)</span>，在 <span class="math inline">\(f^s\)</span> 上用RoIAlignwith <span class="math inline">\(N\)</span> 个actor proposals，得到<span class="math inline">\(N\)</span>个 spatial RoI features。<spanclass="math inline">\(f_{1}^{s},f_{2}^{s},\cdots,f_{N}^{s} \in\mathbb{R}^{C\times S\times S},\)</span>， <span class="math inline">\(S\times S\)</span>是 RoIAlign输出的spatial output size。</p></li><li><p><strong>Temporal Action Features</strong>：除了spatial actionfeatures，temporal properties也很重要，为了得到temporal motioninformation，从feature volume <spanclass="math inline">\(X_5\)</span>中的every frame提取temporalfeatures。因为这里主要关注temporal information，在spatialdimension上用一个global average pooling，来提取temporal RoIfeatures。temporal action feature表示为<spanclass="math inline">\(f_{1}^{t},f_{2}^{t},\cdots,f_{N}^{t}\in\mathbb{R}^{C\times T\times1\times1}\)</span>。</p></li><li><p><strong>Embedding Interaction</strong>：为了得到discriminativefeatures，为了增强instance的特性，引入了spatial 和temporal embedding tobe convolved with aforementioned spatial and temporal features。spatialembedding期望能够encoder spatial properties例如shape,pose等。temporalembedding能够encode temporal dynamicproperties，例如dynamics和action的temporalscale。注意到embedding是对于每个 <span class="math inline">\(N\)</span>features是exclusive的。定义 <span class="math inline">\(E^{s}\in\mathbb{R}^{N\times d},E^{t}\in \mathbb{R}^{N\times d}\)</span> forspatial and temporal embedding。<span class="math inline">\(E_n^s \in\mathbb{R}^d,E_n^t \in \mathbb{R}^d\)</span> are working for n-th RoIfeature。为了获得不同actors之间的relation 信息，构建了一个attentionmodule for all RoI features。因为每个actor RoI有自己的spatial和temporalembedding，embedding相比于featuremap更lighter，在对不同给的embedding之间而不是featuremaps之间采用attention mechanism for efficiency。这里给定一个queryelement和一系列key elements，多头注意力module能够根据attentionweights(measure compatibility of query-key pairs adaptively)来汇聚keycontents。最后，<span class="math inline">\(x = (x_,...,x_n)\)</span>表示 <span class="math inline">\(n\)</span>个input elements，输出 <spanclass="math inline">\(z= (z_1,...,z_n)\)</span>，<spanclass="math inline">\(z_i\)</span>是weighted sum of a linearlytransformed input：</p></li></ol><p><spanclass="math display">\[z_i=\sum_{j=1}^n\alpha_{ij}(x_jW^V).\]</span></p><p>weight coefficient <spanclass="math inline">\(\alpha_{ij}\)</span>是通过softmax计算出来的：<span class="math display">\[\alpha_{ij}=\frac{\expz_{ij}}{\sum_{k=1}^n\exp z_{ik}}, \text{where}z_ij=\frac{(x_iW^Q)(x_jW^K)^T}{\sqrt{d_z}}.\]</span> 将 <spanclass="math inline">\(E^s\)</span>和 <spanclass="math inline">\(E^t\)</span>送到 self-attentionmodule，得到对应的输出 <spanclass="math inline">\(\phi^s，\phi^t\)</span>，和 原始的embeddings <spanclass="math inline">\(E^s, E^t\)</span>有相同的shape，最后的actionfeature是： <spanclass="math display">\[f=\mathcal{G}(\mathcal{F}(f^s,\phi^s),\mathcal{F}(f^t,\phi^t)),\]</span></p><p><span class="math inline">\(F\)</span>是一个卷积操作 with parameters<span class="math inline">\(\phi\)</span>，<spanclass="math inline">\(G\)</span>是spatio-temporal fusion操作。初始化<span class="math inline">\(F\)</span> with <spanclass="math inline">\(1 \times 1\)</span> kernels for efficiency。</p><ol start="4" type="1"><li><strong>ObjectiveFunction</strong>：提出的model以端到端的方式解决了定位和分类，整个的目标函数由对应的两部分组成：<spanclass="math display">\[\mathcal{L}=\underbrace{\lambda_{cls}\cdot\mathcal{L}_{cls}+\lambda_{L1}\cdot\mathcal{L}_{L1}+\lambda_{giou}\cdot\mathcal{L}_{giou}}_{\text{setpredictionloss}}+\underbrace{\lambda_{act}\cdot\mathcal{L}_{act}}_{\text{action}}.\]</span></li></ol><p>第一部分是 <em>set prediciton loss</em>，produces an optimal<strong>bipartite matching between predictions and ground truthobjects</strong>。用 <span class="math inline">\(L_{cls}\)</span>表示cross-entropy loss over two classes(containing actor vs notcontaining actor)。<span class="math inline">\(L_{L1}\)</span> 和 <spanclass="math inline">\(L_{giou}\)</span>是box loss。<spanclass="math inline">\(\lambda_{cls},\lambda_{L1},\lambda_{giou}\)</span>是常量，平衡这些loss的contributions。对于第二部分， <spanclass="math inline">\(L_{act}\)</span>是一个binary cross entropy lossused for action classification，<spanclass="math inline">\(\lambda_{act}\)</span> 是对应的weight。</p></li></ul></li><li><p>实验：</p><ul><li>Spatial-temporal fusion：有不同的instantiations of fusing temporal和spatial action features：summation、concatenation和cross-attention(CA)，结果表明CA效果比另外两个好。</li></ul></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/1-Figure1-1.png"alt="Motivation of WOO" /><figcaption aria-hidden="true">Motivation of WOO</figcaption></figure><p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Motivation ofWOO. (a) Previous dominant video action detection methods usually adopttwo separate networks: an independent 2D detection model for actorlocalization from every key frames, and a 3D video model for actionclassification from video clips. (b) Our end-to-end unified frameworkuses a single backbone network to handle both 2D image detection and 3Dvideo classification (i.e.2D spatial dimensions plus a temporaldimension). This unified backbone only “watches” an input video once,and directly produces both actor localization and actionclassification</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/3-Figure2-1.png"alt="Comparison of Backbone" /><figcaption aria-hidden="true">Comparison of Backbone</figcaption></figure><p><span class="math inline">\(Figure \ 2^{[1]}\)</span>: Comparisons ofbackbone architecture. (a) Two separate backbones for actor localizationand action classification. Video backbone adopts res5 stage with dilatedconvolution (DC5). (b) A single union backbone which can providetask-specific features for actor localization and action classificationsimultaneously, enabling nearly cost-free feature extraction for actorlocalization compared to (a). Key frame features are illustrated inlight orange color. Here we purposely omit the res2 features for visualsimplicity</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/4-Figure3-1.png"alt="Action classification head" /><figcaption aria-hidden="true">Action classification head</figcaption></figure><p><span class="math inline">\(Figure \ 3^{[1]}\)</span>: Actionclassification head. Given the RoI feature of a specific box for Tframes, spatial and temporal action features are generated. Then,spatial and temporal embedding is used to make action featurerepresentation more discriminative through the interaction module.Finally, the multi-layer perceptron (MLP) takes as input the fusedspatial-temporal feature and predicts the action class logits. See textfor details</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/5-Figure4-1.png"alt="Structure of interaction module" /><figcaption aria-hidden="true">Structure of interactionmodule</figcaption></figure><p><span class="math inline">\(Figure \ 4^{[1]}\)</span>: Structure ofinteraction module. Here we plot spatial embedding interaction as anexample. ‘⊗’ denotes matrix multiplication, and ‘⊛’ denotes 1 × 1convolution</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;watch-only-oncean-end-to-end-video-action-detection-framework1&quot;&gt;Watch
Only Once：An End-to-end Video Action Detection
Framework&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自港大的罗平老师组的Shoufa Chen、Peize Sun、Enze
Xie等人。论文引用[1]:Chen, Shoufa et al. “Watch Only Once: An End-to-End
Video Action Detection Framework.” 2021 IEEE/CVF International
Conference on Computer Vision (ICCV) (2021): 8158-8167.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2021.Oct&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;end-to-end unified network&lt;/li&gt;
&lt;li&gt;task-specific features&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;提出了一个端到端的pipeline for video action
detection。&lt;strong&gt;当前的方法要么是将video action detection
这个任务解耦成action localization和action
classification这两个分离的阶段，要么在一个阶段里训练两个separated
models&lt;/strong&gt;。相比之下，作者的方法将actor localization和action
classification弄在了一个网络里。通过统一backbone网络，去掉很多认为的手工components，整个pipeline被简化了。&lt;strong&gt;WOO&lt;/strong&gt;用一个unified
video backbone来提取features for actor location 和action
localization,另外，引入了&lt;font color=red&gt;spatial-temporal action
embeddings&lt;/font&gt;，设计了一个 spatial-temporal fusion
module来得到更多的含有丰富信息的discriminative features，提升了action
classification的性能。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>YOWOv3</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/26/YOWOv3/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/26/YOWOv3/</id>
    <published>2024-08-26T06:35:34.000Z</published>
    <updated>2024-09-02T02:06:53.166Z</updated>
    
    <content type="html"><![CDATA[<h3id="yowov3-an-efficient-and-generalized-framework-for-human-action-detection-and-recognition1">YOWOv3:An Efficient and Generalized Framework for Human Action Detection andRecognition<sup>[1]</sup></h3><blockquote><p>作者是Nguyen Dang Duc Manh, Duong Viet Hang等人。论文引用[1]:Dang,Duc M et al. “YOWOv3: An Efficient and Generalized Framework for HumanAction Detection and Recognition.” (2024).</p></blockquote><h3 id="time">Time</h3><ul><li>2024.Aug</li></ul><h3 id="key-words">Key Words</h3><ul><li>one-stage detector</li><li>different configurations to customie different model components</li><li>efficient while reducing computational resource requirements</li></ul><h3 id="总结">总结</h3><ol type="1"><li>YOWOv3是YOWOv2的增强版，提供了更多的approach,用了不同的configurations来定制不同的model，YOWOv3比YOWOv2更好。</li><li>STAD是计算机视觉中一个常见的任务，涉及到检测<strong>location(bbox),timing(exact frame),and type(class of action)</strong>，需要对时间和空间特征进行建模。有很多的方法来解决STAD的问题，例如ViT，ViT的效果很好，但是计算量比较大。例如Hieramodel由超过600M的参数，VideoMAEv2由超过1B的参数，增加了训练的成本和消耗。为了解决STAD问题，同时最大程度减弱训练和推理时间的成本，有人提出用了YOWO方法，虽然可以做到实时，但是也有限制：不是一个efficientmodel with low computationalrequirements。框架的作者已经停止维护了，但是还有很多的问题。本文的contribution如下：<ul><li>new lightweight framework for STAD</li><li>efficient model</li><li>multiple pretrained resources for application：creating a range ofpretrained resources spanning from lightweight to sophisticated modelsto cater to diverse requirements for real-world applications。</li></ul></li></ol><span id="more"></span><ol start="3" type="1"><li><p>YOWO的架构过时了，缺乏sophistication和davancements seen incontemporarymodels，限制了它的applicability和性能。YOWOv2基于YOWO，用了anchor-freeobjectdetection和FPN，提高了性能。但是增加了GLOP，和高效轻量的model的目标不相符。</p></li><li><p><strong>Framework</strong>：YOWOv3采用了<strong>two-stream</strong>网络，包括连个processingstreams，第一个用来提取 <strong>spatial information and context from theimage using a 2D CNN</strong>，第二个stream，一个3DCNN，主要用来提取temporalinformation和motion。两个stream的输出结合到一起，来得到包含关于视频的spatial和temporal的信息。最后，用一个CNNlayer基于这些extracted features进行预测。</p><ul><li><strong>Spatial Feature Extractor</strong>：模型需要一个spaitalfeature extracto来提供关于location的信息。为了满足这个目的，采用了YOLOv8model，去掉了其中的detection layer，输入这个module的是size为<spanclass="math inline">\([3,H,W]\)</span>的feature map，代表final frame ofthe input video。通过利用pyramid network架构，输出包含3个不同level的feature maps：<spanclass="math inline">\(F_{lv1}:[{C}_{2D},\frac{H}{8},\frac{W}{8}],F_{lv2}:[{C_{2D}},\frac{H}{16},\frac{W}{16}]\)</span>，<spanclass="math inline">\(F_{lv3}:[C_{2D},\frac{H}{32},\frac{W}{32}].\)</span></li><li><strong>Decoupled head</strong>：decoupled head是用来separate分类和回归的任务。<strong>YOLOXmodel团队发现，在早期的模型中，用单个feature map来做分类和回归 madetraining morechallenging</strong>。因此，用了相似的approach，用两个独立的CNN streamsfor each task，来增强模型的comprehension。</li></ul><p><spanclass="math display">\[F_{cls}=Conv_{cls2}(Conv_{cls1}(x))\\F_{box}=Conv_{box2}(Conv_{box1}(x))\]</span></p><p>2D backbone的输出包含3个feature map at different levels，每个featuremap 送到Decoupled Head中来产生<strong>two feature maps forclassification and regression</strong>，DecoupledHead的输入是一个tensor，<span class="math inline">\(F_{lv} :[C_{2D},H_{lv},W_{lv}]\)</span>,输出两个相同shape的tensors：<spanclass="math inline">\(F_{lv}:[C_{inter},H_{lv},W_{lv}].\)</span></p><ul><li><p><strong>temporal motion featureextractor</strong>：为了增强预测action label的精确度，用了3DCNN模型，采用了I3D的模型，输入3D backbone的是一个tensor <spanclass="math inline">\(F_{3D}：[3, D,H,W]\)</span>，是整个视频，输出是一个tensor：<spanclass="math inline">\(F_{3D}:[C_{3D},1,\frac{H}{32},\frac{W}{32}]\)</span>。</p></li><li><p><strong>Fusion Head</strong>：Fusion head用于整合2D CNN和3D CNNstreams的特征。输入这个layer包含两个tensors：<spanclass="math inline">\(F_{lv}:[C_{inter},H_{lv},W_{lv}]\)</span>，<spanclass="math inline">\(F_{3D} :[C_{3D},1,\frac{H}{32},\frac{W}{32}]\)</span>。首先 <spanclass="math inline">\(F_{3D}\)</span>被squeeze to shape <spanclass="math inline">\([C_{3D},\frac{H}{32},\frac{W}{32}]\)</span>，然后，upscale来matchdimension <span class="math inline">\(H_{lv}\)</span> 和 <spanclass="math inline">\(W_{lv}\)</span>。接下来，<spanclass="math inline">\(F_{3D}\)</span> 和 <spanclass="math inline">\(F_{lv}\)</span> 被concatenated，来得到 tensor<span class="math inline">\(F_{concat}\)</span>，最后， <spanclass="math inline">\(F_{concat}\)</span>输入到 CFAMmodule，这是一个attention mechanism。CFAM module的输出是一个feature map<spanclass="math inline">\(F_{final}:[C_{inter},H_{lv},W_{lv}]\)</span>。</p></li><li><p><strong>Detection Head</strong>：有人指出：拥有给<strong>Diarcdistribution</strong>来预测bboxes会使模型训练困难，因此提出了让模型学习更general的distribution，而不是简单地回归到single value。为了减小模型 dependence on selecting hyperparameters forpredefined bboxes as in previousstudies。也用了anchor-free。输入Detection Head的包含两个tensors：<spanclass="math inline">\(F_{cls}\)</span> 和 <spanclass="math inline">\(F_{box}\)</span> for classification and regressiontasks respectively。通过一系列的卷积实现最后的预测： <spanclass="math display">\[Predict_{cls}=\mathrm{conv}(Conv_{cls2}(Conv_{cls1}(F_{cls})))\\Predict_{box}=\mathrm{conv}(Conv_{box2}(Conv_{box1}(F_{box})))\]</span></p></li></ul></li><li><p><strong>Label Assignment</strong>：<strong>用了2个不同的labelassignment mechanisms来match 模型的predictions with the ground truthlabels from the data</strong>：<strong>TAL</strong> and<strong>SimOTA</strong>，SimOTA是OTA的简化版。两个机制都依赖于 <spanclass="math inline">\(d_{predict}\)</span>和 <spanclass="math inline">\(d_{truth}\)</span>之间的相似度测量函数，来performmatching between them。</p><ul><li><strong>TAL</strong>：prediction <spanclass="math inline">\(d_{pred} \in A\)</span> 和 ground truth <spanclass="math inline">\(d_{truth} \in T\)</span>之间的相似度测量函数如下：<span class="math display">\[\begin{aligned}&amp;metric&amp;&amp; =cls_err^{\alpha}box_err^{\beta} \\&amp;cls_err&amp;&amp; =BCE(cls_{pred},cls_{truth}) \\&amp;box_err&amp;&amp; =CIoU(box_{pred},box_{truth})\end{aligned}\]</span></li></ul><p>对于each <span class="math inline">\(d_{truth} \inT\)</span>，将它们和有最高的<spanclass="math inline">\(metric\)</span>的 <spanclass="math inline">\(top_k\)</span> 个 <spanclass="math inline">\(d_{pred}\)</span> 进行匹配，但每个 <spanclass="math inline">\(d_{pred}\)</span>只能和 most one <spanclass="math inline">\(d_{truth}\)</span>进行匹配，如果 <spanclass="math inline">\(d_{pred}\)</span> is in the <spanclass="math inline">\(top_k\)</span> of multiple <spanclass="math inline">\(d_{truth}\)</span>，将有着最高的 <spanclass="math inline">\(CIOU(box_{pred},boxJ_{truth})\)</span>的 <spanclass="math inline">\(d_{pred}\)</span>和<spanclass="math inline">\(d_{truth}\)</span>进行匹配。<strong>另外，只考虑receptive field的center落在 box of <spanclass="math inline">\(d_{truth}\)</span>内、且 receptive field center到box of <span class="math inline">\(d_{depth}\)</span> center的距离不超过</strong>radius<strong>的 <spanclass="math inline">\(d_{pred}\)</span></strong>。</p><p>如果 <span class="math inline">\(d_{pred}\)</span>和 <spanclass="math inline">\(d_{truth}\)</span>匹配，就认为 <spanclass="math inline">\(d_{pred}\)</span>的target是 <spanclass="math inline">\(d_{truth}\)</span>，probability of the target forcorresponding class is set to 1 if they appear,否则就是0。</p><ul><li><strong>SimOTA</strong>：和TAL类似，SimOTA利用了相似度测量，来matchingbetween <span class="math inline">\(d_{pred}\)</span>和 <spanclass="math inline">\(d_{truth}\)</span>： <spanclass="math display">\[\begin{aligned}metric&amp;=BCE(\lambdacls_{pred},cls_{truth})-\alpha\log(\lambda)\\&amp;\lambda=CIoU(box_{pred},box_{truth})\end{aligned}\]</span></li></ul><p>不同于TAL，这里只考虑将 <spanclass="math inline">\(d_{truth}\)</span> 和有<strong>smallestmetric</strong>的 <span class="math inline">\(top_k\)</span>个 <spanclass="math inline">\(d_{pred}\)</span>进行匹配。SimOTA不会去fix <spanclass="math inline">\(top_k\)</span>个 value，而是利用一个方式来估计<span class="math inline">\(top_k\)</span> for each <spanclass="math inline">\(d_{truth}\)</span>。实验表明：用动态的 <spanclass="math inline">\(top_k\)</span>会减慢训练过程，增加额外的计算开销。</p></li><li><p><strong>Loss Function</strong>：用两个loss function对应两个 labelassignment mechanisms。总的loss是由两部分： <spanclass="math display">\[\mathcal{L}=\mathcal{L}_{box}+\mathcal{L}_{cls}\]</span></p></li></ol><p>一个是bbox regression，一个是 loss for label classification，两个loss里有多个subcomponents。对于 <span class="math inline">\(d_{pred} \inN\)</span>， <span class="math inline">\(L_{box}\)</span> =0, <spanclass="math inline">\(cls_{truth}\)</span>也会完全为0.</p><ul><li><strong>TAL</strong>：Loss function为： <spanclass="math display">\[\mathcal{L}=\frac{\mathcal{L}_{box}+\mathcal{L}_{cls}}\omega\]</span></li></ul><p><spanclass="math display">\[\mathcal{L}_{box}=\delta(d_{truth})(\alphaCIoU(d_{pred},d_{truth})+\beta\mathcal{L}_{distribution})\\\mathcal{L}_{cls}=\gammaBCE(cls_{pred},cls_{truth})\]</span></p><p><span class="math inline">\(L_{distribution}\)</span>代表DistributionLoss function。</p><p><spanclass="math display">\[\begin{aligned}\delta(d_{i})&amp;=\sum_{p_{j}\incls_{i}}p_{j}\\\omega&amp;=\sum_{d_{pred}\in\mathcal{A}}\sum_{d_{i}\in\mathcal{M}(d_{pred})}\delta(d_{i})\end{aligned}\]</span></p><p><span class="math inline">\(\alpha，\belta,\gamma\)</span>是超参数，用来 scale components of the <spanclass="math inline">\(L\)</span> function。fix <spanclass="math inline">\(\alpha=7.5, \belta=1.5, \gamma= 0.5\)</span>。</p><ul><li><strong>SimOTA</strong>：loss function： <spanclass="math display">\[\mathcal{L}=\frac{\mathcal{L}_{box}+\mathcal{L}_{cls}}{|\mathcal{P}|}\]</span></li></ul><p><span class="math display">\[\mathcal{L}_{box}=\alphaCIoU(d_{pred},d_{truth})+\beta\mathcal{L}_{distribution}\\\mathcal{L}_{cls}=exp(cls_{t})|cls_{truth}-cls_{pred}|^{\nu}BCE(cls_{pred},cls_{truth})\]</span></p><p><span class="math inline">\(L_{cls}\)</span>是 generalized focal lossfunction，multiplied by a class balancing factor <spanclass="math inline">\(exp(cls_t)\)</span>。</p><p><span class="math display">\[p_i\incls_t=\begin{cases}class_ratio,&amp;p_{truth}\neq0\\1-class_ratio,&amp;p_{truth}=0\end{cases}\]</span></p><p><span class="math inline">\(class_ratio\)</span> 是class balancingfactor,classes have lower frequencies，<spanclass="math inline">\(class_{ratio}\)</span> will be higher。这里 <spanclass="math inline">\(\alpha=5.5,\belta=0.5, \gamma=0.5, \nu =0.5\)</span></p><ol start="7" type="1"><li>实验：在实验中，由于AVAv2.2是一个极度不平衡的dataset，因此，需要额外的方法来减小这个不平衡的影响，来提高overallmAP。解决这个问题的两个方法是：softLabels(qualified Loss)和inclusion ofa class balance term。<strong>对于频繁出现的common classes，<spanclass="math inline">\(class_{ratio}\)</span>会接近0.5, class balancetermexp(cls_{t})会基本不变，意味着如果预测错误的话，loss不会有很大的改变；对于很少出现的classes，它会接近0,模型没有正确预测的话，<spanclass="math inline">\(exp(cls_t)\)</span>会严重地惩罚。这会创造一个bias，能够帮助提高 less commonclasses的预测。</strong>另外，softlabels用在了减小模型overconfidence的影响，特别是出现的很频繁的classes**。<ul><li>对于label assignment one to many, 一个ground truth box会和多个multiple predicted boxes匹配，选择matching的boxes的数量可以被动态的估计或者predetermined。实验表明，选择<spanclass="math inline">\(k\)</span>个能够产生很好的结果，自动估计 <spanclass="math inline">\(top_k\)</span>导致额外增加 <spanclass="math inline">\(6%\)</span>的训练时间。这些结果倾向于将 <spanclass="math inline">\(top_k\)</span>视为一个超参数，而不是用来做优化的方法。</li><li>保留了<strong>Exponential MovingAverage(EMA)</strong>的模型的变体，来评估EMA的影响。结果展示了EMA在初始的epochs中，对模型的性能有很大的影响，在后面的epochs中有很小的影响，表明EMA帮助模型在早期的阶段快速收敛，在之后的训练过程，能提高mAPscore。</li></ul></li><li>看了YOWOv2和YOWOv3的图之后，看上去貌似是一样的，画的不一样，不知道是不是就换了个backbone，具体还是看看代码</li></ol><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/d02a8d30424422b99990cc394ca7d6526efc0a20/3-Figure2-1.png"alt="Structure" /> <span class="math inline">\(figure1^{[1]}\)</span>：overview architecture of YOWOv3</p><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/d02a8d30424422b99990cc394ca7d6526efc0a20/4-Figure3-1.png"alt="CFAM" /> <span class="math inline">\(figure2^{[2]}\)</span>：Overview of CFAM module</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;yowov3-an-efficient-and-generalized-framework-for-human-action-detection-and-recognition1&quot;&gt;YOWOv3:
An Efficient and Generalized Framework for Human Action Detection and
Recognition&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是Nguyen Dang Duc Manh, Duong Viet Hang等人。论文引用[1]:Dang,
Duc M et al. “YOWOv3: An Efficient and Generalized Framework for Human
Action Detection and Recognition.” (2024).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2024.Aug&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;one-stage detector&lt;/li&gt;
&lt;li&gt;different configurations to customie different model components&lt;/li&gt;
&lt;li&gt;efficient while reducing computational resource requirements&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;YOWOv3是YOWOv2的增强版，提供了更多的approach,用了不同的configurations来定制不同的model，YOWOv3比YOWOv2更好。&lt;/li&gt;
&lt;li&gt;STAD是计算机视觉中一个常见的任务，涉及到检测&lt;strong&gt;location(bbox),
timing(exact frame),and type(class of action)&lt;/strong&gt;，
需要对时间和空间特征进行建模。有很多的方法来解决STAD的问题，例如ViT，ViT的效果很好，但是计算量比较大。例如Hiera
model由超过600M的参数，VideoMAEv2由超过1B的参数，增加了训练的成本和消耗。为了解决STAD问题，同时最大程度减弱训练和推理时间的成本，有人提出用了YOWO方法，虽然可以做到实时，但是也有限制：不是一个efficient
model with low computational
requirements。框架的作者已经停止维护了，但是还有很多的问题。本文的contribution如下：
&lt;ul&gt;
&lt;li&gt;new lightweight framework for STAD&lt;/li&gt;
&lt;li&gt;efficient model&lt;/li&gt;
&lt;li&gt;multiple pretrained resources for application：creating a range of
pretrained resources spanning from lightweight to sophisticated models
to cater to diverse requirements for real-world applications。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>YOWOv2</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/26/YOWOv2/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/26/YOWOv2/</id>
    <published>2024-08-26T06:35:29.000Z</published>
    <updated>2024-09-01T02:15:01.524Z</updated>
    
    <content type="html"><![CDATA[<h3id="yowov2-a-stronger-yet-efficient-multi-level-detection-framework-for-real-time-stad1">YOWOv2:A Stronger yet Efficient Multi-level Detection Framework for Real-timeSTAD<sup>[1]</sup></h3><blockquote><p>作者是来自哈工大的 Jianhuan Yang和Kun Dai，论文引用[1]:Yang, Jianhuaand Kun Dai. “YOWOv2: A Stronger yet Efficient Multi-level DetectionFramework for Real-time Spatio-temporal Action Detection.” ArXivabs/2302.06848 (2023): n. pag.</p></blockquote><h3 id="time">Time</h3><ul><li>2023.Feb</li></ul><h3 id="key-words">Key Words</h3><ul><li>combined 2D CNN of diffferent size with 3D CNN</li><li>anchor-free mechanism</li><li>dynamic label assignment</li><li>multi-level detection structure</li></ul><h3 id="总结">总结</h3><ol type="1"><li>YOWOv2利用了3D backbone和2D backbone的优势，来做accurate actiondetection。设计了一个multi-level detectionpipeline来检测不同scales的actioninstances。为了实现这个目标，<strong>构建了一个 简单高效地2D backbonewith FPN，来提取不同level的classification features和regressionfeatures</strong>。对于 3D backbone，采用现有的3D CNN，通过结合3DCNN和不同size的2D CNN，设计了YOWOv2 family,包括:YOWOv2-Tiny，YOWOv2-Medium和YOWOv2-Large。同时引入了<strong>dynamiclabel assignmentstrategy</strong>和<strong>anchor-free</strong>机制，来使得YOWOv2和先进的模型架构一致。YOWOv2比YOWO好很多，同时能够保证实时检测。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>之前的工作中，由于3DCNN的网络的计算量比较大，实时性比较差；因此有人用一个参数共享的2DCNN网络，来提取spatial features frame byframe，然后把他们放到buffer中，在那之后，他们仅处理新的inputframe，将它的spaital feature和features in thebuffer一起形成spatio-temporal features for the finaldetection。然而，这样的pipeline，不能很好地model temporalassociation，实时的detection只能在RGBstreams的条件下实现，当用了光流的时候，尽管表现不错，但是速度下降了很多。之前的工作YOWO，效果很不错，但是也有两个不足。</p><ol type="1"><li><strong>YOWO是一个one-level detector,在一个low-level featuremap上进行最后的detection，损失了small actioninstances的性能。</strong></li><li><strong>YOWO是一个anchor-based 方法，有很多的anchor boxes with manyhyperparameters，例如数量、尺寸和anchor boxes的aspectratio。这些超参数必须仔细地设计，泛化性不够。</strong></li></ol><p>总的来说，设计一个实时的detection framework for spatio-temporalaction detection task仍然是一个挑战。 本文中，提出了一个新的实时的actiondetector，YOWOv2，<strong>YOWOv2包括一个3D backbone和multi-level的2Dbackbone</strong>，辛亏有multi-level 2D backbone withFPN，YOWOv2设计了一个<strong>multi-level detection pipeline来检测actioninstances of different scales</strong>.对于3D backbone，也用一个高效的3DCNN。另外，利用<strong>anchor-free mechanism</strong>，避免了anchorbox的缺点，因为anchor box去掉了，采用了一个<strong>dynamic labelassignmentstrategy</strong>,进一步提高了YOWOv2的多功能性。<strong>通过结合3Dbackbone和不同尺寸的2Dbackbones，构建了多个YOWOv2的模型，包括YOWOv2-Tiny，YOWOv2-Medium和YOWOv2-Large</strong>for the platforms with different computing power。</p><p>相比于YOWO，YOWOv2实现了更好的性能。另外，YOWOv2能够实时运行，相比于其它的实时的actiondetector，YOWOv2实现了更好的性能，contributions如下：</p><ul><li>YOWOv2是一个multi-level detection structure to detect small actioninstances。</li><li>YOWOv2是一个<strong>anchor-free</strong> detection pipeline</li><li>通过结合3D backbones和2D backbones of different sizes for theplatforms with different computing power。</li></ul></li><li><p>相关工作：STAD需要一个actiondetector来locate和identify当前帧中所有的instances。如何提取spatio-temporalfeatures对于精确的action detection是重要的。</p><ul><li>3D CNN-based：一些研究人员用3D CNN来设计actiodetectors，Girdhar用I3D来生产action regionproposals，然后用Transformer完成最后的detection；也有人用3D CNN来encoderinput video，然后用<strong>Transformer with the tuber queries for finaldetection</strong>。虽然3DCNN的方法很成功，但是需要大量的计算。<font color=red>实时性比较差</font></li><li>2D CNN-based：另外一种方式是将spatio-temporalassociations进行解耦，<strong>然后设计一个2D CNN-based action detectorsfor efficient detection</strong>。有人设计了一个one-stage detectionframework,<strong>ActionTubelet(ACT)</strong>：先用SSD来从videoclip中的每帧提取<strong>spatialfeatures</strong>然后stacked，再用一个<strong>detectionhead</strong>来处理stacked spatial features for the finaldetection。然后有人follow ACT的工作，设计了一个<strong>anchor-freeone-stage action detector MovingCenter</strong>。有人用自注意力增强了MOC，然而，这些方法的实时性是仅当输入时RGB的时候才可以，一旦加上了光流，尽管performancegain，但是速度是下降了。Moreover，高质量的光流需要离线获得，不满足在线的操作。</li></ul></li><li><p>Methodology： 给定 <spanclass="math inline">\(K\)</span>个frames的video clip <spanclass="math inline">\(V = \{I_{1},I_{2},\ldots,I_{K}\}\)</span> where<span class="math inline">\(I_K\)</span>是当前帧，YOWOv2用一个高效的3DCNN作为3D backbone来提取spatio-temporal features，<spanclass="math inline">\(F_{ST} \in\mathbb{R}^{\frac{H}{32}\times\frac{H}{32}\times C_{o_{2}}}\)</span>,YOWOv2的2D backbone是一个multi-level 2D CNN，用于输出解耦的multi-levelspatial features <span class="math inline">\(F_{cls} =\{F_{cls_i}\}_{i=1}^{3}\)</span> 和 <span class="math inline">\(F_{reg}= \{F_{reg_i}\}_{i=1}^3\mathrm{~of~}I_K\)</span>，where the <spanclass="math inline">\(F_{cls_{i}}\in\mathbb{R}^{\frac{H}{2^{i+2}}\times\frac{W}{2^{i+2}}\timesC_{o_{1}}}\)</span> 是分类的features，<spanclass="math inline">\(F_{reg_{i}} \in\mathbb{R}^{\frac{H}{2^{i+2}}\times\frac{W}{2^{i+2}}\timesC_{o_{1}}}\)</span>是回归的features。在这两个backbone之后，用了两个channelencoders on each feature map of level to integratefeatures。在这之后，两个额外并行的branches with two <spanclass="math inline">\(3 \times 3\)</span> conv layers followed thechannel encoders 来预测 <span class="math inline">\(Y_{cls_{i}}\in\mathbb{R}^{\frac{H}{2^{i+2}}\times\frac{W}{2^{i+2}}\timesN_{C}}\)</span> for classification。一个confidence branch加在了actionness confidence。</p><ul><li><strong>Design of YOWOv2</strong>：</li></ul><ol type="1"><li><font color=red>2D backbone</font>：2Dbackbone是用来抽取当前帧的multi-level spatialfeature。考虑性能和速度的balance，从先进的objectdetectors中得到了一些ideas。重新用了YOLOv7的backbone和FPN来节省时间，在FPN之后，加了一个额外的<spanclass="math inline">\(1 \times 1\)</span> conv layer 来压缩每个levelfeature map的channel number <span class="math inline">\(F_{s_i}\)</span>to <spanclass="math inline">\(C_o1\)</span>，默认设置为256。加了两个并行的brancheswith two <span class="math inline">\(3 \times 3\)</span> conv layers tooutput decoupled features。</li></ol><p><spanclass="math display">\[F_{cls_{i}}=f_{conv_{2}}^{1}\left(f_{conv_{1}}^{1}\left(F_{S_{i}}\right)\right)\\F_{reg_{i}}=f_{conv_{2}}^{2}\left(f_{conv_{1}}^{2}\left(F_{S_{i}}\right)\right)\]</span></p><p><span class="math inline">\(f^i_{convj}\)</span> 是第<spanclass="math inline">\(i\)</span>个branch的第<spanclass="math inline">\(j\)</span>个 <span class="math inline">\(3 \times3\)</span> conv layer。</p><p>在YOWOv2的框架中，2D backbone输出3个level的解耦的feature maps。 <spanclass="math inline">\(F_{cls} = \{F_{cls_{i}}\}_{i=1}^{3}\)</span> 和<span class="math inline">\(F_{reg} =\{F_{reg_{i}}\}_{i=1}^{3}\)</span>，为了方便，称2Dbackbone的为FreeYOLO。通过控制FreeYOLO的depth和width，设计了两个不同size的FreeYOLO，FreeYOLO-Tinyfor YOWOv2-Tiny，FreeYOLO-Large forYOWOv2-Medium和YOWOv2-Large。为了加速训练，在COCO上预训练带有一个额外<span class="math inline">\(1 \times 1\)</span> conv layers的2Dbackbone。</p><ol start="2" type="1"><li><font color=red>3D backbone</font>：3D backbone用来从videoclip中提取spatio-temporal features <spanclass="math inline">\(F_{ST}\)</span> for spatio-temporalassociation。采用高效的3D CNN来减小计算，保证实时检测，为了和decoupledspatial features融合，简单地upsample <spanclass="math inline">\(F_{ST}\)</span> 来得到 <spanclass="math inline">\(\{F_{STi}\}_{i=1}^{3}\)</span>，公式如下：</li></ol><p><spanclass="math display">\[\begin{aligned}&amp;F_{ST_{1}}=\mathrm{Upsample}_{4\times}\left(F_{ST}\right)\\&amp;F_{ST_{2}}=\mathrm{Upsample}_{2\times}\left(F_{ST}\right)\\&amp;F_{ST_{3}}=F_{ST}\end{aligned}\]</span></p><p>上采样操作是为了在空间维度上对齐 <span class="math inline">\(F_{ST_i}\in \mathbb{R}^{\frac{H}{2^{i+2}}\times\frac{H}{2^{i+2}}\timesC_{o_2}}\)</span> 和 <span class="math inline">\(F_{cls_i}\)</span>和<span class="math inline">\(F_{reg_i}\)</span>。</p><ul><li><font color=red>ChannelEncoder</font>：YOWO提出了ChannelEncoder，用来融合2D 和3D backbone出来的features。给定一个 <spanclass="math inline">\(F_{S} \in \mathbb{R}^{H_{o}\times W_{o}\timesC_{o_{1}}}\)</span> 和<span class="math inline">\(F_{ST} \in\mathbb{R}^{H_{o}\times W_{o}\times C_{o_{2}}}\)</span>，这个ChannelEncoder首先在channel dimension上进行concatenates，用两个conv layerfollowed a BN 和LeakyReLU来实现主要的channel integration： <spanclass="math display">\[F_f=f_{conv_2}\left(f_{conv_1}\left(\text{Concat}\left[F_S,F_{ST}\right]\right)\right)\]</span></li></ul><p><span class="math inline">\(F_{f} \in \mathbb{R}^{H_{o}\timesW_{o}\times C_{o_{3}}}\)</span>，Concat是要给channelconcatenation操作。然后 <spanclass="math inline">\(F_f\)</span>reshape成<spanclass="math inline">\(F_{f_{2}}\in\mathbb{R}^{C_{o_{3}}\timesH_{o}W_{o}}\)</span>，用来做之后的 self-attention mechanism inspired byDANet，这样包括不同levels的features能够完全的集成。</p><p><spanclass="math display">\[F_{f_{3}}=\text{Softmax}\left(F_{f_{2}}F_{f_{2}}^{T}\right)F_{f_{2}}\]</span></p><p>最后，<span class="math inline">\(F_{f_{3}} \in \mathbb{R}^{C_{o_{3}}\times H_{O}W_{o}}\)</span> 通过另一个conv layer被reshape成 <spanclass="math inline">\(F_{f} \in \mathbb{R}^{H_{o}\times W_{o}\timesC_{o_{3}}}\)</span>。</p><ul><li><font color=red>Decoupled fusion head</font>：YOWOv2中，2Dbackbone输出当前帧的decoupled spatial features，3D backbone输出的是videoclip上采样的 <spanclass="math inline">\(F_{ST}\)</span>。<strong>注意到，<spanclass="math inline">\(F_{cls_{i}}\)</span>和<spanclass="math inline">\(F_{reg_{i}}\)</span>包含不同的语义信息，因此需要将它们分别和<spanclass="math inline">\(F_{ST_{i}}\)</span>融合</strong>，因此设计了一个<strong>decoupled fusion head</strong> 来独立地融合 <spanclass="math inline">\(F_{ST_{i}}\)</span> into <spanclass="math inline">\(F_{cls_{i}}\)</span>，<spanclass="math inline">\(F_{reg_{i}}\)</span>。</li></ul><p><span class="math display">\[F_{cls_{i}}^{f}=\text{ChannelEncoder}(F_{cls_{i}},F_{ST_{i}})\\F_{reg_{i}}^{f}=\text{ChannelEncoder}(F_{reg_{i}},F_{ST_{i}})\]</span></p><p>在feature aggregation之后，用两个并行的branches on each level来做最后的detection，设计很简单，一个是分类的branch，一个是boxregression branch。</p><p>对于分类branch，输出分类的预测 <spanclass="math inline">\(Y_{cls_{i}}\)</span>, <spanclass="math inline">\(Y_{cls_{i}}\)</span> 表示 <spanclass="math inline">\(Y_{cls_{i}}\)</span>上面的每个空间位置的actioninstances的概率。 <span class="math inline">\(N_c\)</span>是actionclasses的数量。Taking <spanclass="math inline">\(F^f_{cls_{i}}\)</span>，分支应用两个 <spanclass="math inline">\(3 \times 3\)</span> conv layers，每个有 <spanclass="math inline">\(C\)</span>个filters，followed by SiLUactivations。最后，一个 conv layer with <spanclass="math inline">\(N_c\)</span> filters和sigmoid activations来输出<span class="math inline">\(N_c\)</span>个binary predictions per spatialposition。</p><p>对于box regression 分支，输出box regression prediction <spanclass="math inline">\(Y_{reg_{i}}\)</span>，<spanclass="math inline">\(Y_{reg_{i}}\)</span>代表每个空间位置的4个relativeoffsets。另外，一个额外的 $1 $ conv layer with 1filter加在了这个分支上for actioness confidence prediction，<spanclass="math inline">\(Y_{conf_{i}}\)</span>，注意到，在每个spatialposition，没有anchor box，<strong>YOWOv2是 anchor-free</strong>。</p><ul><li><strong>Loss assignment</strong>: 因为YOWOv2是anchor-free的actiondetector without any anchor boxes，multi-lelve labelassignment重要。最近，dynamic labelassignment在目标检测领域很成功，受YOLOX的启发，用<strong>SimOAT</strong>for the label assignment of YOWOv2。具体地，计算所有predictedbboxes和ground truths之间的cost。每个ground truth分配了<spanclass="math inline">\(top_k\)</span>个预测的bboxes with leastcost，<span class="math inline">\(k\)</span>是由预测的bboxes和targetbboes之间的IoU决定的。</li></ul><p><spanclass="math display">\[c_{ij}\left(\hat{a}_i,a_j,\hat{b}_i,b_j\right)=L_{cls}(\hat{a}_i,a_j)+\gammaL_{seg}(\hat{b}_i,b_j)\]</span></p><p><span class="math inline">\(\hat{\alpha}_i\)</span>和 <spanclass="math inline">\(\alpha_j\)</span>是分类的预测和target，<spanclass="math inline">\(\hat{b}_i\)</span>和 <spanclass="math inline">\(b_j\)</span>是回归的预测和target。<spanclass="math inline">\(\gamma\)</span>是cost平衡系数。</p><ul><li><strong>Loss function</strong>：<spanclass="math display">\[\begin{aligned}L(\{a_{x,y}\},\{b_{x,y}\},\{c_{x,y}\})&amp;=\frac{1}{N_{pos}}\sum_{x,y}L_{conf}(\hat{c}_{x,y},c_{x,y}) \\&amp;+\frac{1}{N_{pos}}\sum_{x,y}\mathbb{I}_{\{\hat{a}_{x,y}&gt;0\}}L_{cls}(\hat{a}_{x,y},a_{x,y})\\&amp;+\frac{\lambda}{N_{pos}}\sum_{x,y}\mathbb{I}_{\{\hat{a}_{x,y}&gt;0\}}L_{reg}(\hat{b}_{x,y},b_{x,y})\end{aligned}\]</span></li></ul><p><span class="math inline">\(L_cls\)</span>是binarycross-entropy，<spanclass="math inline">\(L_{reg}\)</span>是GIoU损失，<spanclass="math inline">\(a_{x,y}\)</span>，<spanclass="math inline">\(b_{x,y}\)</span>和<spanclass="math inline">\(c_{x,y}\)</span>分别是分类的预测、回归的预测和confidenceprediction。<span class="math inline">\(\hat{a}_{x,y}\)</span>，<spanclass="math inline">\(\hat{b}_{x,y}\)</span>和<spanclass="math inline">\(\hat{c}_{x,y}\)</span> 是groundtruths。<spanclass="math inline">\(\mathbb{I}_{\{\hat{a}_{x,y}&gt;0\}}\)</span>是indicatorfunction，<span class="math inline">\(\hat{a}_{x,y}\)</span> &gt;0的时候为1，否则为0。<spanclass="math inline">\(N_{pos}\)</span>是positive的预测的数量。<spanclass="math inline">\(\lambda\)</span>是损失的平衡系数，在实验中为5.</p></li><li><p>实验：实验中表明，<strong>decoupled featurefusion</strong>是有必要的，因为<strong>categorical 和regressivefeatures的语义信息不一样</strong>；3DCNN和光流的方法都会导致计算量过大，很难保证实时性。</p></li></ol><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/5f039c1410a764ad673dbb3336d360fad9e4e9e6/2-Figure1-1.png"alt="Structure" /> <span class="math inline">\(Fig. \ 1^{[1]}\)</span>.Overview of YOWOv2. YOWOv2 uses upsampling operation to align thespatio-temporal features output by the 3D backbone with the spatialfeatures of each level output by the 2D bakcbone and uses the Decoupledfusion head to achieve the fusion of the two features on each level.Finally, YOWOv2 outputs the multi-level confidence predictions,classification predictions, and regression predictions respectively.</p><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/5f039c1410a764ad673dbb3336d360fad9e4e9e6/3-Figure2-1.png"alt="channel encoder" /> <span class="math inline">\(Fig. \2^{[1]}\)</span>. Overview of ChannelEncoder. It contains the channelfusion and channel self-attention mechanism, which are both used to fuse2D and 3D features.</p><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/5f039c1410a764ad673dbb3336d360fad9e4e9e6/4-Figure3-1.png"alt="Coupled fusion head" /> <span class="math inline">\(Fig. \3^{[1]}\)</span>. Coupled fusion head. In the coupled head, the spatialfeatures from the 2D backbone is also coupled which means that theparallel 3 × 3 conv layers after the FPN are removed.</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;yowov2-a-stronger-yet-efficient-multi-level-detection-framework-for-real-time-stad1&quot;&gt;YOWOv2:
A Stronger yet Efficient Multi-level Detection Framework for Real-time
STAD&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自哈工大的 Jianhuan Yang和Kun Dai，论文引用[1]:Yang, Jianhua
and Kun Dai. “YOWOv2: A Stronger yet Efficient Multi-level Detection
Framework for Real-time Spatio-temporal Action Detection.” ArXiv
abs/2302.06848 (2023): n. pag.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2023.Feb&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;combined 2D CNN of diffferent size with 3D CNN&lt;/li&gt;
&lt;li&gt;anchor-free mechanism&lt;/li&gt;
&lt;li&gt;dynamic label assignment&lt;/li&gt;
&lt;li&gt;multi-level detection structure&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;YOWOv2利用了3D backbone和2D backbone的优势，来做accurate action
detection。设计了一个multi-level detection
pipeline来检测不同scales的action
instances。为了实现这个目标，&lt;strong&gt;构建了一个 简单高效地2D backbone
with FPN，来提取不同level的classification features和regression
features&lt;/strong&gt;。对于 3D backbone，采用现有的3D CNN，通过结合3D
CNN和不同size的2D CNN，设计了YOWOv2 family,
包括:YOWOv2-Tiny，YOWOv2-Medium和YOWOv2-Large。同时引入了&lt;strong&gt;dynamic
label assignment
strategy&lt;/strong&gt;和&lt;strong&gt;anchor-free&lt;/strong&gt;机制，来使得YOWOv2和先进的模型架构一致。YOWOv2比YOWO好很多，同时能够保证实时检测。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>TAAD</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/TAAD/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/TAAD/</id>
    <published>2024-08-24T08:53:52.000Z</published>
    <updated>2024-08-30T03:11:10.636Z</updated>
    
    <content type="html"><![CDATA[<h3id="spatio-temporal-action-detection-under-large-motion1">Spatio-TemporalAction Detection Under Large Motion<sup>[1]</sup></h3><blockquote><p>作者是来自ETHZ的Gurkirt Singh, Vasileios Choutas, Suman Saha, FisherYu和Luc Van Gool。论文引用[1]:Singh, Gurkirt et al. “Spatio-TemporalAction Detection Under Large Motion.” 2023 IEEE/CVF Winter Conference onApplications of Computer Vision (WACV) (2022): 5998-6007.</p></blockquote><h3 id="time">Time</h3><ul><li>2022.Oct</li></ul><h3 id="key-words">Key Words</h3><ul><li>track information for feature aggregation rather than tube fromproposals</li><li>3 motion categories: large motion、medium motion、small motion</li></ul><h3 id="总结">总结</h3><ol type="1"><li>当前的STAD的tubedetection的方法经常将一个给定的<strong>keyframe</strong>上的bboxproposal扩展成一个<strong>3D temporalcuboid</strong>，然后从邻近帧进行poolfeatures。如果actor的位置或者shape表现出了large 2D motion和variabilitythrough frames，这样的pooling不能够积累有意义的spaito-temporalfeatures。在这个工作中，作者旨在研究<strong>cuboid-aware featureaggregation in action detection under largeaction</strong>。进一步，提出了在<strong>largemotion</strong>的情况下，通过<strong>tracking actors和进行temporalfeature aggregation along the respective tracks</strong>增强actorfeature representation，定义了在不同的固定的time scales下的<strong>actormotion的IoU</strong>。有large motion的action会随着时间导致lowerIoU，slower actions会随着时间维持higher IoU。作者发现<strong>track-awarefeature aggregation持续地实现了很大的提升in actiondetection</strong>。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>目前的很多工作聚焦于通过复杂的上下文建模和更大的backbone，或者利用光流stream，来提升actiondetection的性能。上述方法用了cuboid-aware temporal pooling for featureaggregation。在这个工作中，旨在研究actioninstance的不同角度的情况下的cuboid-aware action detection。<strong>largeobject motion</strong>有不同的原因：例如fast camera motion，fastaction，body shape deformation due to pose change, or mixed camera andaction motions。所有的这些原因会造成sub-optimal featureaggregation，导致action classification的错误。</p></li><li><p>作者将actions分成了3类：largemotion、medium-motion和small-motion。这个区分是基于同一个actor overtime的boxes的IoU,可以用actors的ground truthtubes来计算。研究cuboid-aware的基线方法在不同motioncategories上的表现，不用context features或者long-termfeatures之类的。因为large-motion在很短的时间窗口中发生的很快。<strong>large-motioncases, IoU会比较小，因此一个3D cuboid-aware featureextractor不能获取actor location上的features throughtoutaction</strong>。为了解决large-motion，提出了随着时间来<strong>track theactor</strong>，用 <strong>Track-of-InterestAlign(TOI-Align)</strong>来extract features，resulting in <strong>TrackAware Action Detector(TAAD)</strong>。Further，研究了TOI-Alignedfeatures上的不同类型的 feature aggregation modules for TAAD网络。</p><p>为了这个目的，有以下的contributions：</p><ul><li>是第一个用evaluaiton metrics for each type ofmotion，来研究large-motion action detection。</li><li>提出了用 <strong>tube/track-aware feature aggregationmodules</strong>来处理largemotion，这种类型的module实现了很好的提升。</li></ul></li><li><p>Related work: Action recognition models提供了很强的videorepresentation models；action detection是一个在largemotion下的理解actions的问题。主要关注于STAD 问题，这里一个<strong>actioninstance被定义为随着时间的一系列的linked bboxes</strong>。最近的onlineactiondetection的方法的性能直追离线的STAD的方法。MultiSports数据集有更多的fine-grainedactionclass；有多个actors在同一个视频里执行不同类型的action。另外，MultiSports是一个<strong>denselyannotated</strong>。 很多的方法关注于keyframe，基于action detection onAVA，long-term feature banks起到了重要作用，获得了一些temporalcontext，但是没有actors之间的temporalassociations；另外也有人研究了actors和object之间的interactions。这些以上的方法都是用<strong>cuboid-awarepooling for local feature aggregation</strong>，作者们发现，当motion isquick和large，这不是最理想的方法。作者这里用SlowFast作为基线方法。</p><p><strong>Weinzaepfel</strong>是首个用<strong>tracking for actiondetection</strong>的工作，用一个tracker来解决tube generationpart的<strong>linking</strong>问题，给定tracks中的bboxesproposals，可以on a frame-by-frame basis做actionclassification。作者提出通过<strong>pooling features from within entiretracks</strong>来处理action detection。</p></li><li><p>Methodology：提出了处理large motions的方法，称之为Track AwareAction Detector(TAAD)，在视频中trackactor，同时，用了一个神经网络designed for videorecognition，来从每个clip中提取特征。用track boxes和video features，poolper-frame features with a RoI-Align operation，之后，<strong>TemporalFeature Aggregation module</strong>得到per-trackfeatures，计算单个feature vector，这里classifier预测最后的actionlabel。</p><ul><li><p><font color=red>Baseline ActionDetector</font>：选择Slowfast作为videobackbone，原因是它相对于大型的transformer模型仍然有竞争力 on the task ofSTAD；另外，Slowfast比其它的transformer的方法更高效。而且提供了不同temporalscales的features，有不同的temporalscales是重要的，特别是作者旨在处理fast/large motions,where a smallerscale是必须的；最后Slofast是MultiSports和UCF24datasets默认的backbone。用基于Slowfast的架构的pySlowFast with aResNet-50来执行baseline。首先，增加background frame,作为训练actiondetector的额外的negative samples。接下来，用一个multiclassclassifier来代替multi-label，switching from a binary cross entropy perclass to a cross entropy loss(CE-loss)。最后，加一个<strong>downward FPNblock</strong></p></li><li><p><font color=red>Tracker</font>：用YOLOv5-DeepSort的classagnostic版本作为tracker，这个是基于YOLOv5和TrochReID，fine-tuneYOlOv5的medium size作为detection model for personclasses。一个预训练的OsNet-x0-25被用作ReID模型。一个high recall、smallnumber of association的tracker对于提高action tubedetection的性能是重要的。<strong>微调detector</strong>也是重要的一步。<strong>Tracker可以被用作bboxesproposal filtering module</strong>,有时候detector产生多个high scoringdetections，有些会导致falsepositives,这些detections不和任何的tracks相匹配，因为它们时序上不是一致的。tracks产生的proposals能够在测试的时候用上。</p></li><li><p><font color=red>Temporal Feature Aggregation</font>：</p></li></ul><ol type="1"><li><strong>Track-of-Interest Align(TOI-Align)</strong>：SlowFast videobackbone处理输入的clip，产生 <span class="math inline">\(T \times H\times W\)</span> feature tensor，然而trackers返回一个 <spanclass="math inline">\(N_t \times T \times4\)</span>的array，包含物体附近的boxes。RoI-Align将这两个arrays作为输入，得到一个featurearray <span class="math inline">\(N_t \times T \times H \timesW\)</span>，one feature tube per track，在track的length小于inputclip的情况下，在时序上复制最后的available bbox。</li><li><strong>Featureaggregation</strong>：为了预测keyframe中的bbox的label，需要aggregatefeatures across time andspace。首先，在TOI-Align提取出来的features上，在空间维度上做一个averagepooling，然后执行Temporal Feature Aggregation的一个变体。<ul><li>Max-pooling over temporal axes(MaxPoo)</li><li>A sequence of temporal conv(TCN)</li><li>A temporal variant of Atrous Spatial PyramidPooling(ASPP)，修改Detectron2中的ASPP，用1D conv代替2D也尝试了ConvNeXt和VideoSwin的temporalversion，然而，这些导致不稳定的训练，即使调整学习率和其它的超参数，实验中，仅用了temporalconv for TCN module的一个layer。</li></ul></li></ol><ul><li><font color=red>Tube Construction</font>：<strong>Video-level tubedetection要求从per-frame detections中构建actiontubes。这个过程分为两步：首先将proposals连接起来形成tubehypotheses；然后trim这些proposals，得到有action的部分。可以将这两部视为trackingstep加上一个temporal action detection step</strong>。大多数的<strong>action tube detectionmethod</strong>在第一步用一个贪心的proposal连接算法；<strong>由于TAAD已经有tracks，所以不需要linkingstep</strong>。action tracks的temporal trimming是通过<strong>labelsmoothingoptimisation</strong>来做的，之前的很多工作都用到了，具体地，用了<strong>class-wisetemporal trimming</strong>。</li></ul></li><li><p>实验：</p></li><li><p>讨论和结论：在实验中发现，TAAD用<strong>tracking information forfeature aggregation，而不是从proposalboxes得到的tube</strong>,提高了性能，这不意味着没有了提高的空间，作者的方法对于<strong>tracker的性能比较敏感</strong>，因为这是pipeline的第一步。用更好的SOTA的tracker和persondetector，可以进一步提高性能。通过在<strong>TAAD中加上spatial/actorcontext modelling, long-term temporal context 或者一个transformer heador backbone</strong>，也能提高性能。 关于motion分类的定义可以说是不精确，不同于MS COCO中的object size类别，motion类别不容易定义。除了普遍的复杂的相机运动和quick actormotion外，必须特别注意错误标记。</p></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/978129084bd486157f3cc0997204554956a7cca3/4-Figure4-1.png"alt="Pipleline of TAAD" /><figcaption aria-hidden="true">Pipleline of TAAD</figcaption></figure><p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Proposed TrackAware Action Detector (TAAD): Given an input clip with T frames, weextract features using a video recognition network and Nt per-actortracks from a tracker. The TOI-Align operation extracts per-trackfeatures from the entire video sequence, using an RoI-Align operationand the track boxes, returning a <span class="math inline">\(N_t × T ×C\)</span> feature array. Next, the Temporal Feature Aggregation (TFA)module aggregates the features along the temporal dimension and passesthe resulting <span class="math inline">\(N_t × C\)</span> array to theaction classifier that predicts the action label.</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;spatio-temporal-action-detection-under-large-motion1&quot;&gt;Spatio-Temporal
Action Detection Under Large Motion&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自ETHZ的Gurkirt Singh, Vasileios Choutas, Suman Saha, Fisher
Yu和Luc Van Gool。论文引用[1]:Singh, Gurkirt et al. “Spatio-Temporal
Action Detection Under Large Motion.” 2023 IEEE/CVF Winter Conference on
Applications of Computer Vision (WACV) (2022): 5998-6007.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2022.Oct&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;track information for feature aggregation rather than tube from
proposals&lt;/li&gt;
&lt;li&gt;3 motion categories: large motion、medium motion、small motion&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;当前的STAD的tube
detection的方法经常将一个给定的&lt;strong&gt;keyframe&lt;/strong&gt;上的bbox
proposal扩展成一个&lt;strong&gt;3D temporal
cuboid&lt;/strong&gt;，然后从邻近帧进行pool
features。如果actor的位置或者shape表现出了large 2D motion和variability
through frames，这样的pooling不能够积累有意义的spaito-temporal
features。在这个工作中，作者旨在研究&lt;strong&gt;cuboid-aware feature
aggregation in action detection under large
action&lt;/strong&gt;。进一步，提出了在&lt;strong&gt;large
motion&lt;/strong&gt;的情况下，通过&lt;strong&gt;tracking actors和进行temporal
feature aggregation along the respective tracks&lt;/strong&gt;增强actor
feature representation，定义了在不同的固定的time scales下的&lt;strong&gt;actor
motion的IoU&lt;/strong&gt;。有large motion的action会随着时间导致lower
IoU，slower actions会随着时间维持higher IoU。作者发现&lt;strong&gt;track-aware
feature aggregation持续地实现了很大的提升in action
detection&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>TubeR</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/TubeR/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/TubeR/</id>
    <published>2024-08-24T08:53:46.000Z</published>
    <updated>2024-08-26T09:28:55.720Z</updated>
    
    <content type="html"><![CDATA[<h3 id="tuber-tubelet-transformer-for-video-action-detection1">TubeR:Tubelet Transformer for Video Action Detection<sup>[1]</sup></h3><blockquote><p>作者是来自阿姆斯特丹大学、罗格斯大学和AWS AI Labs的JiaojiaoZhao、Yanyi Zhang等人。论文引用[1]:Zhao, Jiaojiao et al. “TubeR: TubeletTransformer for Video Action Detection.” 2022 IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR) (2021): 13588-13597.</p></blockquote><h3 id="time">Time</h3><ul><li>2021.April</li></ul><h3 id="key-words">Key Words</h3><ul><li>tubelet queries, tubelet-attention</li><li>in sequence-to-sequence manner</li><li>scales well to longer video clips</li><li>end-to-end without person detectors, anchors or proposals</li></ul><h3 id="总结">总结</h3><ol type="1"><li>不同于现有的依赖于离线检测器或者人工设计的actor-positionalhypotheses like proposals or anchors，提出了一个通过同时进行actionlocalization和recognition from a singlerepresentation，直接检测视频里的actiontubelet的方法。TubeR学习一系列的tubelet queries，利用tubelet-attentionmodule来model video clip里的动态的spatio-tempralnature。相比于用actor-positional hypotheses in the spatio-temporalspace，它能够有效的强化模型的能力。对于包含transitional states或者scenechanges的视频，提出了一个context aware classificationhead，来利用short-term和long-term context to strengthen actionclassification，和一个action switch regression head来检测精确的时序上的行为范围。TubeR直接产生不同长度的actiontubelets，对于长的视频clips，也能保持一个比较好的结果。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>Actiondetection是一个复杂的任务，要求逐帧的任务的定位、将检测到的personinstances连接成actiontubes、预测action的类别。STAD中两种路径很流行：frame-level和video-level。<strong>frame-level的detection在每一帧上独立地进行检测和分类，然后将per-framedetections连接起来，形成连贯的actiontubes</strong>；为了弥补时序信息的缺失，一些方法简单地重复2D proposals或者离线的人物检测 over time，来得到时空特征。<strong>tubelet-leveldetection，直接生成spatio-temporal volumes from a videoclip，来获得连贯和动态的natures of actions。通常预测actionlocalization和classification jointly over spatio-temporalhypotheses</strong>。像 3D cuboid proposals。然而， 这些 3Dcuboids仅能够 capture a short period of time, when spatial location of aperson changes as soon as they move or due to cameramotion。Ideally，这些模型能够用灵活的spatio-temporal tubelets，能够trackthe person over a longer time。但是，large configuration space of such aparameterization限制了之前的方法只能用shortcuboids。这里，作者提出了一个tubelet-level的detectionapproach，<strong>能够同时定位和识别action tubelets in a flexiblemanner，使得tubelets能够随着时间改变size和location</strong>。这使得系统能够利用更长的tubelets，在更长的时间上汇聚人物和它们的行为的视觉信息。</p><p>从NLP中的sequence-to-sequence获得启发，特别是机器翻译和它在目标检测上的应用，DETR。DETR是一个frame-level的actiondetection。这里，用decoderqueries来表示整个视频序列上的人物和它们的行为，不限制tubelets是一个固定的cuboids。</p><p>提出了<font color=red>tubelet-transformer</font>：称之为<strong>TubeR for localizing and recognizing actions from a singlerepresentation</strong>。基于DETR的框架，TubeR学习一系列的 tubeletqueries，来从spatio-temporal video representation中pull action-specifictubelet-level features。TubeR的包括特别的 <strong>spatial and temporaltubelet attention</strong>，使得tubelets能够随着时间在它们的spatiallocation 和scale上没有限制。克服了之前对于cuboidsd限制。TubeR随着时间在一个 tubelet里 回归bboxes。考虑到tubelets之间的时序关联。汇聚visual features over thetubelet to classifyactions。这个涉及表现很好，但是并没有比用离线人物检测器的方法好很多。猜想是因为querybased features缺少全局上下文，only look at a singleperson的话，很难对涉及到的relationships 行为例如：listening-to或者talking-to进行分类。因此，提出了一个 <strong>contextaware classification head</strong>， along with the tubelet feature,利用完整的clipfeatures，分类头能够得出上下文信息。这个设计能够使得网络将persontubelet和完整的scene context(where tubeletappears)关联起来。这个设计的限制就是 *context feature仅能够从 tubelet占据的same clip中得到。包含long term contextualfeatures对于最后的行为分类很重要。因此受到要锁和存储tubelet附近的视频内容的contextual features的启发，引入了memorysystem。用相同的feature injection策略，将long term contextual memory给到分类头。</p><p>主要的贡献如下：</p><ol type="1"><li>提出来TubeR：一个tubelet-level的transformer 框架 for human actiondetection</li><li>tubelet query和attention basedformulation能够产生任意位置和尺寸的tubelets。</li><li>context aware classification head能够汇聚short-term和long-term上下文信息。</li></ol></li><li><p>相关工作：</p><ul><li><strong>frame-level action detection</strong>：用2D positionalhypothese(anchors) 或者离线的person detector on a keyframe来定位actors，然后更多地关注提高actionrecognition。通过利用光流分支来包含temporal patterns。其它的用 3DconvNet来获取时序信息来识别行为。不同于frame-level的方法，面向tubelet-levelvideo actiondetection，用一个统一的configuration，来进行定位和识别。</li><li><strong>Tubelet-level action detection</strong>：通过将tubelet作为一个representation unit来 detect actions变得流行了，有人重复 2Danchors per-frame来pooling ROI features，然后stack frame-wisefeatures来预测行为类别。有人依赖严格设计的 3D cuboidproposal，前者直接俄检测tubelets，后者逐步第refines 3D cuboid proposalsacross time。除了box/cuboid anchors，也有人通过centerposition假设，来检测tubeletinstances。基于假设的方法来处理长视频clips有很多困难。通过学习tubeletqueries的子集，来表示tubelets的动态nature。将action detection taskreformulate程一个 sequence-to-sequence学习的问题，在一个tubelet里显式地model temporal correlations。</li><li><strong>Transformer-based action detection</strong>：Girdhar提出来一个video action transformer network for detectingactions。用region-proposal-network forlocalization,通过汇聚actors附近的时空上下文信息，提高actionrecognition.</li></ul></li><li><p>TubeR：TubeR的输入是一个video clip，直接输出一个<strong>tubelet：a sequence of bboxes and the actionlabel</strong>，TubeR是受 DETR的启发，但是将transformer架构 reformulatefor sequence-to-sequence modeling in video。 给定一个video clip $IR^{T_{in} H W C} <span class="math inline">\(，\)</span>T_{in}, H, W,C$分别表示帧数，height,width和channel， TubeR首先用一个 3Dbackbone来提取 video feature <spanclass="math inline">\(F_{\mathrm{b}}\in\mathbb{R}^{T^{\prime}\timesH^{\prime}\times W^{\prime}\times C^{\prime}}\)</span>，<spanclass="math inline">\(T&#39;\)</span> 表示 temporal dimension， <spanclass="math inline">\(C&#39;\)</span>是 featuredimension。用一个transformer的encoder-decoder来transform视频的特征 intoa set of tubelet-specific feature <spanclass="math inline">\(F_{\mathrm{tub}}\in\mathbb{R}^{N\timesT_{\mathrm{out}}\times C^{\prime}}\)</span>，<spanclass="math inline">\(N\)</span>是 tubelets的数量。为了处理长的videoclips，用temporal 下采样，使得 $ T_{out} &lt; T' &lt; T_{in}$，减小memory的 requirement。TubeR 产生了稀疏的sparse，对于短的videoclips，去掉时序下采样，使得 <span class="math inline">\(T_{out} &lt;T&#39; &lt; T_{in}\)</span>，results in dense tubelets。Tubelet回归和associated action classification能够用一个separated task heads同时实现：</p><p><spanclass="math display">\[y_{\mathrm{coor}}=f(F_{\mathrm{tub}});y_{\mathrm{class}}=g(F_{\mathrm{tub}}),\]</span></p><p><span class="math inline">\(f\)</span>表示 tubelet 回归头， <spanclass="math inline">\(y_{coor} \in \mathbb{R}^{N\timesT_{\mathrm{out}}\times4}\)</span> 表示<spanclass="math inline">\(N\)</span>个 tubelets的坐标，each of which isacross <span class="math inline">\(T_{out}\)</span> frames(或者 <spanclass="math inline">\(T_{out}\)</span> sampled frames for longclips)。<span class="math inline">\(g\)</span>表示行为分类头，<spanclass="math inline">\(y_{\mathrm{class}}\in\mathbb{R}^{N\timesL}\)</span>表示 <span class="math inline">\(N\)</span>个 tubelets with<span class="math inline">\(L\)</span> 个可能的labels的行为分类。</p><ul><li><font color=red>TubeREncoder</font>：不同于普通的transformer的encoder，TubeR encoder用于处理3D 时空中的信息。每个 encoder layer由 self-attentionlayer(SA)、两个normalization layer和一个 FFN组成。core attentionlayers的公式如下：</li></ul><p><spanclass="math display">\[F_\mathrm{en}=\mathrm{Encoder}(F_\mathrm{b}),\]</span><spanclass="math display">\[\mathrm{SA}(F_\mathrm{b})=\mathrm{softmax}(\frac{\sigma_q(F_\mathrm{b})\times\sigma_k(F_\mathrm{b})^T}{\sqrt{C^{\prime}}})\times\sigma_v(F_\mathrm{b}),\]</span><spanclass="math display">\[\sigma(*)=\mathrm{Linear}(*)+\mathrm{Emb}_{\mathrm{pos}},\]</span></p><p><span class="math inline">\(F_b\)</span>是 backbone feature,<spanclass="math inline">\(F_{en} \in R^{T&#39;H&#39;W&#39; \timesC&#39;}\)</span>，<span class="math inline">\(C&#39;\)</span>表示dimensional encoded feature embedding。<spanclass="math inline">\(\sigma(*)\)</span> 是线性变换加上 positionalembedding。<span class="math inline">\(Emb_{pos}\)</span>是 3Dpositional embedding。optional temporal down-sampling 能够用在backbonefeatures上，来shrink 输入的sequence length to transformer for bettermemory efficiency。</p><ul><li><p><font color=red>TubeR Decoder</font>：</p><ul><li><p><strong>tubelet query</strong>：基于anchor假设来直接检测tubelets是相当有挑战的。tubelet space along thespatio-temporal dimension相比于single frame bbox space来说是巨大的。考虑FasterRCNN，requires for each position in a feature map with spatialsize <span class="math inline">\(H^{\prime}\times W^{\prime},K(=9)\)</span> anchors，总共有 <spanclass="math inline">\(KH&#39;W&#39;\)</span>个anchors。对于一个across<span class="math inline">\(T_{out}\)</span> frames的tubelet来说，需要<spanclass="math inline">\(KH&#39;W&#39;^{T_{out}}\)</span>个anchors，来保持同样的samplingin space-time。为了减小tubelet space，一些方法通过忽略短的videoclip中的action的空间位移，用 3Dcuboids来近似tubelets。然而，视频clip越长， 3Dcuboids代表的tubelet的精度越低。提出了学习tubelet queries小的子集，<span class="math inline">\(Q{=}\{Q_{1},...,Q_{N}\}\)</span>, <spanclass="math inline">\(N\)</span>是queries的数量。 第 <spanclass="math inline">\(i\)</span>个 tubelet query <spanclass="math inline">\(Q_{i}=\{q_{i,1},...,q_{i,T_{\mathrm{out}}}\}\)</span>包含 <span class="math inline">\(T_{out}\)</span> box query embeddings<span class="math inline">\(q_{i,t} \in R^{C&#39;}\)</span> across <spanclass="math inline">\(T_{out}\)</span> frames。学习一个tubeletquery表示dynamics of a tubelet, 而不是手工设计的 3D anchors。初始化boxembeddings identically for a tubelet query。</p></li><li><p><strong>tubelet attention</strong>：为了model tubeletqueries内的relations，提出来一个 tubelet-attention (TA)module，包含连个self-attention layers。首先有一个 <em>spatialself-attention layer</em>来处理一帧内的box queryembeddings的空间relations。这个layer的intuition是识别actions 受益于interactions between actors，或者between actos and objects in the sameframe。接下来有 <strong>temporal self-attentin layer</strong>来models同一个tubelet里的box query embeddings acrosstiem的correlations。这一层促使 TubeR query 去trackactors，然后产生action tubelets，聚焦于single actors而不是一个fixed areain the frame。TubeR decoder把tubelet attention module用在了 tubeletqueries <span class="math inline">\(Q\)</span>上，来产生 tubelet queryfeature <spanclass="math inline">\(F_{\mathfrak{a}}\in\mathbb{R}^{N\timesT_{\mathrm{out}}\times C^{\prime}}\)</span>：</p><p><span class="math inline">\(F_q = TA(Q)\)</span></p></li><li><p><strong>Decoder</strong>：decoder包含一个 tubele-attentionmodule和一个 cross-attention (CA) layer，用来decode tubelet-specificfeature <span class="math inline">\(F_{tub}\)</span> from <spanclass="math inline">\(F_{en}\)</span> to <spanclass="math inline">\(F_q\)</span>：</p></li></ul><p><spanclass="math display">\[\mathrm{CA}(F_{q},F_{\mathrm{en}})=\mathrm{softmax}(\frac{F_{q}\times\sigma_{k}(F_{\mathrm{en}})^{T}}{\sqrt{C^{\prime}}})\times\sigma_{v}(F_{\mathrm{en}}),\\F_{\mathrm{tub}}=\mathrm{Decoder}(F_{q},F_{\mathrm{en}}).\]</span></p><p><span class="math inline">\(F_\mathrm{tub}\in\mathbb{R}^{N\timesT_\mathrm{out}\times C^{\prime}}\)</span> 是tubelet specificfeatures。有temporal pooling的时候，<span class="math inline">\(T_{out}&lt; T_{in}\)</span>，TubeR产生 <strong>sparse tubelets</strong>，对于<span class="math inline">\(T_{out} = T{in}\)</span>，TubeR产生 densetubelets。</p></li></ul><p><font color=red>Task-Specific Heads</font>：对于每个tubelet，bbox和action classification可以用独立的task-specificheads来处理。这样的设计最大化的减小了计算量。</p><ul><li><strong>Context aware classificationhead</strong>：这个分类用一个简单的linear project就能实现。 <spanclass="math display">\[y_{\mathrm{class}}=\mathrm{Linear_c}(F_{\mathrm{tub}}),\]</span></li></ul><p><span class="math inline">\(y_{class} \in R^{N \timesL}\)</span>表示在 <spanclass="math inline">\(L\)</span>个可能的label上的分类的分数。one foreach tubelet。</p><ol type="1"><li><em>Short-term context head</em>:对于理解sequences，context是重要的。进一步提出利用spatio-temporal videocontext来帮助理解sequence。query the action specific feature <spanclass="math inline">\(F_{tub}\)</span> from some context feature <spanclass="math inline">\(F_{context}\)</span> to strengthen <spanclass="math inline">\(F_{tub}\)</span>，得到了 feature <spanclass="math inline">\(F_{c}\in R^{N \timesC&#39;}\)</span>，用于最后的分类： <spanclass="math display">\[F_\text{c}=\text{CA}(\text{Pool}_t(F_\text{tub}),\text{SA}(F_\text{context}))+\text{Pool}_t(F_\text{tub}).Eq.9\]</span></li></ol><p>这里设置 <span class="math inline">\(F_{context} = F_b\)</span> forutilizing the short-term context in the backbone feature。称之为<strong>Short-term context head</strong>，<spanclass="math inline">\(F_{context}\)</span> 首先用一个自注意力层，然后cross-attenion layer utilizes <spanclass="math inline">\(F_{tub}\)</span> to query from <spanclass="math inline">\(F_{context}\)</span>。<spanclass="math inline">\(F_{c}\)</span>经过线性层，用于最后的分类。</p><ol start="2" type="1"><li><em>Long-term context head</em>：为了利用long-range的时序信息，但是在有限的memory下，采用两阶段的decoder for long-termcontext compression。</li></ol><p><spanclass="math display">\[\mathrm{Emb}_{\mathrm{long}}=\mathrm{Decoder}(\mathrm{Emn}_{n1},\mathrm{Decoder}(\mathrm{Emb}_{n0},F_{\mathrm{long}}).\]</span></p><p>long-term context <spanclass="math inline">\(F_{\mathrm{long}}\quad\in\quad\mathbb{R}^{T_{\mathrm{long}}\timesH^{\prime}W^{\prime}\times C^{\prime}}\)</span> 是一个buffer，包含从<spanclass="math inline">\(2W\)</span>个在时间上concatenated的相邻的clips抽取出来的backbonefeature。为了将long-term video feature buffer压缩到 embedding <spanclass="math inline">\(Emb_{long}\)</span> with a lower temporaldimension，用了两个 stacked decoders with token <spanclass="math inline">\(Emn_{n0}\)</span> 和 <spanclass="math inline">\(Emn_{n1}\)</span>。首先用一个压缩的token <spanclass="math inline">\(Emb_{n0} (n0 &lt; T_{long})\)</span> to query<spanclass="math inline">\(F_{long}\)</span>中重要的信息，得到一个temporaldimension 为 <spanclass="math inline">\(n0\)</span>的中间压缩embedding。然后，进一步利用另外一个压缩的token<span class="math inline">\(Emb_{n1} (n1 &lt; n0)\)</span> to query from中间压缩的embedding，然后得到最后的压缩embedding <spanclass="math inline">\(Emb_{long}\)</span>。<spanclass="math inline">\(Emb_{long}\)</span>包含long-term的视频信息，但是有着 lower temporal dimension <spanclass="math inline">\(n1\)</span>，然后，对 <spanclass="math inline">\(F_b\)</span>和 <spanclass="math inline">\(Emb_{long}\)</span>采用cross-attentionlayer，来得到long-term context feature <spanclass="math inline">\(F_{\mathrm{lt}}\in\mathbb{R}^{T^{\prime}\timesH^{\prime}\times\bar{W}^{\prime}\times C^{\prime}}\)</span>：</p><p><spanclass="math display">\[F_{\mathrm{lt}}=\mathrm{CA}(F_{\mathrm{b}},\mathrm{Emb}_{\mathrm{long}}),\]</span></p><p>设置 <span class="math inline">\(F_{context} = F_{lt} inEq.9\)</span>，来利用 long-term context for classification。</p><p><font color=red>Action Switch regression head</font> <spanclass="math inline">\(T_{out}\)</span> bboxes in a tubelet是用一个 FClayer同时进行回归。</p><p><spanclass="math display">\[y_{\mathrm{coor}}=\mathrm{Linear}_{\mathrm{b}}(F_{\mathrm{tub}}),\]</span></p><p><span class="math inline">\(y_{\mathrm{coor}}\in\mathbb{R}^{N\timesT_{\mathrm{out}}\times4}\)</span>， <spanclass="math inline">\(N\)</span>是 action tubelet的数量， <spanclass="math inline">\(T_{out}\)</span>是一个action tubelet的temporallength。为了去掉tubelet里的non-action boxes。进一步用 FC layer来决定 abox 是否描述了tubelet里actor的行为。称之为 action switch。这个actionswitch 使得能够产生action tubelets witha more precise temporalextent。<span class="math inline">\(T_{out}\)</span> predicted boxes ina tubelet的概率是： <spanclass="math display">\[y_\mathrm{switch}=\mathrm{Linear}_\mathrm{s}(F_\mathrm{tub}),\]</span></p><p><span class="math inline">\(y_\mathrm{switch}\in\mathbb{R}^{N\timesT_\mathrm{out}}\)</span>，对于每个预测的tubelet, each of its <spanclass="math inline">\(T_{out}\)</span> bboxes 包含一个action switchscore。</p><p><font color=red> Losses</font>：4个loss的线性组合是： <spanclass="math display">\[\mathcal{L}=\lambda_{1}\mathcal{L}_{\mathrm{switch}}(y_{\mathrm{switch}},Y_{\mathrm{switch}})+\lambda_{2}\mathcal{L}_{\mathrm{class}}(y_{\mathrm{class}},Y_{\mathrm{class}})\\+\lambda_{3}\mathcal{L}_{\mathrm{box}}(y_{\mathrm{coor}},Y_{\mathrm{coor}})+\lambda_{4}\mathcal{L}_{\mathrm{iou}}(y_{\mathrm{coor}},Y_{\mathrm{coor}}),\]</span></p><p><span class="math inline">\(y\)</span>是模型的输出，<spanclass="math inline">\(Y\)</span>表示ground truth，action switch loss<span class="math inline">\(L_{switch}\)</span> 是一个binarycross-entropy loss，<span class="math inline">\(L_{class}\)</span> crossentropy loss，<span class="math inline">\(L_{box}\)</span> 和 <spanclass="math inline">\(L_{iou}\)</span> 表示per-frame bboxes matchingerror。当 <span class="math inline">\(T_{out} &lt; T_{in}\)</span>，tubelet是sparse，coordinate ground truth <spanclass="math inline">\(Y_{coor}\)</span>是来自对应的时序下采样的framesequence。用 匈牙利匹配，根据经验，设置参数 <spanclass="math inline">\(\lambda_{1}=1, \lambda_{2}=5, \lambda_{3}=2,\lambda_{4}=2\)</span>。</p></li><li><p>消融实验：</p><ul><li><strong>benefit of tubelet queries</strong>：在实验中发现了tubeletquery sets的好处，每个query set是由 <spanclass="math inline">\(T_{out}\)</span> per-frame query embeddings组成，能够在各自的frame上预测spatial location of the action。将其和single query embedding which represents a whole tubelet and must regress<span class="math inline">\(T_{out}\)</span> box locations for allframes in the clip.进行对比。结果是要好一些，证明了modeling actiondetection as a sequence-to-sequencetask，能够有效地利用transformer的架构。</li><li><strong>effect of tubelet attention</strong>：tubeletattention相比于典型的self-attention，能够节省memory。</li><li><strong>benefic of action switch</strong>：actionswitch能够精确地判断action的temporal start and end。没有actionswitch，TubeR会将transitional states误分类为actions。</li><li><strong>effect of short and long term contexthead</strong>：在AVA数据集上有很好的性能提升，网络能够<strong>seeingfull context of the clip</strong>。</li></ul></li><li><p>局限：</p><ul><li>3D backbone会占用很大的memory和计算量，限制了在长视频上应用TubeR。近期的工作是将transformer的encoder用于videoembedding，会占用较少的memory。</li><li>如果in one pass处理一个长视频，需要足够的queries来cover视频中per-person的不同的最多的行为数量。这会造成在自注意力层中，需要大量的queries，造成memory问题。一个可能的解决方法是产生persontubelets而不是 actiontubelets。因此当一个新的action发生的时候，不需要splittubelets。对于每个person instance，只需要一个query。</li></ul></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/e36c35f3b3d898898a1b63817ce70397555cef76/1-Figure1-1.png"alt="tube" /><figcaption aria-hidden="true">tube</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/e36c35f3b3d898898a1b63817ce70397555cef76/3-Figure2-1.png"alt="structure of TubeR" /><figcaption aria-hidden="true">structure of TubeR</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;tuber-tubelet-transformer-for-video-action-detection1&quot;&gt;TubeR:
Tubelet Transformer for Video Action Detection&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自阿姆斯特丹大学、罗格斯大学和AWS AI Labs的Jiaojiao
Zhao、Yanyi Zhang等人。论文引用[1]:Zhao, Jiaojiao et al. “TubeR: Tubelet
Transformer for Video Action Detection.” 2022 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (2021): 13588-13597.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2021.April&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;tubelet queries, tubelet-attention&lt;/li&gt;
&lt;li&gt;in sequence-to-sequence manner&lt;/li&gt;
&lt;li&gt;scales well to longer video clips&lt;/li&gt;
&lt;li&gt;end-to-end without person detectors, anchors or proposals&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;不同于现有的依赖于离线检测器或者人工设计的actor-positional
hypotheses like proposals or anchors，提出了一个通过同时进行action
localization和recognition from a single
representation，直接检测视频里的action
tubelet的方法。TubeR学习一系列的tubelet queries，利用tubelet-attention
module来model video clip里的动态的spatio-tempral
nature。相比于用actor-positional hypotheses in the spatio-temporal
space，它能够有效的强化模型的能力。对于包含transitional states或者scene
changes的视频，提出了一个context aware classification
head，来利用short-term和long-term context to strengthen action
classification，和一个action switch regression head
来检测精确的时序上的行为范围。TubeR直接产生不同长度的action
tubelets，对于长的视频clips，也能保持一个比较好的结果。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>EVAD</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/EVAD/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/EVAD/</id>
    <published>2024-08-24T08:51:19.000Z</published>
    <updated>2024-08-27T07:18:15.529Z</updated>
    
    <content type="html"><![CDATA[<h3id="efficient-video-action-detection-with-token-dropout-and-context-refinement1">EfficientVideo Action Detection with Token Dropout and ContextRefinement<sup>[1]</sup></h3><blockquote><p>作者是来自nju、蚂蚁集团、复旦和上海AI Lab的Lei Chen、ZhanTong、Yibing Song等人。论文引用[1]:Chen, Lei et al. “Efficient VideoAction Detection with Token Dropout and Context Refinement.” 2023IEEE/CVF International Conference on Computer Vision (ICCV) (2023):10354-10365.</p></blockquote><h3 id="time">Time</h3><ul><li>2023.Aug</li></ul><h3 id="key-words">Key Words</h3><ul><li>spatiotemporal token dropout</li><li>maintain all tokens in keyframe representing scene context</li><li>select tokens from other frames representing actor motions</li><li>drop out irrelavant tokens.</li></ul><h3 id="总结">总结</h3><ol type="1"><li>视频流clips with large-scale vieo tokens 阻止了ViTs for efficientrecognition，特别是在video actiondetection领域，这是需要大量的时空representations来精确地actoridentification。这篇工作，提出了端到端的框架 <strong>for efficient videoaction detection(EVAD) based on vanillaViTs</strong>。EVAD包含两个为视频行为检测的特殊设计。首先：提出来时空tokendropout from a keyframe-centric perspective. 在一个video clip中，mainall tokens from its keyframe，保留其它帧中和actormotions相关的tokens。第二：通过利用剩余的tokens，refine scene contextfor better recognizing actor identities。actiondetector中的RoI扩展到时间域。获得的时空actor identity representationsare refined via scene context in a decoder with the attentionmechanism。这两个设计使得EVAD高效的同时保持精度。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>image patches视为ViT输入的tokens for自注意力的计算。当识别一个video clip的时候，tokens来自每个frame，形成大规模的input for ViTs，这些videotokens会在训练和推理的时候引入很多计算，特别是在计算自注意力的时候。有一些尝试来减少visiontokens for fast computations。对于video action detection任务来说，平衡精度和效率是个挑战。这是因为在VAD中，需要定位每一帧中的actors，视频序列中的temporalmotion需要保持 for consistent identification。同时，scene context应该被保留以区别其它actors。大量的表示actor motions和scenecontext的video tokens会保持VAD的精度。这篇文章中，保留表示actormotions和scene context的video tokens，同时dropping out不相关的tokens。<strong>基于视频clips的时序连贯性，从keyframe-centric的角度，提出来时空tokendropout</strong>。<strong>对于每个视频clip,选择代表scenecontext的keyframe，这里的所有tokens将被maintained。同时，从其它代表actormotions的帧中选择tokens。另外，drop out 这个clip中的剩余的videotokens</strong>。</p><p><font color=red>通过一个keyframe-centric token pruning module withthe ViT encoder backbone，实现spatiotemporal tokendropout。这个keyframe要么是均匀采样得到的，要么人为在videoclips中定义的。默认是选择input clip中带有box annotations的middleframe</font>。另外，提取由这个keyframe增强的attention map。这个attentionmap在non-keyframe中知道tokedropout。在定位之后，需要分类每个定位好的bbox for actoridentification。当video tokens在non-keyframes中是不完整的时候，分类的性能在bbox regions (where tokenshave been droppedout)中是较差的。然而，视频的内在时序一致性使得能够refine bothactor和scene context from the remaining videotokens。扩展时域上的localized bboxes for RoIAlign，来获得与actormotion相关的tokenfeatures。然后，引入一个decoder，通过这个clip中的剩余videotokens的指导，来refine actor features。这个decoder concatenatesactor和token features，进行自注意力操作来产生丰富的actor features forbetter identification。在token dropout之后，退化的actionclassification能够用剩余的video tokens for contextrefinement来恢复。这个恢复的性能和用整个video tokens for actionclassification是一样的。通过这个context refinement，用减少了的videotokens保持了VAD的性能。</p></li><li><p>相关工作:</p><ul><li><strong>Spatio-temporal ActionDetection</strong>：当前的SOTA方法采用两个分开的backbones、2个stagespipelines。例如2D backbone for actor localization on keyframes，3Dbackbone for video featureextraction.之前的方法通过端到端的方式训练这两个backbone，来简化pipeline。这个会导致很高的复杂度和优化困难。最近的方法用统一的一个backbone来进行actiondetection。 VAT是一个transformer风格的actiondetector，来汇聚目标actor附近的时空上下文。WOO和TubeR是query-based的actiondetectors，来预测actor bboxes和actionclasses。STMixer是一个但阶段的query-based detector，来adaptively samplediscriminativefeatures。几个新的基于transformer的方法，用ViT的变种backbone，用两阶段的pipeline得到了一个比较好的结果。</li><li><strong>Spatio-temporal Redundancy</strong>：<ol type="1"><li>SpatialRedundancy：<strong>DynamicViT</strong>观察到，精确的图像识别是基于最富有信息的tokens的子集，设计了一个dynamictokensparsification的框架，来剪掉多余的tokens。<strong>EViT</strong>计算了classtoken to each token的attentiveness，确定了top-k tokens using theattentiveness value。<strong>ATS</strong>引入了一个differentiableadaptive token sampler for adaptively sampling 重要的tokens based onimage content。</li><li>Spatio-temporalredundancy：由于视频数据集的高冗余性，有很多的研究关注于开发高效的视频识别。最近的方法展示了高效的schemesspecialized for videotransformers。<strong>MAR</strong>提出了一个人为设计的masking策略，来丢弃 部分patches，减少videotokens。<strong>K-centered</strong>提出了patch-basedsampling，超越了传统的frame-basedsampling。<strong>STTS</strong>用transformer在时间和空间上sequentially选择tokens。</li></ol>为了和这些方法对比，考虑keyframe和邻近帧的correlation,来drop out冗余的tokens，然后执行端到端的基于transformer的action detector。</li></ul></li><li><p>Method：EVAD使encoder with token pruning来去掉多余的tokens，decoder来refine actor spatiotemporalfeatures。和<strong>WOO</strong>设置相同，利用encoder中的keyframe的多个中间的spatialfeature maps for actor localization，最后一个encoderlayer输出的spatiotemporal feature map 用来做action classification。</p><ul><li><strong>Keyframe-centric TokenPruning</strong>：相邻帧有着相似语义信息的video是高度冗余的，使得用一个比较高的dropoutrate on video transformers来做tokenpruning是可能的。这里，将所有的spatiotemporal tokens分成keyframetokens和non-keyframe tokens。keyframe中，保留所有的tokens for accurateactor localization。</li></ul><ol type="1"><li><font color=red>Non-keyframe token pruning</font>：用<strong>EViT</strong>中的方法，预先计算一个attentionmap来表示每个token的重要性，不需要额外的learnableparameters和计算开销。首先，average attentionmap的<em>num_heads</em>维度，来得到一个 <span class="math inline">\(N\times N\)</span>的matrix，表示tokens之间的<strong>attentiveness</strong>。因为没有classification token forvideo-level recognition，所以计算每个token的平均重要性分数 <spanclass="math inline">\(I_{j}=\frac{1}{N}\sum_{i=1}^{N}attn(i,j)\)</span>。然后从<span class="math inline">\(N_2\)</span> non-keyframetokens中，通过重要性分数的降序排列，找到 top(<spanclass="math inline">\(N \times \rho - N_1\)</span>) tokens，这里<spanclass="math inline">\(N, N_1, N_2\)</span>分别表示所有的tokens、keyframetokens和non-keyframe tokens。<span class="math inline">\(\rho\)</span>表示 token keepingrate，通常情况下，keyframe包含当前样本的最精确的语义信息，其它帧会带来信息bias。guidedby keyframe进行token pruning是符合实际的。为了这个目的，在acquiringattention map和计算non-keyframe token重要之间插入一个 <strong>KeyframeAttentiveness Enhancement</strong> step。用一个greater weight value tokeyframe queries，保留和keyframetokens高关联的tokens。每个token的重要性分数是这样更新的：</li></ol><p><span class="math display">\[I_j=\frac{1}{N}\sum_{i=1}^N\begin{cases} w_{tf} \cdot attn(i,j),&amp;i \in(0,N_1)\\\\attn(i,j),&amp;i \in (N_1,N)\end{cases}\]</span></p><p>假设first <span class="math inline">\(N_1\)</span>tokens属于keyframe，weigth value <spanclass="math inline">\(w_{kf}\)</span>是一个超参数。丢掉仅和non-keyframes 有highresponse的tokens，这些可能不是高质量的tokens。仅当non-keyframe变成了previousor next samples的keyframe的时候，这些highly responsivetokens才是高质量的。通过dropout这些冗余的tokens，进一步减少tokens的数量；做完tokenpruning之后，将这些保留的tokens给到之后的FFN。</p><p>第一次token pruning是在encoder layers 的1/3开始，之后，每个 1/4的total layers进行一次 token pruning，丢弃冗余的tokens，保留effectiveones。</p><ul><li><strong>Video Action Detection</strong>：</li></ul><ol type="1"><li><p><font color=red>Actor localization branch</font> 得到了keyframetokens，就能得到多个完整的keyframe feature maps。然后对这些featuremaps进行上采样或者下采样，来从palin ViT中产生hierarchicalfeatures。受Sparse RCNN的启发，引入了query-based actor localizationhead，来检测keyframe中的actors。actor localization branch的输出是 <spanclass="math inline">\(n\)</span>个 prediction boxes in thekeyframe和对应的actor confidence scores。</p></li><li><p><font color=red>Action Classification branch</font>不同于传统的特征提取，EVAD产生<spanclass="math inline">\(M\)</span>个离散的video tokens。需要恢复videofeature map的时空结构，然后进行location-related操作例如RoIAlign。初始化blank feature map shape as <spanclass="math inline">\((T/2,H/16,W/16)\)</span>，用保留的tokens来填充这个featuremap according to 它们对应的时空位置，剩余的用0进行pad。</p></li></ol><p>然后，用localization branch产生的boxes，通过3D RoIAlign来提取actorRoI features for subsequent action prediction。由于actormovement或者相机的变化，actor的空间位置是逐帧变化的，用keyframe box for3D RoIAlign不能得到partial actor feature deviated from thebox。直接扩展scope of the box来cover整个motiontrajectory，可能会引入背景或者其它的干扰信息，对actor featurerepresentation不利。然而，在EVAD featureextraction阶段，<strong>视频中的干扰项会被逐步去掉，因此可以扩展scope ofbox来增加deviated feature</strong>. <strong>观察发现：</strong>直接用vision classification的token pruning方法能减少计算中的tokens的数量，但是会对最后的检测性能有负面的影响。Videoaction detection要localizing和classifying actions of allactors，但是token pruning算法会导致时空上不连贯的actorfeatures**。在encoder中，pair-wise self-attention能够modeling globaldependency among tokens，actor regions内的dropouttokens的语义信息能够被incorporated into some preservedtokens，因此能够从保留的video tokens中恢复去掉的actorfeatures。为了这个目的，设计了一个<font color=red>context refinementdecoder</font>来refine actor的spatiotemporalrepresentation。具体地说：concatenate <spanclass="math inline">\(M\)</span>个video tokens的 <spanclass="math inline">\(n\)</span>个actor RoIfeatures，然后送到deocder中，guiding by 保留的tokens，actor features canenrich themselves with actor represntation and motion information fromother frames。没有token pruning，decoder会被用来作为relational modelingmodules，来得到inter-actor和actor-context的interaction information。</p><p>decoder输出的<span class="math inline">\(n\)</span>个refined actorfeatures are retrieved，然后通过一个classification layer，做最后的actionprediction。</p></li><li><p>实验：</p><ul><li><strong>RoI extension</strong>：由于人的largemotion，从keyframe中得到的box不能cover整个motiontrajectory，Intuitively，通过适当地扩展box scope来解决这个问题。pruningmechanism能够消除extension带来的interference information。结合了RoIextension的pruning能够在一定程度上消除human movements的影响。</li><li>EVAD能够实现实时的推理in an end-to-end manner。</li><li>之前流行的model是VideoMAE，是一个两阶段的model，需要一个离线的persondetector来pre-compute personproposals。现在EVAD，用同样的预训练的backbone，能够获得和VideoMAE相当的性能。和其它端到端的模型例如<strong>WOO</strong>和<strong>TubeR</strong>，用明显的性能提升；比基于CNN的model有更快的推理速度,more friendly to real-time action detection。比STMixer的性能略微好一点，<strong>STMixer是最近的端到端的模型，设计了一个decoder来采样discriminativefeatures。EVAD is deivsed for efficient video featureextraction，进一步结合这两个或许会得到更好的检测性能</strong>。</li></ul></li><li><p>Conclusion：受videosequence的transformer的大量的计算开销和视频检测中高度冗余时空信息的的启发，通过<strong>droping out spatiotemporal tokens and refining scene context toenable efficient transformer-based actiondetection</strong>,设计了EVAD的方法。 EVAD的局限是：它需要<strong>retraining once to take the benefits of reduced computations andfaster inference from removing redundancy</strong>。一个潜在的方法是探索transformer-adaptive token pruning algorithms。另外，follow 端到端的框架<strong>WOO</strong>来验证<strong>EVAD的效率和有效性</strong>，但是<strong>WOO</strong>是一个两阶段的pipeline，sequentially执行actor localization和action classificationmodules。在未来的工作中，旨在将这两个modules集成到一个 unifiedhead，能够减小通过 detector head的推理时间，能够amplifyEVAD的高效的特点。</p></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/f3ea68b1948c43518259dc22dc121b2c9730103a/3-Figure2-1.png"alt="Structure" /><figcaption aria-hidden="true">Structure</figcaption></figure><p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Pipeline ofEVAD. Our detector consists of three parts: an encoder withkeyframe-centric token pruning for efficient video feature extraction, aquery-based localization branch using multiscale features of thekeyframe for actor boxes prediction, and a classification branchconducting actor spatiotemporal feature refinement and relationalmodeling between actor RoI features and compact context tokens from ViTencoder.</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/f3ea68b1948c43518259dc22dc121b2c9730103a/4-Figure3-1.png"alt="Keyframe-centric token pruning" /><figcaption aria-hidden="true">Keyframe-centric tokenpruning</figcaption></figure><p><span class="math inline">\(Figure \ 2^{[1]}\)</span>: Structure ofkeyframe-centric token pruning.We conduct token pruning within theoriginal encoder layer.Specifically, we calculate the importance scoresof non-keyframe tokens using a keyframe-enhanced attention map.Then, wepreserve the top-k important non-keyframe tokens concatenated withkeyframe tokens as the results</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;efficient-video-action-detection-with-token-dropout-and-context-refinement1&quot;&gt;Efficient
Video Action Detection with Token Dropout and Context
Refinement&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自nju、蚂蚁集团、复旦和上海AI Lab的Lei Chen、Zhan
Tong、Yibing Song等人。论文引用[1]:Chen, Lei et al. “Efficient Video
Action Detection with Token Dropout and Context Refinement.” 2023
IEEE/CVF International Conference on Computer Vision (ICCV) (2023):
10354-10365.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2023.Aug&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;spatiotemporal token dropout&lt;/li&gt;
&lt;li&gt;maintain all tokens in keyframe representing scene context&lt;/li&gt;
&lt;li&gt;select tokens from other frames representing actor motions&lt;/li&gt;
&lt;li&gt;drop out irrelavant tokens.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;视频流clips with large-scale vieo tokens 阻止了ViTs for efficient
recognition，特别是在video action
detection领域，这是需要大量的时空representations来精确地actor
identification。这篇工作，提出了端到端的框架 &lt;strong&gt;for efficient video
action detection(EVAD) based on vanilla
ViTs&lt;/strong&gt;。EVAD包含两个为视频行为检测的特殊设计。首先：提出来时空token
dropout from a keyframe-centric perspective. 在一个video clip中，main
all tokens from its keyframe，保留其它帧中和actor
motions相关的tokens。第二：通过利用剩余的tokens，refine scene context
for better recognizing actor identities。action
detector中的RoI扩展到时间域。获得的时空actor identity representations
are refined via scene context in a decoder with the attention
mechanism。这两个设计使得EVAD高效的同时保持精度。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>YOWO</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/YOWO/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/YOWO/</id>
    <published>2024-08-24T08:51:02.000Z</published>
    <updated>2024-08-27T07:15:14.850Z</updated>
    
    <content type="html"><![CDATA[<h4id="you-only-watch-once-a-unified-cnn-architecture-for-real-time-spatiotemporal-action-localization1">YouOnly Watch Once: A Unified CNN Architecture for Real-Time SpatiotemporalAction Localization<sup>[1]</sup></h4><blockquote><p>作者是来自Technical Univ of Munich的Okan Kopuklu, Xiangyu Wei,Gerhard Rigoll。论文引用[1]:Köpüklü, Okan et al. “You Only Watch Once: AUnified CNN Architecture for Real-Time Spatiotemporal ActionLocalization.” ArXiv abs/1911.06644 (2019): n. pag.</p></blockquote><h4 id="time">Time</h4><ul><li>2019.Nov.15(v1)</li><li>2021.Oct.18(v5)</li></ul><h4 id="key-words">Key Words</h4><ul><li>single-stage with two branches</li></ul><h4 id="总结">总结</h4><ol type="1"><li>当前的网络抽取时序信息和keyframe的空间信息是用两个分开的网络，然后用一个额外的mechanism来融合得到detections。YOWO是一个单阶段的架构，有两个分支，来同时抽取当前的时序和空间信息，预测bboxes和action的概率 directly from video clips in oneevaluation。因为架构是统一的，因此可以端到端的优化。YOWO架构速度快，能够做到在16-framesinput clips上做到 34 frames-per-second，62 frames-per-second on 8-framesinput clips。是当前在STAD任务上最快的架构。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>和静态图像里的目标检测相比，时序信息很重要，受目标检测FasterRCNN的启发，SOTA的工作将经典两阶段的网络架构扩展到actiondetection，第一阶段产生proposals，在第二阶段进行分类和定位的refinement，然而，两阶段的STAD任务有3个主要的缺点：(1):action tube是由bboxes acrossframes组成的，它的产生比2Dcase更复杂和耗时。分类的性能极度依赖这些Proposals，然而detectedbboxes可能对于后续的分类任务是sub-optimal；(2)actionproposals只关注视频里的人物的features，忽略人和背景中的其它特征，而这这能够提供对于actionprediction相当重要的上下文信息。(3)训练RPN网络和分类网络是分开的，不能保证找到全局最优。只有局部最优from the combination of two stages can be found.训练的成本比单阶段的高，因此花费很多时间和存储。</p></li><li><p>YOWO克服了上述提到的缺点，YOWO的本能的idea是来自人类视觉认知系统，为了理解视频中的人物的行为，需要<strong>将当前帧的信息2Dfeatures from key frame</strong>与之前记忆里获得的知识(3D features fromclip)相关联，之后，两种features融合到一起，提供一个合理的结论。YOWO架构是一个单阶段的、有两个分支的网络。一个分支<strong>提取keyframe的spatial features via a 2D-CNN，另一个分支models spatiotemporalfeatures of the clip consisting of previous frames via a 3DCNN</strong>。YOWO是一个<strong>causal架构(因果架构)，就是没有利用future frames</strong>，能够operate onlineon incoming video streams。为了aggregate 2D CNN和3D CNN的featuressmoothly, 用了一个channel fusion和attention机制，get the utmost out ofinter-channel dependencies。最后，用融合的特征，产生frame-leveldetections，用一个<strong>linking 算法</strong>来产生action tubes.YOWO不局限于RGB的模态，其它的例如光流也是可以的；任何一个CNN的架构根据实时性能的要求，都可以用。YOWOoperates with maximum 16 frames input，因为short cliplengths对于实现STAD人物的实时性是必要的。然而，small clipsize是时序信息累计的限制因素。因此，利用<strong>long-term featurebank</strong>，通过训练好的3DCNN从整个视频中提取非重叠的8帧片段的特征。在推理的时候，averaged 3Dfeatures centering the key-frame.</p></li><li><p>主要贡献如下：</p><ul><li>提出了在视频流里的单阶段的STAD框架，能够端到端的训练。实现了在3DCNN和2D CNN上特征的bboxes 回归，同时，这两个特征对于彼此是互补的 forfinal bboxes 回归和分类。用了<strong>channel attention</strong>来汇聚两个branches的特征。实验证明：channel-wise attention机制，modelsinter-channel relationship within the concatenated featuremaps，提高了性能。</li></ul></li><li><p>Related work: 为了考虑时序信息，有twe-streamCNN来提取分别提取空间和时间特征，然后汇聚到一起；这样的工作大部分是基于光流的，很耗时和耗计算资源。然后就是3DCNN，用它来提取时空特征。为了resource efficiency，一些工作用2DCNN来学习2D 特征，然后用一个3DCNN来将它们融合到一起，学习时间特征。Attention是一个有效的mechanism来capturelong-range dependencies，用在了CNNs中来尝试提高图像分类的性能。Attentionmechanism在<strong>spatial-wise和channel-wise</strong>来执行。<strong>spatialattention解决inter-saptial relationship among features，channelattention增强最有意义的channels，弱化其它的</strong>。作为一个channel-wiseattention block，Squeeze-and-Excitationmoduel对于提高CNN的性能有益。另一方面，对于视频分类人物，non-localblock考虑时空信息，来学习acrossframes的特征的dependencies。可以视为自注意力策略。不同于之前的工作，YOWO只使用clip一次，检测keyframe里的对应的actions。为了避免光流的复杂计算，用keyframe的2Dfeatures和clip的3D features。之后，两种类型的features用<strong>attentionmechanism</strong>融合在一起，就能够考虑到丰富的上下文信息。</p></li><li><p>YOWO的架构主要分为4个部分：<strong>3D CNNbranch</strong>、<strong>2D CNN branch</strong>、<strong>CFAM</strong>和 <strong>bbox regression parts</strong>.</p><ul><li><font color=red>3DCNN</font>：因为上下文信息对于人类行为理解很重要，因此用3DCNN来提取时空特征。3DCNN能够获得运动信息(在空间和时间维度做卷积)，基本的3D CNN架构这里用的是3D-ResNext-101，对于所有的3DCNN架构，在最后的卷积层之后的所有层都会被丢弃。3D网络的输入是一个videoclip。是由时间上连续的视频帧组成，shape为 <span class="math inline">\(C\times D \times H \times W\)</span>，最后一个3DResNext-101的输出的feature map的shape为 <spanclass="math inline">\(C&#39; \times D&#39; \times H&#39; \timesW&#39;\)</span>，<span class="math inline">\(C=3, D&#39;=1, H&#39; =\frac{H}{32}, W&#39; = \frac{W}{32}\)</span>，<spanclass="math inline">\(D\)</span> 是输入的帧数。输出特征图的depth维度减小到1，以至于 output volume可以squeezed to <spanclass="math inline">\(C&#39; \times H&#39; \timesW&#39;\)</span>，为了和2D-CNN的输出匹配。</li><li><font color=red>2D CNN</font>：为了解决空间定位问题，keyframe的2D特征被extracted in parallel，用Darknet-19作为2DCNN的基本架构，因为它在精度和效率上取得了平衡。key frame with the shape<span class="math inline">\(C \times H \times W\)</span> 是输入clip的<strong>most recentframe</strong>，因此不需要一个额外的dataloader，Darknet-19的输出特征图的shape为<span class="math inline">\(C&#39;&#39; \times H&#39; \timesW&#39;\)</span>，<span class="math inline">\(C= 3\)</span>，<spanclass="math inline">\(C&#39;&#39;\)</span> 是输出的channels，<spanclass="math inline">\(H&#39;= \frac{H}{32}, W&#39; =\frac{W}{32}\)</span>，和3DCNN的情况类似。YOWO的另一个重要特性是，它的2D CNN和3DCNN的分支能够被任意的CNN架构代替，使其更灵活。需要注意的是：<strong>虽然YOWO有两个分支，但是它是一个统一的架构，能够端到端的训练</strong>。</li><li><font color=red>Feature aggregation: Channel Fusion and AttentionMechanism(CFAM) </font>：让3D和2D网络的输出有相同的shape in the last twodimensions，以至于两个featuremaps能够简单地融合。用concatenation来融合两个featuremaps。因此，融合的feature map能够 <strong>encoders both motion andappearanceinformatin</strong>，然后这个融合的特征输到CFAM模块里，这个模块是基于<font color=red>Gram matrix</font>来映射 <strong>inter-channeldependencies</strong>。虽然<strong>Grammatrix</strong>最初是用来做<strong>styletransfer</strong>，最近用在了segmentationtask，这样一个注意力机制对于来自不同sources的feature的融合是有益的。能够提高性能。concatenatedfeature map <span class="math inline">\(A \in R^{(C&#39; + C&#39;&#39;)\times H \times W}\)</span>，可以被视为3D 和2D 信息的一个 <strong>abruptcombination</strong>，这忽略了它们之间的<strong>interrelationship</strong>。因此，将<spanclass="math inline">\(A\)</span>输给两个conv layers来产生新的特征图<span class="math inline">\(B \in R^{C \times H&#39; \timesW&#39;}\)</span>，之后，在特征图<spanclass="math inline">\(B\)</span>上进行一些操作。$F R^{C N} $是 featuremap <span class="math inline">\(B\)</span>reshape之后的tensor， <spanclass="math inline">\(N=H \timesW\)</span>，意味着每个channel中的features被 <strong>vectorized to onedimension</strong>。然后 对 <span class="math inline">\(F \in R^{C\times N}\)</span> 和它的转置 <span class="math inline">\(F^T \in R^{N\times C}\)</span> 进行 矩阵乘法，来得到 <strong>Gram Matrix</strong><span class="math inline">\(G \in R^{C \timesC}\)</span>，这个矩阵能够表明不同channel之间的<strong>correlations</strong>。</li></ul><p><spanclass="math display">\[\begin{array}{rcl}\mathbf{G}&amp;=&amp;\mathbf{F}\cdot\mathbf{F}^\mathrm{T}&amp;with&amp;G_{ij}&amp;=&amp;\sum_{k=1}^{N}F_{ik}\cdotF_{jk}\end{array}\]</span></p><p><strong>Gram matrix G</strong>中的每个元素 <spanclass="math inline">\(G_{ij}\)</span>代表 <strong>vectorize feature mapsi and j</strong> 之间的<strong>inner product</strong>。</p><p>在计算完Gram matrix之后，用一个softmax layer来得到 <strong>channelattention map M</strong>, <span class="math inline">\(M \in R^{C \timesC}\)</span>，<span class="math inline">\(M_{ij}\)</span>表示<spanclass="math inline">\(j^{th}\)</span> channel对 <spanclass="math inline">\(i^{th}\)</span> channel之间的的影响。因此，<spanclass="math inline">\(M\)</span> summaries 给定featuremap的features的<strong>inter-channel dependency</strong>。为了performimpact of attention map to original features，对<spanclass="math inline">\(M\)</span>和<spanclass="math inline">\(F\)</span>做一个矩阵乘法，将结果shape到3维空间<span class="math inline">\(R^{C \times H \timesW}\)</span>，这将和输入的tensor有相同的shape。</p><p><span class="math display">\[\mathbf{F}^{\prime} =\mathbf{M}\cdot\mathbf{F}\\\mathbf{F}^{\prime}\in\mathbb{R}^{C\timesN}\xrightarrow{reshape}\mathbf{F}^{\prime\prime}\in\mathbb{R}^{C\timesH\times W}\]</span></p><p>channel attention module的输出 <span class="math inline">\(C \in R^{C\times H \times W}\)</span>是 <spanclass="math inline">\(F&#39;&#39;\)</span>和初始输入特征图 <spanclass="math inline">\(B\)</span> 的的结合，with a trainable scalarparameter <span class="math inline">\(\alpha\)</span>，用<strong>element-wise sum operation</strong>，<spanclass="math inline">\(\alpha\)</span> <strong>learns a weight from0</strong>。 <span class="math display">\[\mathbf{C} =\alpha\cdot\mathbf{F}^{\prime\prime}+\mathbf{B}\]</span></p><p>该方程表明：每个channel的最后的特征是 <strong>weighted sum of thefeatures of all channels and original features</strong>，这是对featuremaps之间的long-range的semantics dependencies的建模。最后 <spanclass="math display">\[\mathbf{C}\in\mathbb{R}^{C\times H^{\prime}\timesW^{\prime}}\]</span> 给到了两个conv layer，来得到 <em>CFAM module</em>的输出特征图 <spanclass="math display">\[\mathbf{D}\in\mathbb{R}^{C^{*}\timesH^{\prime}\times W^{\prime}}\]</span>，在CFAM模块的开始和结束的2 个convlayer很重要，因为它们 <strong>mix the features from differentdistributions</strong>，没有这些convlayers，CFAM模块的性能提升会有限。</p><p>这样的一个架构 promote feature representativeness in terms of<strong>inter-dependencies amongchannels</strong>，因此来自不同branches的features能够被reasonably andsmoothly汇聚到一起。另外，<strong>Gram matrix</strong> 考虑整个 featuremap。两个flattened feature vectors的<strong>点乘</strong> 展示了<strong>它们之间的relationinformation</strong>。一个比较大的product表明两个 channels的features<strong>more correlated</strong>，smallerproduct表明它们彼此不一样。对于一个给定的channel，<strong>allocate moreweights to other channels which are much correlated and have more impactto it</strong>。通过这种机制，上下文的关系被<strong>emphasized，features discriminability is enhanced</strong>。</p><ul><li><font color=red> Bounding box regression</font>：followYOLO相同的guidelines for bbox regression。最后一个conv layer with <spanclass="math inline">\(1 \times 1\)</span> kernels用来产生<strong>desired number of output channels。对于每个grid cel in <spanclass="math inline">\(H&#39; \times W&#39;\)</span>。用 k-means方法在对应的datasets上选择5个prior anchors, with </strong>NumCls classconditional action scores, 4 coordinates and confidencescore**。YOWO的最后的输出的size是 <spanclass="math inline">\([(5\times(NumCls+5))\times H&#39;\timesW&#39;]\)</span>。bboxes的回归然后基于这些anchors进行refined。</li></ul></li><li><p>在训练和测试阶段的输入分辨率都为 <span class="math inline">\(224\times 224\)</span>，用不同的分辨率进行multi-scaletraining在实验中没有发现有性能的提高。损失函数和原始的YOLOv2的网络中类似，除了这个采用了smooth L1 Loss with beta=1 for localization，</p><p><spanclass="math display">\[L_{1,smooth}(x,y)=\begin{cases}0.5(x-y)^2&amp;if|x-y|&lt;1\\\\|x-y|-0.5&amp;otherwise\end{cases}\]</span></p><p><span class="math inline">\(x,y\)</span>分别指prediction和groundtruth。L1 loss相比于MSE loss，对outliers不那么敏感，能够在某些情况阻止梯度爆炸。用MSE loss forconfidence scores.</p><p><span class="math display">\[L_{MSE}(x,y)=(x-y)^2\]</span></p><p>最后的detection loss是 individual coordiante losses forx,y,width,height和confidence score loss，</p><p><spanclass="math display">\[L_D=L_x+L_y+L_w+L_h+L_{conf}\]</span></p><p>用focal loss for classification:</p><p><span class="math display">\[L_{focal}(x,y)=y(1-x)^\gammalog(x)+(1-y)x^\gamma log(1-x)\]</span></p><p>x是 softmaxed network prediction, <span class="math inline">\(y \in{0,1}\)</span> is grouth truth class label。<spanclass="math inline">\(\gamma\)</span>是modulating factor，reduce loss ofsamples with high confidence(easy samples), increase the loss of sampleswith low confidence(hardsamples)。AVA数据集是一个多标签数据集，每个人执行一个poseaction和多个human-human or human-object interaction actions。因此，用<strong>softmax to pose classes and sigmoid to the interactionactions</strong>。另外，AVA是一个不平衡的数据集，modulating factor <spanclass="math inline">\(\gamma\)</span>不足以处理数据集的不平衡问题。因此用了一个focal loss的 <spanclass="math inline">\(\alpha -balanced variant\)</span>，对于 <spanclass="math inline">\(\alpha\)</span>，<strong>we have used exponentialof class sampleratios</strong>。最后的YOWO用的loss是检测的loss和分类的loss的和。</p><p><span class="math display">\[L_{final}=\lambdaL_D+L_{Cls}\]</span></p><p>这里 <span class="math inline">\(\lambda =0.5\)</span>在实验中表现最好。</p></li><li><p>分别初始化3D 和2D CNN网络：用在Kinetics上预训练的models来初始化3D CNN，用在PASCAL VOC上预训练的models来初始化2D CNN。虽然架构是由2DCNN和 3D CNN组成。这些参数能够一起更新。选择 <strong>mini-batch SGD withmomentum and weight decay</strong>来优化loss function。学习率初始为0.0001。在训练的时候，由于J-HMDB-21的样本数量少，冻结所有的 3D convnet的参数，因此收敛会更快，减小过拟合的风险。另外，在训练中用一些数据增强的方式例如flipping,random scaling等。</p></li><li><p><strong>linking strategy</strong>：在得到了frame-level的 actiondetection之后，下一步是将这些检测到的 bboxes 连接起来，构建<strong>action tubes in the whole video</strong>。利用连接算法，来找到最优的video-level action detections。 假设 <spanclass="math inline">\(R_t\)</span>和 <spanclass="math inline">\(R_{t+1}\)</span>是连续帧 <spanclass="math inline">\(t\)</span> 和 <spanclass="math inline">\(t+1\)</span>的两个区域。linking score for anaction class <span class="math inline">\(c\)</span> 定义为： <spanclass="math display">\[\begin{aligned}s_{c}(R_{t},R_{t+1})&amp;=\quad\psi(ov)\cdot[s_{c}(R_{t})+s_{c}(R_{t+1})\\&amp;+\alpha\cdots_{c}(R_{t})\cdot s_{c}(R_{t+1})\\&amp;+\beta\cdotov(R_{t},R_{t+1})]\end{aligned}\]</span></p><p><span class="math inline">\(s_{c}(R_{t})\)</span>和<spanclass="math inline">\(s_{c}(R_{t+1})\)</span> 是regions <spanclass="math inline">\(R_t\)</span>和 <spanclass="math inline">\(R_{t+1}\)</span>的 class specific socres。<spanclass="math inline">\(ov\)</span>是两个区域的IoU，如果overlap存在，则<spanclass="math inline">\(\psi(ov)\)</span>为1，否则为0.在linkingscore的定义上增加了一个额外的项：<span class="math inline">\(\alpha\cdots_{c}(R_{t})\cdots_{c}(R_{t+1})\)</span>。将两个连续帧的剧烈变化考虑进来了，能够提高videodetections的性能。在计算出所有的linking scores之后，用 <strong>Viterbialgorithm</strong>来找到最有的路径，生成 action tubes。</p></li><li><p><strong>long-term featurebank</strong>：虽然YOWO的推理是在线的和因果的 with small clipsize，但是16帧的输入限制了 temporal information required for actionunderstanding。因此，利用long-term featureback(LFB)，这个包含了不同的timestamps的来自3DCNN的features。在推理时，3D features centering the keyframe areaveraeged and resulting feature map用作输入，给到CFAM block，<strong>LFBfeatures are extracted for non-overlapping 8-frames clips using thepretrained 3D ResNeXt-101 backbone</strong>。用 8 features centering thekey-frame。因此在推理的时候，利用了总共64帧的数据。LFB增加了actionclassificatin的性能，类似于difference between clip accuracy and videoccuracy in video datasets。然后，LFB会导致一个非因果的架构，因为 future3D features在推理的时候用到了。</p></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/08c936517b8e8b9fcab9a544c735be2909fab3ee/8-Figure2-1.png"alt="YOWO" /><figcaption aria-hidden="true">YOWO</figcaption></figure><p><span class="math inline">\(Figure \ \ 1^{[1]}\)</span>: The YOWOarchitecture. An input clip and corresponding key frame is fed to a 3DCNN and 2D-CNN to produce output feature volumes of <spanclass="math inline">\([C&#39;&#39; × H&#39; × W&#39;]\)</span> and <spanclass="math inline">\([C&#39; × H&#39; × W&#39;]\)</span>,respectively.These output volumes are fed to channel fusion and attention mechanism(CFAM) for a smooth feature aggregation. Finally, one last conv layer isused to adjust the channel number for final bounding boxpredictions.</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/08c936517b8e8b9fcab9a544c735be2909fab3ee/9-Figure3-1.png"alt="Channel Fusion" /><figcaption aria-hidden="true">Channel Fusion</figcaption></figure><p><span class="math inline">\(Figure \ \ 2^{[1]}\)</span>: Channelfusion and attention mechanism for aggregating output feature mapscoming from 2D-CNN and 3D-CNN branches</p>]]></content>
    
    
    <summary type="html">&lt;h4
id=&quot;you-only-watch-once-a-unified-cnn-architecture-for-real-time-spatiotemporal-action-localization1&quot;&gt;You
Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal
Action Localization&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自Technical Univ of Munich的Okan Kopuklu, Xiangyu Wei,
Gerhard Rigoll。论文引用[1]:Köpüklü, Okan et al. “You Only Watch Once: A
Unified CNN Architecture for Real-Time Spatiotemporal Action
Localization.” ArXiv abs/1911.06644 (2019): n. pag.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;time&quot;&gt;Time&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;2019.Nov.15(v1)&lt;/li&gt;
&lt;li&gt;2021.Oct.18(v5)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;key-words&quot;&gt;Key Words&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;single-stage with two branches&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;总结&quot;&gt;总结&lt;/h4&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;当前的网络抽取时序信息和keyframe的空间信息是用两个分开的网络，然后用一个额外的mechanism来融合得到detections。YOWO是一个单阶段的架构，有两个分支，来同时抽取当前的时序和空间信息，预测bboxes和action
的概率 directly from video clips in one
evaluation。因为架构是统一的，因此可以端到端的优化。YOWO架构速度快，能够做到在16-frames
input clips上做到 34 frames-per-second，62 frames-per-second on 8-frames
input clips。是当前在STAD任务上最快的架构。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Holistic Interaction Transformer</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/Holistic-Interaction-Transformer/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/Holistic-Interaction-Transformer/</id>
    <published>2024-08-24T01:24:04.000Z</published>
    <updated>2024-08-27T07:12:05.312Z</updated>
    
    <content type="html"><![CDATA[<h4id="holistic-interaction-transformer-network-for-action-detection1">HolisticInteraction Transformer Network for Action Detection<sup>[1]</sup></h4><blockquote><p>作者是来自国立清华大学和微软AI的Gueter Josmy Faure, Min-HungChen和Shang-Hong Lai.论文引用[1]:Faure, Gueter Josmy et al. “HolisticInteraction Transformer Network for Action Detection.” 2023 IEEE/CVFWinter Conference on Applications of Computer Vision (WACV) (2022):3329-3339.</p></blockquote><h3 id="time">Time</h3><ul><li>2022.Nov.18</li></ul><h3 id="key-words">Key Words</h3><ul><li>bi-modal structure</li><li>combine different interactions</li></ul><h3 id="总结">总结</h3><ol type="1"><li>行为是关于我们如何与环境互动的，包括其他人、物体和我们自己。作者提出了一个新的多模态的<strong>HolisticInteraction Transformer Network(HIT)</strong>，利用大量被忽略的、但是对人类行为重要的手部和姿态信息。HIT网络是一个全面的bi-modal框架，由一个RGBstream和pose stream组成。每个stream独立地建模person、object和handinteractions，对于每个子网络，用了一个<strong>Intra-Modality Aggregationmodule(IMA)</strong>，选择性地融合个体的交互。从每个模态的得到的features然后用一个<strong>AttentiveFusion Mechanism(AFM)</strong>进行融合，最后，从temporalcontext中提取cues，用cached memory来更好地分类存在的行为。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>一个合理的时空行为检测的框架旨在正确地label帧里的每个人。应该在相邻帧之间keep a link来更好地理解带有连续性特点的活动例如："open"、"close"。最近，更多鲁棒的工作来考虑spatialentities之间的关系，因为如果两个人在同一帧，他们大概率会和彼此互动。然而，仅用personfeatures是不足以获得object-related action。其他人try to understand不仅是帧里的人物之间的关系，而且还有他们周围的物体。这些方法有两个主要的缺点：(1)他们仅依赖于highdetectionconfidence的目标，可能会导致忽略重要的没有检测到的目标；(2)这些模型努力检测没有出现在frame里的和目标相关的actions。例如：考虑到行为："pointto (an object)",actor所指向的object可能不在当前帧中。</p></li><li><p>HIT网络用细粒度的上下文，包括personpose，hands，objects，来构建一个<font color=red> bi-modal interactionstructure </font>，每个modality包括3个主要的components：<font color=red>person interaction </font>，<font color=red> object interaction</font>，<font color=red>hand interaction</font>，每个component学习有价值的local actionpatterns。在从相邻帧学习时序信息之前，用<font color=red>Attentive FusionMechanism </font>结合不同模态的信息，来帮助更好地检测发生在当前帧中的行为。主要的contributions有：</p><ul><li>提出了新的框架、<strong>结合RGB、pose和hand features for actiondetection</strong></li><li>引入了一个<strong>bi-modal HIT网络，结合不同的interaction in anintuitive and meaningful way</strong></li><li>提出了一个<strong>Attentive FusionModuel</strong>，作为一个选择性地过滤器，保持每个modal中信息量最多的features，<strong>Intra-ModalityAggregator</strong>用来学习modalities内有用的actionrepresentations.</li></ul></li><li><p>Related work:</p><ul><li><p>spatio-temporal actiondetection，不同于将整个视频分为1类，需要在空间上和时间上检测到行为。很多近期的spatio-temporalaction detection上的工作是用3D CNN作为backbone来提取视频特征，然后用ROIpooling或者ROI align来crop person features from videofeatures，这么做，抛弃了视频中潜在的有用信息。</p></li><li><p>时空行为检测任务实际是一个<strong>交互建模</strong>的任务，大多数的行为是在和环境互动。很多研究是用<strong>attentionmechanism</strong>，有人提出了<strong>Temporal RelationNetwork(TRN)</strong>，学习帧间的依赖，或者说，interaction betweenentities from相邻帧。其它的方法进一步，不仅建模temporal，也建模spatialinteractions between different entities from the same frame.然而，选择什么entities来model interactions也是因model而异的，不只用humanfeatures，也有人用background information来model interactions between theperson in the frame and context. 选择crop persons'features，但是不抛弃剩下的backgroundfeatures，这样提供了丰富的信息，但是，可能会引入很多噪音。也有人尝试bemore selective about the features to use. 有人首先pass the video framesthrough an object detecotr, crop both the object and personfeatures，model它们的interactions。interaction的额外的层，提供了更好的representationsthan 单独的human interaction modeling models，能够helps with classesrelated to objects such as "work on acomputer"，然而，当目标太小没有检测到或者没有出现在frame中的时候，这些方法会fallshort。</p></li><li><p>很多最近的action detection frameworks仅用RGBfeatures，也有人用光流来获取motion，有人用inception-likemodel来concatenates RGB和flow features at the <font color=red> Mixed4blayer </font>，然而也有人用I3D 网络来分别得到RGB和flowfeatures，然后在actionclassifier之前，concatenate两个模态。作者提出的bi-modal方法，用了<font color=red>visual and skeleton-based features</font>，每个modality计算一系列的interactions includingperson、object、and hands before being fused。一个temporal interactionmodule用在fused features上，来学习global information regardingneighboring frames.</p></li></ul></li><li><p>Methods: HIT由<strong>RGB和posesubnetwork组成</strong>,每个旨在学习persions interactions with theirsurroundings by focusing on the key entities that drive most of ouractions。在融合了两个sub-networks的输出之后，进一步通过<font color=red>lookingat cached features from past and future frames，model how actions evolveintime</font>。这样全面的活动的理解方案能够帮助实现更好的行为检测性能。几个步骤：<strong>entityselection</strong>、<strong>RGB modality</strong>、<strong>posemodality</strong>、<strong>Attentive FusionModule(AFM)</strong>、<strong>Temporal Interaction Module</strong>.</p><ul><li><strong>entity selection</strong>: HIT由两个mirroring modalitieswith distinct modules组成，来学习不同类型的interactions。Humanactions大部分基于它们的pose、hand movements和interactions with theirsurroundings。基于这些观察，选择human poses 和handsbboxes作为模型的entities along with object and personbboxes。用<strong>Detectron</strong> for human posedetection，然后create a bbox 包围location of the person's hands.用<font color=red> Faster-RCNN</font> 来计算<strong>object bboxproposals</strong>。 Video feature extractor是一个 3D CNN backbone。poseencoder是一个轻量的<font color=red>spatialtransformer</font>，用ROIAlign 来trim videofeatures，来抽取person、hands和object features。</li><li><strong>RGB branch</strong>：RGBbranch包含3个components，每个包含一系列的操作，来学习目标person的特定的信息。这个object和handsinteraction modules model person-object and person-handsinteraction。person interaction moduel学习当前帧之间的persons的interaction。在每个interaction unit的heart，是要给<font color=red>cross-atttentincomputation</font>，这个<strong>query是target person(or the output ofthe previous unit), key and value are derived from the objects, or handsfeatures,depending on which moduele we are at</strong>.<font color=blue> it is like asking "how can these particular featureshelp detect what the target person is doing?"</font>，公式如下： <spanclass="math display">\[F_{rgb}=(A(\mathcal{P})\to z_{r}\toA(\mathcal{O})\to z_{r}\to A(\mathcal{H})\toz_{r})\\A(*)=softmax(\frac{w_q(\widetilde{P})\timesw_k(*)}{\sqrt{d_r}})\times w_v(*)\\z_{r}=\sum_{b}A(b)\timessoftmax(\theta_{b}),b\in(\widetilde{P},\mathcal{O},\mathcal{H},\mathcal{M})\]</span></li></ul><p><span class="math inline">\(d_r\)</span> 代表RGB features的dimensionchannel,<font color=red> <span class="math inline">\(w_q\)</span>、<spanclass="math inline">\(w_k\)</span>、<spanclass="math inline">\(w_v\)</span> project their inputs into query,keyand value</font>，<em>A(*)</em>是cross-attention机制，only takes personfeatures as input when computing person interaction <spanclass="math inline">\(A(P)\)</span>，对于hand interaction(objectsinteraction):只有两个sets的输入：output of <spanclass="math inline">\(z_r\)</span> which serves as query(<spanclass="math inline">\(\bar{P}\)</span>)、hands features(object features)from which we obtain the key and values.</p><p><span class="math inline">\(Z_r\)</span>是所有interactionmodules的加权和，包括termporal interaction module <spanclass="math inline">\(TI\)</span>，<spanclass="math inline">\(Z_r\)</span>很重要，首先，它允许网络aggregate尽可能多的信息；另外可学习的参数<spanclass="math inline">\(\theta\)</span>帮助过滤不同sets的features，hand-pickingthe best each of them has to offer while discarding noisy andunimportant information。</p><ul><li><strong>Pose branch</strong>：pose model类似于它的RGBcounterpart，reuses most of its outputs。首先通过一个轻量的transformerencoder <span class="math inline">\(f\)</span>来抽取pose features <spanclass="math inline">\(K&#39;\)</span> <spanclass="math display">\[\mathcal{K}^{\prime}=f(\mathcal{K})\]</span></li></ul><p>然后通过mirroring RGB modality的不同的constituents来计算 <spanclass="math inline">\(F_pose\)</span>，然后reusing 对应的outputs，<spanclass="math inline">\(P&#39;\)</span>、<spanclass="math inline">\(O&#39;\)</span>、<spanclass="math inline">\(H&#39;\)</span>是对应的outputs of <spanclass="math inline">\(A(P),A(O),A(H)\)</span>。 <spanclass="math display">\[F_{pose}=(A(\mathcal{K}^{\prime},\mathcal{P}^{\prime})\toz_{p}\to A(\mathcal{O}^{\prime})\to z_{p}\to A(\mathcal{H}^{\prime})\toz_{p})\\A(\mathcal{K}^{\prime},\mathcal{P}^{\prime})=softmax(\frac{w_{q}(\mathcal{K}^{\prime})\timesw_{k}(\mathcal{P}^{\prime})}{\sqrt{d_{p}}})\timesw_{v}(\mathcal{P}^{\prime})\]</span></p><p><spanclass="math inline">\(A(\mathcal{K}^{\prime},\mathcal{P}^{\prime})\)</span>计算cross-attentionbetween pose features <spanclass="math inline">\(K&#39;\)</span>和增强的person interaction features<span class="math inline">\(P&#39;\)</span>，这样的cross-modal blendenforces the pose features by focusing on the key correspondingattributes of the RGB features。其它的components，<spanclass="math inline">\(A(O&#39;)\)</span>和<spanclass="math inline">\(A(H&#39;)\)</span>对 <spanclass="math inline">\(Z_p\)</span>做一个线性的projection as query whiletheir key-value pairs stem from <span class="math inline">\(A(O) andA(H)\)</span>，<spanclass="math inline">\(Z_p\)</span>是要给intra-modality aggregationcomponent for the pose model,类似于 <spanclass="math inline">\(Z_r\)</span>，它过滤和汇聚了每个interactionmodule的信息。</p><ul><li><p>Attentive Fusion Module(AFM)：在进入action分类器之前，RGB和posestream需要结合到一个set of features。出于这个目的，提出了AttentionFusion Module，用<font color=red>channel-wise concatenation of twofeatures sets followed by self-attention for featurerefinement</font>。通过用project matrix <spanclass="math inline">\(\Theta_{fused}\)</span>来减小输出特征的值。 <spanclass="math display">\[F_{fused}=\Theta_{fused}(SelfAttention(F_{rgb},F_{pose}))\]</span></p></li><li><p>Temporal Interaction Unit：fusion module之后，就是一个temporalinteraction block <span class="math inline">\(TI\)</span>，humanactions是连续发生的，因此，long-term context对于理解行为是重要的，alongwith <span class="math inline">\(F_fused\)</span>，这个Module receives压缩的 memory data <span class="math inline">\(M\)</span> with length<span class="math inline">\(2S+1\)</span>，memory cache包含了videobackbone得到的person features. <spanclass="math inline">\(F_fused\)</span> inquirs <spanclass="math inline">\(M\)</span> as to which of the neighboring framescontains informative features, 然后absorb。<spanclass="math inline">\(TI\)</span>是另一个 cross-attention module where<span class="math inline">\(F_fused\)</span>是query,memory <spanclass="math inline">\(M\)</span>两个不同的projections构成了key-valuepairs. <spanclass="math display">\[F_{cls}=TI(F_{fused},\mathcal{M})\]</span>最后，分类头 <span class="math inline">\(g\)</span>是有两个 feed-forwardlayers with relu activation 和output layer组成的， <spanclass="math display">\[\hat{y}=g(F_{cls})\]</span></p></li></ul></li><li><p>实验：</p></li></ol><ul><li><p>person和object detector:从数据集中的每个视频抽取keyframes，然后用detected person bbox from(YOWO[16]) fo inference。作为一个object detector，用FasterRCNN withResNet-50-FPN作为backbone，模型是在ImageNet上进行的预训练，然后再MSCOCO上微调。</p></li><li><p>Keypoints Detection and Processing：对于keypoints detection，采用<span class="math inline">\(Detectron\)</span>中的posemodel，用在ImageNet for object detection上预训练 and fine-tuned onMSCOCO keypoints using precomputed RPN proposals的ResNet-50-FPN，targetdataset中的每个keyframe通过model，输出17个keypoints for each detectedperson corresponding to the COCO format.进一步后处理这些检测到的posecoordinates，因此match GT person bboxes(during training) and bboxes from<a href="during%20testing">16</a>。对于person handslocation，仅对人的手腕的部位的keypoints感兴趣，因此，对这个keypoints做一个bboxes，来highlight人物的hands和everything in between。</p></li><li><p>用SlowFast网络作为视频的backbone</p></li><li><p><strong>Limitations</strong>: 该框架依赖于离线的detector和poseestimator，因此detector和poseestimator的精度可能会对这个方法有影响。similar-lookingclasses例如："throw"和"catch"看起来很像；第二个是部分的occlusion。</p></li></ul><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/ea3763cbc7bc7b502dd50fe0a54b7d06d8d8163b/3-Figure2-1.png"alt="HIT Network" /><figcaption aria-hidden="true">HIT Network</figcaption></figure><p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Overview ofour HIT Network. On top of our RGB stream is a 3D CNN backbone which weuse to extract video features. Our pose encoder is a spatial transformermodel. We parallelly compute rich local information from bothsub-networks using person, hands, and object features. We then combinethe learned features using an attentive fusion module before modelingtheir interaction with the global context</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/ea3763cbc7bc7b502dd50fe0a54b7d06d8d8163b/3-Figure3-1.png"alt="Illustration of the Interaction module" /><figcaption aria-hidden="true">Illustration of the Interactionmodule</figcaption></figure><p><span class="math inline">\(Figure \ 2^{[1]}\)</span>: Illustrationof the Interaction module. ∗ refers to the module-specific inputs whilePe refers to the person features in <spanclass="math inline">\(A(P)\)</span> or the output of the module thatcomes before <span class="math inline">\(A(∗)\)</span></p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/ea3763cbc7bc7b502dd50fe0a54b7d06d8d8163b/3-Figure4-1.png"alt="Illustration of the Intra-Modality" /><figcaption aria-hidden="true">Illustration of theIntra-Modality</figcaption></figure><p><span class="math inline">\(Figure \ 3^{[1]}\)</span>: Illustrationof the Intra-Modality Aggregator. Features from one unit to the next arefirst augmented with contextual cues then filtered</p>]]></content>
    
    
    <summary type="html">&lt;h4
id=&quot;holistic-interaction-transformer-network-for-action-detection1&quot;&gt;Holistic
Interaction Transformer Network for Action Detection&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自国立清华大学和微软AI的Gueter Josmy Faure, Min-Hung
Chen和Shang-Hong Lai.论文引用[1]:Faure, Gueter Josmy et al. “Holistic
Interaction Transformer Network for Action Detection.” 2023 IEEE/CVF
Winter Conference on Applications of Computer Vision (WACV) (2022):
3329-3339.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2022.Nov.18&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;bi-modal structure&lt;/li&gt;
&lt;li&gt;combine different interactions&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;行为是关于我们如何与环境互动的，包括其他人、物体和我们自己。作者提出了一个新的多模态的&lt;strong&gt;Holistic
Interaction Transformer Network
(HIT)&lt;/strong&gt;，利用大量被忽略的、但是对人类行为重要的手部和姿态信息。HIT网络是一个全面的bi-modal框架，由一个RGB
stream和pose stream组成。每个stream独立地建模person、object和hand
interactions，对于每个子网络，用了一个&lt;strong&gt;Intra-Modality Aggregation
module(IMA)&lt;/strong&gt;，选择性地融合个体的交互。从每个模态的得到的features然后用一个&lt;strong&gt;Attentive
Fusion Mechanism(AFM)&lt;/strong&gt;进行融合，最后，从temporal
context中提取cues，用cached memory来更好地分类存在的行为。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Video Understanding</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/20/Video-Understanding/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/20/Video-Understanding/</id>
    <published>2024-08-20T06:12:07.000Z</published>
    <updated>2024-08-23T08:38:57.194Z</updated>
    
    <content type="html"><![CDATA[<h4id="视频理解及分析的计算机视觉任务">视频理解及分析的计算机视觉任务</h4><ol type="1"><li><p>之前看的时候，不管是论文还是一些博客，感觉都不是很清晰和全面，大家的定义不全面，特别是英文的名称上，这里写一下我的理解：</p></li><li><p>几个任务：</p><ul><li><strong>行为识别(Action Recognition)</strong>:实质是对视频的分类任务，可以类别图像领域的分类任务</li><li><strong>时序动作定位(Temporal Action Localization)</strong>:在时间上对视频进行分类，给出动作的起止时间和类别</li><li><strong>时空行为检测(Spatio-Temporal Action Detection)</strong>:不仅识别出动作出现的<strong>区间</strong>和<strong>类别</strong>，还要在空间范围内用一个boundingbox标记处目标的<strong>位置</strong>。</li><li><strong>还有人提出了时空动作定位(Spatio-temporal Actionlocalization)</strong>：和上一个是一样的</li><li>Action Detection在Paperswithcode上的定义： aims to find both whereand when an action occurs within a video clip and classify what theaction is taking place. Typically results are given in the form of<strong>action tublets</strong>, which are action bounding boxes linkedacross time in the video. <em>This is related to temporal localization,which seeks to identify the start and end frame of an action, and actionrecognition, which seeks only to classify which action is taking placeand typically assumes a trimmed video.</em></li><li>论文里还提到了<strong>temporal action segmentation</strong>：针对细粒度的actions和videos with dense occurrence of actions to predictaction label labels at every frame of the video.</li></ul></li><li><p>时空行为检测的算法：之前的论文都是都是基于行为识别(actionrecognition)的，很多都是基于早期的Slowfast的那个检测的方式：需要一个额外的检测器，实现行为检测。也就是在行为识别的基础上，再进行时空行为检测。但这并不是我理想中的方式，所以很多行为识别的算法，在AVA上也能上榜；最近看VideoMAE看了之后，就一直在看这个，没有去看看其它的。</p></li><li><p>Action Detection数据集：</p><ul><li>J-HMDB</li><li>UCF101-24</li><li>MultiSports</li><li>AVA</li><li>其中，JHMDB和UCF101-24是密集标注数据集(每一帧都标注，25fps)，这类数据集每个视频只有一个动作，大部分视频是单人做一些语义简单的重复动作；AVA为代表的稀疏标注数据集(隔一段时间标注一帧，1fps)，没有给出明确的动作边界</li></ul></li></ol><span id="more"></span><h4id="deep-learnign-based-action-detection-in-untrimmed-videos-a-survey">DeepLearnign-based Action Detection in Untrimmed Videos: A Survey</h4><blockquote><p>作者是来自纽约城市大学的Elahe Vahdani and YingliTian,论文引用[1]:Vahdani, Elahe and Yingli Tian. “Deep Learning-BasedAction Detection in Untrimmed Videos: A Survey.” IEEE Transactions onPattern Analysis and Machine Intelligence 45 (2021): 4302-4320.</p></blockquote><ol type="1"><li><p>很多action recognition 算法是在untrimmedvideo里，真实世界中的视频大部分是漫长的和untrimmed with sparse segmentsof interest. temporal activitydetection在没有剪辑的视频里的任务是定位行为的时间边界和分类行为类别。spatio-temporalactiondetection：action在temporal和spatial维度上都进行定位，还需识别行为的类别。因为长的未修剪的视频的标注费时费力，所以<strong>action detection with limited supervision</strong>是一个重要的研究方向。</p></li><li><p><strong>Temporal Action Detection</strong> 旨在 在untrimmedvideo里找到精确的时间边界和行为实例的label。依赖于训练集的标注的availability，可以分为：</p><ul><li>全监督 action detection: 时间边界和labels of action instances areavailable</li><li>弱监督action detection：only the video-level labels of actioninstances are available, the order of action labels can be provided ornot.</li><li>unsupervised action detection: no annotations for actioninstances</li><li>semi-supervised action detection: 数据被划分为小的子集 <spanclass="math inline">\(S_1\)</span> 和大的子集 <spanclass="math inline">\(S_2\)</span>，<spanclass="math inline">\(S_1\)</span> 中的视频是全标注的，<spanclass="math inline">\(S_2\)</span>中的视频没有标注(as infully-supervised)或者only annotated with video-level labels(asweakly-supervised)</li><li>self-supervised action detection:用一个代理任务从数据中抽取信息，然后用于提高性能，例如一般的自监督预训练，然后有监督微调。</li><li>Temporal action detection：或者说temporal action localization,思路和图像中的目标检测类似，会用到 proposals、RoIpooling这些类似的思路。</li></ul></li><li><p>Untrimmedvideos通常很长，由于计算资源的限制，很难直接把整个视频给到visual encoder来提取特征。通常的做法是把视频划分为相同大小的temporal intervals called<em>snippets</em>，然后对每个snippet都用visual encoder。</p></li><li><p><strong>Spatio-temporal Action Detection</strong>:有frame-levelaction detection和clip-level action detection</p><ul><li><strong>frame-level action detection</strong>:早期的方式是基于滑动窗口的一些扩展方法，要求一些很强的假设：例如cuboidshape，一个actor的跨帧的固定的空间范围。图像的目标检测启发了识别人类行为at frame level; 第一阶段，通过region proposal 或者 densely sampledanchors 产生action proposals，然后第二阶段proposals用于actionclassification和localization refinement. 在检测frames里的actionregions之后，一些方法，用光流来获取运动信息，<strong>用linkingalgorithm来连接frame-level bounding box into spatio-temporal actiontubes</strong>；有人用dynamic programming approach来连接 resultingper-frame detection，这个costfunction是基于boxes的检测分数和连续帧之间的重叠；也有人用tracking-by-detection方法来代替linkingalgorithm；另外一组是依赖于actionness measure, 例如pixel-wiseprobability of containing any action.为了估计actionness，它们用low-level cues例如光流；通过thresholding theactionness scores来抽取action tubes，这个输出是action的roughlocalization。这些方法的主要的缺点是没有完全利用视频的时序信息，检测是在每一帧上独立做的，<strong>有效的时序建模是很重要，因为当时序上下文信息是可用的时候，大量的actions才是可识别的。</strong></li><li><strong>Clip-level action detection</strong>: 通过在cliplevel执行action detection来利用时序信息。Kalogeiton提出了action tubeletdetector(ACT-detector)，输入为一系列的frames，输出action来别和回归的tubelets：系列的带有associatedscores的bounding box。tubelets被连接，来构造actiontubes。Gu等人进一步通过用longer clips和利用I3D pre-trained on thelarge-scale video dataset，展示了时序信息的重要性。为了生成actionproposals，把2D region proposals扩展到3D，假定spatialextent在一个clip内是固定的。随着时间的推移，用较大空间唯一的actiontubes将要违反假设，特别是当clip是很长，涉及actors或者camera的快速移动。</li><li><strong>modeling spatio-temporal dependencies</strong>:理解人类行为要求理解它们身边的人和物体。一些方法用图结构的网络、注意力机制来汇集视频中的物体和人的上下文信息。时空关系通过多层图结构的自注意力来学习，这个能够连接连续clips的entities，因此考虑long-rangespatial and temporal dependencies。</li><li>Metrics for Spatio-temporal action detection:<strong>frame-AP</strong>: measures the area under the precision-recallcurve of the detections for each frame. frame中的IoU大于某个阈值且actionlabel是正确的，则detection 是正确的。<strong>video-AP</strong>: measuresthe area under the precision-recall curve of the action tubespredictions。如果整个视频帧中，mean per frame IoU大于某个阈值且actionlabel预测正确，则tube 是正确的.</li></ul></li></ol><h4id="a-survey-on-deep-learning-based-spatio-temporal-action-detection"><strong>ASurvey on Deep Learning-based Spatio-temporal ActionDetection</strong></h4><blockquote><p>作者是来自浙大和蚂蚁集团的Peng Wang, Fanwei Zeng和YuntaoQian，论文引用[2]:Wang, Peng et al. “A Survey on Deep Learning-basedSpatio-temporal Action Detection.” ArXiv abs/2308.01618 (2023): n.pag.</p></blockquote><ol type="1"><li><p>Spatio-temporal action detection(STAD)旨在对视频中出现的行为进行分类，然后在空间和时间上进行定位。传统的STAD方式涉及到了滑动窗口，例如deformablepart models, branch and bound approach.模型主要划分为2类：frame-level和clip-level；<em>frame-level</em>预测 2Dbounding box for a frame; <em>clip-level</em>预测 3D spatio-temporaltubelets for a clip.</p></li><li><p><strong>Frame-level</strong>:目标检测做的很成功，研究人员将目标检测的模型泛化到STAD领域，直接的思路是：把STAD in video视为 2D image检测的集合。具体地说，在每一帧上用action detector来检测得到<strong>frame-level 2D bounding box</strong>。然后用<strong>linking ortracking算法</strong>关联这些frame-level detection results，生成<strong>3D action proposals</strong>。作者从<strong>Temporalcontext</strong>、<strong>3D CNN</strong>、<strong>High efficiency andreal-time speed</strong>、<strong>Visual RelationModeling</strong>这几个角度给出了相关的算法。</p><p>有些借鉴了RCNN、FasterRCNN的思路，用了RPN网络，然后用两个分支分别处理RGB和光流；然后融合外观和运动信息，Linkedup里得到 class-specific action tubes. 也有基于actionnessmaps的方法。<strong>actionness是指在图像的特定位置包含一般的actioninstance的可能性</strong>。上述这些STAD方法独立的对待frame，忽视了时序上下文关系。为了克服这个问题，有人提出了<strong>cascadeproposal and location anticipationmodel(CPLA)</strong>的方法，能够推理发生在两帧之间的运动趋势。用<em>frame <span class="math inline">\(I_t\)</span> 上检测到的bbox来推理<span class="math inline">\(I_{t+k}\)</span> frame上对应的 bbox。<spanclass="math inline">\(k\)</span>是 anticipation gap</em>.除了通过光流来获取视频里的运动特性之外，可以用 <font color=red> 3D CNN</font>来从多个相邻帧提取运动信息。后续的还有用<strong>X3D</strong>网络、<strong>ACDnet</strong>、将<strong>光流和RGB</strong>嵌入到一个单流网络中，利用光流来modulateRGB特征、用<strong>SSD</strong>作为检测器、借鉴YOLO的<strong>YOWO</strong>：3DCNN来提取时空信息，2Dmodel来提取空间信息、<strong>WOO</strong>：单个统一的网络，只用一个backbone来做actorlocalization和actionclassification、<strong>SE-STAD</strong>用FCOS作为目标检测器、<strong>EVAD</strong>用ViTs，通过dropout non-kkeyframe tokens减小计算开销，refine scenencontext来增强模型性能。</p></li><li><p>frame-level的方法没有完全利用时序的信息，将视频帧视为独立的图像，因此提出了clip-level的STAD方法，将一系列的frames作为输入，直接输出检测到的<font color=red>tubelet proposals(short sequence of bounding boxes)</font></p></li><li><p><strong>Clip-level</strong>: 输入一个video clip，模型输出一个3Dspatio-temporal tubelet proposals。3D tubelet proposals是由一系列的bboxes that tightly bound the actions of interest形成。然后这些tubelet proposals在successiveclips连接在一起，形成完整的action tubes。作者从几个<strong>Largemotion</strong>、<strong>Progressive learning</strong>、<strong>Anchorfree</strong>、<strong>Visual RelationModeling</strong>这几个角度给出了相关的算法。为了克服 3D anchors的 fixedspatial exntet的问题，有人提出了 <font color=red> two-framemicor-tubes</font>的方法。为了避免3D cuboid anchor，也有人提出了<font color=red> 通过frame-level actor detection，然后将detectedbboxes连接起来形成class-independent action tubelets,然后给到temporalunderstanding module来做行为分类 </font>。还有<strong>sparse-to-dense</strong>的方法。在<em>progressivelearning</em>方面，通过progressive learning 方法，反复修正proposalstowards actions over a fewsteps。有人提出了<strong>PCSC</strong>框架，以迭代的方式，用一个stream(RGB/Flow)里的regionproposals和features来帮助另一个stream(RGB/Flow)提高action localizationresults。计算anchor是一个比较费劲的事情，提出了一些<strong>anchor-free</strong>的方法：有人把每个actioninstance 视为moving points的轨迹。</p><ul><li><strong>MovingCenterDetector(MOC-detector)</strong>，它由3个branches组成：center-branch forinstance center detection and action recognition; movement branch formovement estimation;box branch for spatial extent detection.</li><li><strong>VideoCapsuleNet</strong>：用3D conv along withcapsules来学习必要的语义信息 for action detection and recognition。有一个定位的component，利用capsules得到的action representation for apixel-wise localization of actions.</li><li><strong>TubeR</strong>：直接检测视频里的actiontubelet，同时执行action localization和recognition from a singlerepresentation. 设计了一个tubelet-attention moduel 来model dynamicspatio-temporal nature of a video clip. TubeR学习了tubeletqueries的集合，输出actio tubelets.</li></ul></li></ol><p>在<strong>Visual Relation Modeling</strong>方面，clip-level 的visualrelations也被探索了，来增强STAD模型；有人提出了 long short-term relationnetwork(LSTR)，获取short-term 和long-term relations in videos。具体地说： LSTR先产生3D bboxes(tubelets) in eachvideo。然后通过<strong>spatio-temporal attention mechanism</strong> ineach clip 来建模human-context interactions。推理long-term temporaldynamics across video clips via <em>graph ConvNet in a cascadedmanner</em>。 actor tubelets和objectproposals的特征然后被用于构建关系图，建模human-object manipulations andhuman-human interaction actions。</p><ol start="4" type="1"><li><p><strong>Linking up the Detection Results</strong>:actions会持续一段时间，通常跨很多帧和clips。在frame-level或者clip-level检测结果得到之后，很多方法用一个<strong>linkingalgorithm</strong>来detections across frames orclips连接起来，形成video-level的action tubes。</p><ul><li><strong>linking up frame-level detection boxes</strong>：第一个frame-level action detection linking算法是由<strong>Gkioxari</strong>提出的，他们假设两个相邻regionproposals(bboxes)的空间范围有很好的重叠，且scores很高，有很大的可能性belinked。计算两个region proposals的linking score的公式为： <spanclass="math display">\[s_c(R_t,R_{t+1})=s_c(R_t)+s_c(R_{t+1})+\lambda\cdotov(R_t,R_{t+1}), Eq.(1)\]</span></li></ul><p><span class="math inline">\(s_c\)</span>(R_i)是region proposalR_i的class specific score，<spanclass="math inline">\(ov(R_i,R_j)\)</span> 是<spanclass="math inline">\(R_i\)</span>和<spanclass="math inline">\(R_j\)</span>的 IoU(overlap)。<spanclass="math inline">\(\lambda\)</span>是一个超参数，对IoU项进行加权，有些模型输出的bbox是带有actionnessscores，这里就用actionness scores代替class-specificscores。计算出所有的linking scores之后，<font color=red>最优的path</font>通过这个来搜索： <span class="math display">\[\barR_c^*=\underset{\barR}{\text{argmax}}\frac{1}{T}\sum_{t=1}^{T-1}s_c(R_t,R_{t+1}),Eq.(2)\]</span></p><p><span class="math inline">\(\bar{R}_{c} =[R_{1},R_{2},\ldots,R_{T}]\)</span> 是action class <spanclass="math inline">\(c\)</span>的一系列的linked region.通过维特比算法来解这个优化问题。找到最有的path之后，region proposals in<span class="math inline">\(\bar{R}_{c}\)</span> 从 set of regionproposals中去掉，然后再继续解该方程，直到set of regionproposals是空的。从Eq.(2)中计算得到的path被称为 <font color=red> actiontube</font>。 action tube <spanclass="math inline">\(\bar{R}_{c}\)</span> 定义为：<spanclass="math inline">\(S_{c}(\bar{R}_{c})=\frac1T\sum_{t=1}^{T-1}s_{c}(R_{t},R_{t+1}).\)</span></p><ul><li>基于Gkioxari的思路，Peng等人提出了在 Eq.(1)增加一个阈值函数，linkingscore between two region proposals变成了： <spanclass="math display">\[s_{c}(R_{t},R_{t+1})=s_{c}(R_{t})+s_{c}(R_{t+1})+\lambda\cdotov(R_{t},R_{t+1})\cdot\psi(ov)\]</span></li></ul><p><span class="math inline">\(\psi(ov)\)</span>是一个阈值函数，当<spanclass="math inline">\(ov\)</span>大于<spanclass="math inline">\(\tau\)</span>时，<spanclass="math inline">\(\psi(ov)=1\)</span>，否则<spanclass="math inline">\(\psi(ov)=0\)</span>。Peng在实验中发现，有了这个阈值函数，linkingscore比之前更好了更robust了。Kopuklu进一步扩展了这个linkingscore的定义：</p><p><span class="math display">\[\begin{aligned}s_{c}\left(R_{t},R_{t+1}\right)=&amp;\psi(ov)\cdot[s_{c}\left(R_{t}\right)+s_{c}\left(R_{t+1}\right) \\&amp;+\alpha\cdot s_{c}\left(R_{t}\right)\cdot s_{c}\left(R_{t+1}\right)\\&amp;+\beta\cdot ov\left(R_{t},R_{t+1}\right)] ,\end{aligned}\]</span></p><p>其中<span class="math inline">\(\alpha\)</span>和<spanclass="math inline">\(\beta\)</span>是超参数，<spanclass="math inline">\(\alpha \cdot s_c(R_t)\cdots_c(R_{t+1})\)</span>项将两个连续帧之间的dramatic change考虑进去，提高video detection的性能</p><ul><li>在<strong>Temporal trimming</strong>中，上述的linking算法得到了action tubes 横跨整个video duration，然而，human actions通常只占很小一部分。为了决定一个actioninstance的<strong>时间范围</strong>。有一些temporaltrimming的工作。Saha限制了consecutive proposals来得到smooth actionnessscores。通过动态规划解一个energymaximization的问题。Peng等人依赖一个高校的maximum subarray方法：给定一个video-level action tube <spanclass="math inline">\(\bar{R}\)</span>, 它的理想的时间范围是从frame<span class="math inline">\(s\)</span> to frame <spanclass="math inline">\(e\)</span>，满足下列公式：</li></ul><p><spanclass="math display">\[s_{c}(\bar{R}_{(s,e)}^{\star})=\underset{(s,e)}{\operatorname*{argmax}}\{\frac{1}{L_{(s,e)}}\sum_{i=s}^{e}s_{c}(R_{i})-\lambda\frac{|L_{(s,e)}-L_{c}|}{L_{c}}\},\]</span></p><p><span class="math inline">\(L_{(s,e)}\)</span>是 action tube的长度，<span class="math inline">\(L_c\)</span>是 class <spanclass="math inline">\(c\)</span>在训练集中的平均时长。</p><ul><li><p>在<strong>online action tubegeneration</strong>中，在视频的第一帧，用 <spanclass="math inline">\(n\)</span>个detected bboxes来初始化 <spanclass="math inline">\(n\)</span> action tubes for each class <spanclass="math inline">\(c\)</span>。然后，actiontubes通过增加frame中的box扩大或者在<spanclass="math inline">\(k\)</span>个连续帧之后没有匹配的boxes，就会终结。最后，每个更新的tube通过执行binarylabeling using a online Viterbi算法来进行temporally trimmed。</p></li><li><p><strong>Linking up Clip-level DetectionResults</strong>:clip-level tubelet linking算法旨在 <strong>associate asequence of clip-level tubelets into video-level actiontubes</strong>。它们通常是从frame-levelbox中得到。一个tubelet内的内容应该获取一个action，在任何两个连续的clips连接的tubelets应该有一个大的<font color=red> temporal overlap</font>，因此，他们定义tubelet's的linking score是这样的： <spanclass="math display">\[S=\frac{1}{m}\sum_{i=1}^{m}Actionness_{i}+\frac{1}{m-1}\sum_{j=1}^{m-1}Overlap_{j,j+1}\]</span></p></li></ul><p><span class="math inline">\(Actionness_i\)</span> 表示 第<spanclass="math inline">\(i\)</span>个clip的tubelet的actionness score。<spanclass="math inline">\(Overlap_{j,j+1}\)</span>表示来自第<spanclass="math inline">\(j\)</span>和第<spanclass="math inline">\(j+1\)</span>个clip的两个proposals的Overlap。<spanclass="math inline">\(m\)</span>是videoclips的总数，两个tubelets之间的overlap是基于<strong>第<spanclass="math inline">\(j\)</span>个tubelet的最后一帧和<spanclass="math inline">\(j+1\)</span>个tubelet的第一帧</strong>来计算的。在计算出了tubelets'分数之后；另外，有人把frame linking的算法扩展到tubelet linking来构建action tubes。核心的idea是这样的： -初始化：在视频的第一帧，对每个tubelet开始一个new link，这里a link 指asequence of linked tubelets - Linking: 给定一个new frame <spanclass="math inline">\(f\)</span>，扩展存在的links with one o the tubeletcandidates starting at this frame. 选择tubeletcandidate的标准如下：没有被其它links选择；有最高的actionscore；和要被扩展的link的overlap高于给定的阈值。 -终止：对于一个存在的Link，如果这个标注在<spanclass="math inline">\(K\)</span>个连续帧之后没有被满足，这个link就会终止，<spanclass="math inline">\(K\)</span>是一个给定的超参数。</p><p>由于它的简单和高效，tubelet linking算法被很多最近的工作采用。</p><p>在<strong>temporal trimming</strong>方面。tubelet linking算法，初始和终止决定了actiontubes的时间范围，有人发现它不能彻底的解决transition state产生的temporallocation error。定义为ambiguous states，但不属于targetactions。为了解决这个问题，有人提出了transition-awareclassifier：能够区分transitional states和realactions；后续也有人通过引入一个action switch regressionhead：决定一个boxprediction是否描述了一个执行actions的actor。这个regressionhead给出了一个tubelet每个bbox的action switchscore。如果这个score高于给定的阈值，这个box就包含这个action。这个actionswitch regression head能够有效减小transitional states的误分类。</p></li><li><p>数据集：STAD中经常用到的数据集有：</p><ul><li><strong>Weizmann</strong>：在一个统一的背景中用一个静态相机记录，包含90个videoclips grouped 10 action classes, performed by 9 diffferentsubjects，每个video clip 包含多个单一行为的实例，空间分辨率为$180$，每个clip是从1-5s。</li><li><strong>CMU Crowded Videos</strong>：包含5个actions，每个action有5个training videos和48个testvideos。所有的video被缩放，空间分辨率为<span class="math inline">\(120\times 160\)</span>，testvideos是5-37秒(166-1115帧)，这个数据集是在一个凌乱的和动态的环境中记录的，以至于这个数据集上的actiondetection更加有挑战性。数据集是denselyannotated，提供时间和空间坐标(x,y,height,width,start,end andframes)。</li><li><strong>MSR Action I andII</strong>：是微软研究组弄的，II是I的扩展，Action I包含62个actioninstances in 16个video sequences。II包含203 instances in 54videos，每个video包含不同个体执行的多个actions。所有的视频是32-76秒，每个actioninstance的时间和空间标注是提供的。包含3个action 类别。</li><li><strong>J-HMDB</strong>：是joint-annotatedHMDB数据集，HMDB包含5个action categories，每个category包含至少101个视频片段，数据集包含6849个视频片段，分布在51个actioncategories中。J-HMDB包含从HMDB数据集中宣导的21类视频，选择的视频涉及单个任务的动作，每个actionclass有36-55个clips，每个clip包含15-40帧，总共928个clips，每个clip被裁剪了，第一帧和最后一帧对应一个action的开始和终止。frame的分辨率是<span class="math inline">\(320 \times 240\)</span>, frame rate 是30fps。</li><li><strong>UCF Sports</strong>：包含体育领域的10个actions，所有的视频包含相机运动和复杂背景，包含150个clips，每个clip的framerate是10 fps，空间分辨率是 <span class="math inline">\(480 \times360\)</span> to <span class="math inline">\(720 \times576\)</span>，持续时间是 2.2 - 14.4秒，平均6.39秒。</li><li><strong>UCF101-24</strong>：UCF101的数据来自Youtube，包含101行为类别，总共13320个视频。对于行为检测任务，包含24个行为类别的3207个视频子集提供了密集标注，这个子集称为UCF101-24，不同于UCFSports和J-HMDB，视频是被剪过的，UCF101-24是没有被剪过的。</li><li><strong>THUMOS andMultiTHUMOS</strong>：THMOS系列数据集包含4个数据集：THUMOS13，THUMOS14,THUMOS15和MultiTHUMOS，所有的视频是来自UCF101，THUMOS数据集包含24个actionclasses，视频的时长从几秒到几分钟不等。数据集包含13000个被剪过的视频，超过1000个没有剪过的视频，超过2500个negativesamplevideo。这些视频可能包含none、one、或者单个行为或者多个行为的实例。MultiTHUMOS是一个THUMOS的增强的版本，是一个dense、multi-class、frame-wiselabeled video dataset with 400 videos of 30hours和65个类别的38690个标注。平均每帧有1.5个标注，每个视频10.5个行为类别。</li><li><strong>AVA</strong>：来自Youtube的430个movies，每个movie提供了第15到30分钟的这个clip，每个clip分成897个重叠3s的segmentswith a stride of 1second。对于每个segment，中间帧被选为keyframe，在每个keyframe，每个人都用bbox和actions标注，430个movies分成235个training，64个validation和131test movies，差不多是55:15:30的比例，包含80个原子行为，60个actions用来evaluation。</li><li><strong>MultiSports</strong>：这个视频来自Youtube上奥林匹克和世界杯的竞赛，包含4个运动，66个行为类别，每个运动800个clips，共3200个clips；包含37701个actioninstances with902k个bboxes，每个行为类别的instance从3个到3477个不等，显示了自然的长尾分布。每个视频被多个行为类别的多个实例标注，视频的平均长度是750帧，每个行为的segment比较短，平均24帧。</li></ul></li><li><p>评估指标：主要是两个：frame mAP和video-mAP</p><ul><li><strong>Frame mAP</strong>: area under the PR curve of bboxdetections at each frame.<strong>如果和GTbbox的IoU大于给定的阈值且actionlabel是正确的，则detection是对的，阈值设为0.5</strong>。Frame-mAP能够独立于linkingstrategy来比较检测精度。</li><li><strong>Video-mAP</strong>：area under PR curve of action tubepredictions。<strong>如果和GT tube的IoU大于给定的阈值且actionlabel是正确的，则tubedetection是对的</strong>，两个tubes之间的IoU被定义为时序上的IoU，<strong>multipliedby the average of the IoU between boxes averaged over all overlappingframes</strong>。 video-mAP的阈值通常设为0.2、0.5、0.75，and0.5:0.95。对应于average video-mAP for thresholds with step 0.05 in thisrange.然而frame-mAP衡量的是单帧里的分类和空间检测的能力，video-mAP能够进一步评估时序检测的能力。</li></ul></li><li><p>未来的方向：</p><ul><li><strong>Lable-efficient learning forSTAD</strong>：STAD需要密集的标注，然而密集的标注是昂贵的。</li><li><strong>Online real-timeSTAD</strong>：STAD有很多的在线的应用，必须基于过去的数据来给出当前帧的预测。这要求模型必须是轻量和高效的，还有很长的路。</li><li><strong>STAD under largemotion</strong>：在真实场景中，很多行为由于fast actordisplacement，camera motion,actions有很大的motion。</li><li><strong>Multimodal learning for STAD</strong>：actionvideo包含多个模态，包括视觉、声音甚至语言，因此，通过多模态学习，有潜力实现比单个模态更好的检测精度。另一方面，actions可以通过多种传感器得到，例如深度相机，红外相机，Lidar等，STAD或者可以从多个模态数据中学到的融合表征受益。</li><li><strong>Diffusion models forSTAD</strong>：扩散模型作为一类生成模型，从 sample in随机分布开始，通过逐步地去噪恢复样本数据。尽管它们属于生成模型，它们对于表征的感知任务(例如目标检测和时序动作定位)，表现有效,输入随机的spatialboxes(temporal proposals)，基于扩散的模型能够精确地产生目标框(actionproposals)，自从STAD视为目标检测和时序动作定位(temporal actionlocation)和结合体，有一些工作展示了利用diffusionmodels来解决STAD任务。</li></ul></li></ol><h4 id="参考链接">参考链接：</h4><ul><li>https://0809zheng.github.io/2021/07/15/stad.html：时空行为检测的比较好的介绍</li></ul><p>一些STAD相关的解释和算法图示：</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/2217f65a3fade532f079dbefe1230b47063417b7/8-Figure7-1.png"alt="Spatio-Temporal Action Detection Task" /><figcaption aria-hidden="true">Spatio-Temporal Action DetectionTask</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/2-Figure1-1.png"alt="Spatio-temporal action detection" /><figcaption aria-hidden="true">Spatio-temporal actiondetection</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/2-Table1-1.png"alt="Comparision" /><figcaption aria-hidden="true">Comparision</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/2-Figure2-1.png"alt="illustration" /><figcaption aria-hidden="true">illustration</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/3-Figure3-1.png"alt="Taxonomy of STAD models" /><figcaption aria-hidden="true">Taxonomy of STAD models</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/3-Figure4-1.png"alt="RPN for STAD" /><figcaption aria-hidden="true">RPN for STAD</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/4-Figure5-1.png"alt="I3D for STAD" /><figcaption aria-hidden="true">I3D for STAD</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/4-Figure6-1.png"alt="YOWO" /><figcaption aria-hidden="true">YOWO</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/5-Figure7-1.png"alt="ARCN" /><figcaption aria-hidden="true">ARCN</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/6-Figure8-1.png"alt="CFAD" /><figcaption aria-hidden="true">CFAD</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/10-Table2-1.png"alt="STAD datasets" /><figcaption aria-hidden="true">STAD datasets</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/12-Table3-1.png"alt="Performance on J-HMDB and UCF101-24" /><figcaption aria-hidden="true">Performance on J-HMDB andUCF101-24</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/13-Table4-1.png"alt="Performance on UCF Sports and MultiSports datasets" /><figcaption aria-hidden="true">Performance on UCF Sports and MultiSportsdatasets</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/13-Table5-1.png"alt="Performance on AVA" /><figcaption aria-hidden="true">Performance on AVA</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;h4
id=&quot;视频理解及分析的计算机视觉任务&quot;&gt;视频理解及分析的计算机视觉任务&lt;/h4&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;之前看的时候，不管是论文还是一些博客，感觉都不是很清晰和全面，大家的定义不全面，特别是英文的名称上，这里写一下我的理解：&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;几个任务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;行为识别(Action Recognition)&lt;/strong&gt;:
实质是对视频的分类任务，可以类别图像领域的分类任务&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时序动作定位(Temporal Action Localization)&lt;/strong&gt;:
在时间上对视频进行分类，给出动作的起止时间和类别&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时空行为检测(Spatio-Temporal Action Detection)&lt;/strong&gt;:
不仅识别出动作出现的&lt;strong&gt;区间&lt;/strong&gt;和&lt;strong&gt;类别&lt;/strong&gt;，还要在空间范围内用一个bounding
box标记处目标的&lt;strong&gt;位置&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;还有人提出了时空动作定位(Spatio-temporal Action
localization)&lt;/strong&gt;：和上一个是一样的&lt;/li&gt;
&lt;li&gt;Action Detection在Paperswithcode上的定义： aims to find both where
and when an action occurs within a video clip and classify what the
action is taking place. Typically results are given in the form of
&lt;strong&gt;action tublets&lt;/strong&gt;, which are action bounding boxes linked
across time in the video. &lt;em&gt;This is related to temporal localization,
which seeks to identify the start and end frame of an action, and action
recognition, which seeks only to classify which action is taking place
and typically assumes a trimmed video.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;论文里还提到了&lt;strong&gt;temporal action segmentation&lt;/strong&gt;：
针对细粒度的actions和videos with dense occurrence of actions to predict
action label labels at every frame of the video.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;时空行为检测的算法：之前的论文都是都是基于行为识别(action
recognition)的，很多都是基于早期的Slowfast的那个检测的方式：需要一个额外的检测器，实现行为检测。也就是在行为识别的基础上，再进行时空行为检测。但这并不是我理想中的方式，所以很多行为识别的算法，在AVA上也能上榜；最近看VideoMAE看了之后，就一直在看这个，没有去看看其它的。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Action Detection数据集：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;J-HMDB&lt;/li&gt;
&lt;li&gt;UCF101-24&lt;/li&gt;
&lt;li&gt;MultiSports&lt;/li&gt;
&lt;li&gt;AVA&lt;/li&gt;
&lt;li&gt;其中，JHMDB和UCF101-24是密集标注数据集(每一帧都标注，25fps)，这类数据集每个视频只有一个动作，大部分视频是单人做一些语义简单的重复动作；AVA为代表的稀疏标注数据集(隔一段时间标注一帧，1fps)，没有给出明确的动作边界&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Video Understanding" scheme="https://young-eng.github.io/YoungBlogs/tags/Video-Understanding/"/>
    
  </entry>
  
  <entry>
    <title>ollama 大模型部署</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/19/ollama-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/19/ollama-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/</id>
    <published>2024-08-19T13:22:22.000Z</published>
    <updated>2024-08-20T02:49:57.242Z</updated>
    
    <content type="html"><![CDATA[<h3id="记录一下用ollama和openwebui来部署几个大模型qwen2llama3和llava">记录一下用ollama和openwebui来部署几个大模型：Qwen2、LLaMa3和LLaVa</h3><h4 id="安装ollama-及pull-model">安装Ollama 及pull model</h4><ol type="1"><li><p>去ollama的官网下载安装ollama</p></li><li><p>更改变量：windows中添加环境变量: OLLAMA_MODELS:XXXXpath，linux需要到systemd中找到ollama的哪个文件，然后进行修改，这样ollamapull 模型的时候，就会安装到指定的路径</p></li><li><p>ollama安装完成后，可以用ollama pullqwen2:7b这样来下载模型，也可以下载模型的GGUF文件，然后需要写一个配置文件，如config.txt，内容如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">FROM &quot;path/to/llama3-8b-cn-q6/Llama3-8B-Chinese-Chat.q6_k.GGUF&quot;</span><br><span class="line"></span><br><span class="line">TEMPLATE &quot;&quot;&quot;&#123;&#123;- if .System &#125;&#125;</span><br><span class="line">&lt;|im_start|&gt;system &#123;&#123; .System &#125;&#125;&lt;|im_end|&gt;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">&#123;&#123; .Prompt &#125;&#125;&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">SYSTEM &quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">PARAMETER stop &lt;|im_start|&gt;</span><br><span class="line">PARAMETER stop &lt;|im_end|&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p></li></ol><span id="more"></span><p>这个From后面的路径需要修改，然后使用<code>ollama create llama3-cn -f ./config.txt</code>导入模型，导入成功后，可以用<code>ollama list</code>查看，使用<code>ollama run xxx</code>运行，使用<code>/exit</code>退出</p><h4 id="openwebui">OpenWebUI</h4><ol type="1"><li><p>运行之前需要先安装docker</p></li><li><p>运行命令:<code>docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</code>，通过Docker运行OpenWebUI，这里输入<code>localhost:3000</code>就能访问，如果没有看到模型，需要修改<code>OLLAMA_HOSTS=0.0.0.0</code>，然后重新启动ollama的服务</p></li><li><p>docker的相关命令：</p><ul><li><p><code>docker ps</code>：查看正在运行的docker容器</p></li><li><p><code>docker kill container_id</code>：杀死正在运行的docker容器</p></li><li><p><code>docker rm container_id</code>：删除已经停止的docker容器</p></li></ul></li></ol><h4id="llava及相关视觉语言模型vlm的微调">LLaVa及相关视觉语言模型(VLM)的微调</h4><h4 id="参考链接">参考链接</h4><ul><li><p>https://www.cnblogs.com/obullxl/p/18295202/NTopic2024071001</p></li><li><p>https://blog.csdn.net/u010522887/article/details/140651584</p></li><li><p>https://liaoxuefeng.com/blogs/all/2024-05-06-llama3/index.html</p></li><li><p>https://www.cnblogs.com/obullxl/p/18295202/NTopic2024071001</p></li><li><p>https://github.com/qianniucity/ollama-doc/blob/main/ollama/docs/ollama%20%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E7%AD%94.md</p></li><li><p>https://www.cnblogs.com/ting1/p/18358286</p></li><li><p>https://github.com/open-webui/open-webui</p></li><li><p>https://github.com/ollama/ollama/blob/main/docs/faq.md</p></li><li><p>https://blog.csdn.net/joeyoj/article/details/136427362</p></li><li><p>https://blog.csdn.net/2401_85328934/article/details/139749167(用lobe chat代替openWebUI)</p></li><li><p>https://cuterwrite.top/p/ollama/ (用Continue 插件部署自己的 codecopliot)</p></li><li><p>https://mp.weixin.qq.com/s/vt1EXVWtwm6ltZVYtB4-Tg</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;记录一下用ollama和openwebui来部署几个大模型qwen2llama3和llava&quot;&gt;记录一下用ollama和openwebui来部署几个大模型：Qwen2、LLaMa3和LLaVa&lt;/h3&gt;
&lt;h4 id=&quot;安装ollama-及pull-model&quot;&gt;安装Ollama 及pull model&lt;/h4&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;去ollama的官网下载安装ollama&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;更改变量：windows中添加环境变量: OLLAMA_MODELS:
XXXXpath，linux需要到systemd中找到ollama的哪个文件，然后进行修改，这样ollama
pull 模型的时候，就会安装到指定的路径&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ollama安装完成后，可以用ollama pull
qwen2:7b这样来下载模型，也可以下载模型的GGUF文件，然后需要写一个配置文件，如config.txt，内容如下：
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;FROM &amp;quot;path/to/llama3-8b-cn-q6/Llama3-8B-Chinese-Chat.q6_k.GGUF&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;TEMPLATE &amp;quot;&amp;quot;&amp;quot;&amp;#123;&amp;#123;- if .System &amp;#125;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;lt;|im_start|&amp;gt;system &amp;#123;&amp;#123; .System &amp;#125;&amp;#125;&amp;lt;|im_end|&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;#123;- end &amp;#125;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;lt;|im_start|&amp;gt;user&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;#123; .Prompt &amp;#125;&amp;#125;&amp;lt;|im_end|&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;lt;|im_start|&amp;gt;assistant&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SYSTEM &amp;quot;&amp;quot;&amp;quot;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;PARAMETER stop &amp;lt;|im_start|&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;PARAMETER stop &amp;lt;|im_end|&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Tools" scheme="https://young-eng.github.io/YoungBlogs/categories/Tools/"/>
    
    
    <category term="LLM" scheme="https://young-eng.github.io/YoungBlogs/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>强化学习</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/14/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/14/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</id>
    <published>2024-08-14T09:51:44.000Z</published>
    <updated>2024-09-04T13:49:43.177Z</updated>
    
    <content type="html"><![CDATA[<h3 id="强化学习">强化学习</h3><ol type="1"><li>基本概念<ul><li>智能体(agent)</li><li>环境(environment)</li></ul></li></ol><span id="more"></span><ol start="2" type="1"><li><p>马尔可夫决策过程：智能体与环境交互进行学习最终实现目标的过程就是马尔可夫决策过程(MarkovDecisionProcess,MDP)：智能体与环境交互的过程：在t时刻，智能体在当前状态<spanclass="math inline">\(S_t\)</span>采取动作 <spanclass="math inline">\(A_t\)</span>。在下一时刻 <spanclass="math inline">\(t+1\)</span>，智能体接收到环境反馈的对于动作 <spanclass="math inline">\(A_t\)</span>的奖励 <spanclass="math inline">\(R_{t+1}\)</span>，以及该时刻状态 <spanclass="math inline">\(S_{t+1}\)</span>。从而，MDP和智能体共同给出一个轨迹：<span class="math display">\[S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2,R_3, S_3, A_3, ...\]</span> <spanclass="math inline">\(S_t\)</span>是有限状态的集合，<spanclass="math inline">\(A_t\)</span>是有限动作的集合，<spanclass="math inline">\(P\)</span>是基于环境的状态转移矩阵：其中每一个项为智能体在某个状态下<spanclass="math inline">\(s\)</span>下，采取动作 <spanclass="math inline">\(a\)</span>后，与环境交互后转移到其它状态 <spanclass="math inline">\(s&#39;\)</span>的概率值，表示为 <spanclass="math inline">\(P(S_{t+1}=s&#39;|s_t = s, a_t = a)\)</span>。</p><p>R是奖励函数：智能体在某个状态<spanclass="math inline">\(s\)</span>下，采取动作a后，与环境交互后所获得的奖励，表示为<spanclass="math inline">\(R(s_t = s, a_t = a)\)</span>。 <spanclass="math inline">\(\gamma\)</span>是折扣因子(discounted factor),取值区间为[0,1]。</p><p>所以<strong>MDP过程可以表示为 <spanclass="math inline">\((S,A,P,R,\gamma)\)</span>，如果该过程中的状态转移矩阵<spanclass="math inline">\(P\)</span>和奖励 <spanclass="math inline">\(R(s,a)\)</span>对智能体都是可见的，称这样的Agent为Model-basedAgent，否则称为Model-free Agent</strong>。</p></li><li><p>策略梯度定理：策略函数的参数化可表示为 <spanclass="math inline">\(\pi_{\theta}(s,a)\)</span>，其中<spanclass="math inline">\(\theta\)</span>为一组参数，函数取值表示在状态<spanclass="math inline">\(s\)</span>下选择动作a的概率。为了优化策略函数，首先需要有一个对策略函数优劣进行衡量的标准。假设强化学习问题的初始状态为<span class="math inline">\(s_0\)</span>，则希望达到最大化的目标为：<span class="math display">\[J(\theta):=V_{\pi_\theta}(s_0)\]</span>其中, <span class="math inline">\(v_{\pi_theta}\)</span>是在策略 <spanclass="math inline">\(\pi_{\theta}\)</span>下的真实价值函数，这个价值函数代表衡量一个状态的价值，<strong>即一个状态采取所有行为后的一个价值的期望值</strong>。如果能求出梯度<spanclass="math inline">\(\nabla_{\theta}J(\theta)\)</span>，就可以用<strong>梯度上升法</strong>求解这个问题，即求解价值函数的最大值。</p><p>在这个函数中，<spanclass="math inline">\(J(\theta)\)</span>既依赖于动作的选择，又依赖于动作选择时所处状态的分布。给定一个状态，策略参数对动作选择及收益的影响可以根据参数比较直观地计算出来，但因为状态分布和环境有关，所以策略参数对动作选择及收益的影响可以根据参数比较直观地计算出来，但因为状态分布和环境有关，所以策略对状态分布的影响一般很难确切知道。<spanclass="math inline">\(J(\theta)\)</span>对模型参数的梯度却依赖于这种未知的影响，那么如何估计这个梯度呢。Sutton等人在文献中给出了这个梯度的表达式： <spanclass="math display">\[\nabla_\thetaJ(\theta)\propto\sum_s\mu_{\pi_\theta}(s)\sum_aq_{\pi_\theta}(s,a)\nabla_\theta\pi_\theta(s,a)\]</span>其中，<span class="math inline">\(\mu_{\pi_\theta}(s)\)</span>称为策略<span class="math inline">\(\pi_{theta}\)</span>的策略分布。在持续问题中，<spanclass="math inline">\(\mu_{\pi_\theta}(s)\)</span>为算法 <spanclass="math inline">\(s_0\)</span>出发经过无限多步后位于状态 <spanclass="math inline">\(s\)</span>的概率。</p></li><li><p>蒙特卡洛策略梯度定理：更具策略梯度定理表达式计算策略梯度并不是一个简单的问题，其中对<spanclass="math inline">\(\mu_{\pi_\theta}\)</span>和 <spanclass="math inline">\(q{\pi_\theta}\)</span>的准确比较难。用蒙特卡洛法能用来估计这类问题的取值。</p></li><li><p>REINFORCE算法：REINFORCE（蒙特卡洛策略梯度）算法是一种策略参数学习方法，其中策略参数<span class="math inline">\(\theta\)</span>的更新方法为梯度上升法，它的目标是为了最大化性能指标 <spanclass="math inline">\(J(\theta)\)</span>，其更新公式为：</p><p><spanclass="math display">\[\theta_{t+1}=\theta_t+\alpha\widehat{\nablaJ(\theta_t)}\]</span> 根据蒙特卡洛定理中对 <spanclass="math inline">\(\nabla_\theta J(\theta)\)</span>的计算，则有：</p><p><span class="math display">\[\nabla_\thetaJ(\theta)=\mathbb{E}_{s,a\sim\pi}[G_t\nabla_\theta\ln\pi_\theta(s,a)]\]</span>根据上述梯度更新公式，得到蒙特卡洛策略梯度更新公式：</p><p><spanclass="math display">\[\theta=\theta+\eta\gamma^{&#39;}G\nabla_\theta\ln\pi_\theta(s_t,a_t)\]</span>其中， <span class="math inline">\(\Eta\)</span>为学习率，<spanclass="math inline">\(\gamma&#39;\)</span>为衰减率，在REINFORCE算法中，暂不考虑衰减问题，设置<spanclass="math inline">\(\gamma&#39;=1\)</span>。</p><p>REINFOCE算法流程： 输入：马尔可夫决策过程<spanclass="math inline">\(MDP=(S,A,P,R,\gamma)\)</span>，即状态，智能体，决策，奖励和折现系数。输出：策略<span class="math inline">\(\pi(a|s,\theta)\)</span>。即在状态为<spanclass="math inline">\(s\)</span>，参数为<spanclass="math inline">\(\theta\)</span>的条件下，选择动作a的概率，具体流程如下：</p><ol type="1"><li>随机初始化；</li><li>repeat</li><li>根据一个策略<spanclass="math inline">\(\pi_{\theta}\)</span>采样一个片段(episode，即智能体由初始状态不断通过动作与环境交互，直至终止状态的过程)，得到<spanclass="math inline">\(s_0,a_0,R_1,s_1,a_1,r_2,...,s_{T-1},a_{T-1},R_T\)</span>；<ol type="1"><li>for t <span class="math inline">\(\leftarrow\)</span> 0 to <spanclass="math inline">\(T-1\)</span> do <span class="math inline">\(G=\sum_{k=t}^{T-t}\gamma_{k-t}R_{t+k}\)</span>；G是对回报的计算，回报是奖励随时间步的积累，这里<span class="math inline">\(\gamma=1\)</span>。 <spanclass="math inline">\(\theta=\theta+\eta \gamma&#39;G\nabla_\theta\ln\pi_\theta(s_t,a_t)\)</span>；其中<spanclass="math inline">\(\eta\)</span>是学习率，策略梯度采用神经网络来拟合策略梯度函数，计算策略梯度用于优化策略网络。</li><li>end for</li></ol></li><li>until 收敛；</li></ol></li><li><p><strong>SARSA(State-Action-Reward-State-Action)是一个学习马尔可夫决策过程策略的算法</strong>，通常用于机器学习和强化学习领域中。其学习更新函数依赖的5个值：**当前状态S1，当前状态选中的动作A1，获得的奖励Reward，S1状态下执行A1后取得的状态S2及S2状态下将会执行的动作A2。核心思想简化为：<span class="math display">\[Q(S_t,A_t)=Q(S_t,A_t)+\alpha[R_{t+1}+\gammaQ(S_{t+1},A_{t+1})-Q(S_t,A_t)]\]</span></p><p>其中， <span class="math inline">\(Q(S_{t+1},A_{t+1})\)</span>是下一时刻的状态和实际采取的行动对应的Q值，<spanclass="math inline">\(Q(S_{t},A_{t})\)</span>是当前时刻的状态和实际采取的行动对应的Q值，折扣因子<spanclass="math inline">\(\gamma\)</span>的取值范围为[0,1]，其本质是一个衰减值，如果gamma更接近0，agent趋向于只考虑瞬时奖励值；反之如果接近1，则agent为延迟奖励赋予更大的权重，侧重于延迟奖励；奖励值<spanclass="math inline">\(R_{t+1}\)</span>为 t+1时刻得到的奖励值，<spanclass="math inline">\(\alpha\)</span>是学习率。</p></li><li><p><strong>Q-Learning</strong>：<strong>Q-Learning属于值函数近似算法中，蒙特卡洛方法和时间差分法结合的算法。Q-Learning假设可能出现的动作a和状态S是有限多，这时a和S的全部组合也是有限多个</strong>，并且引入价值量Q表示智能体认为做出某个a时所能够获得的利益。在这种假设下，智能体收到S，选择可以产生最大的Q的a。</p><ul><li>Q-learning的训练过程就是Q表的Q值逐渐调整的过程，其核心是根据已经知道的Q值，当前选择行动a作用于环境获得的回报R，和下一轮<spanclass="math inline">\(S_{t+1}\)</span>对应可以获得的最大利益Q，总共3个量，进行加权求和算出新的Q值，来更新Q表：<span class="math display">\[Q(S_t,A_t)=Q(S_t,A_t)+\alpha[R_{t+1}+\gammamax Q(S_{t+1},a)-Q(S_t,A_t)]\]</span></li></ul><p>其中，<span class="math inline">\(Q(S_{t+1},a)\)</span>是在<spanclass="math inline">\(t+1\)</span>时刻的状态和采取的行动(并不是实际行动，所以公式采用了所有可能采取行动的Q的最大值)对应的Q值，<spanclass="math inline">\(Q(S_{t},a_t)\)</span>是当前时刻的状态和实际采取的行动对应的Q值，折扣因子 <spanclass="math inline">\(\gamma\)</span>的取值范围为[0,1]。</p><p>Q-Learning的缺点是有限且离散的状态。</p></li><li><p><strong>DQN</strong>：当状态比较多的时候，Q表格很难进行存储。为了解决这个问题，一种解决方案是将Q表格参数化，使用深度神经网络拟合动作价值函数<spanclass="math inline">\(q_{\pi}\)</span>。<strong>参数化可以解决无限状态下的动作价值函数的存储问题，因为算法只需记住一组参数，动作价值函数的具体值可根据这一组参数算出</strong>。</p><p>但是，动作价值函数的参数化也会带来一些新的问题：因为相邻样本来自同一条轨迹，会导致样本间关联性过强，而集中优化关联性过强的样本会导致神经网络处理其他样本时无法取得较好的结果。有人提出了深度Q网络(DeepQ-Network,DQN)，其本质是Q-learning算法，<strong>但使用深度学习网络拟合Q函数，解决了无线状态下的动作价值函数存储问题，同时采用经验重现(ExperienceReplay)和固定Q目标(Fixed-Q-Target)两个创新点来解决上述问题</strong>。</p><ul><li><font color=red>经验重现(ExperienceReplay)</font>：使用一个经验池存储多条经验<spanclass="math inline">\(s,a,r,s&#39;\)</span>，再从中随机抽取一批用于训练，很好地解决了样本关联性的问题，同时，因为经验池里的经验可以得到重复利用，也提升了效率。</li><li><font color=red>固定Q目标(Fixed-Q-target)</font>：复制一个和原来Q网络结构一样的TargetQ网络，用于计算Q目标值，这样在原来的Q网络中，targetQ就是一个固定的数值，不会再产生优化目标不明确的问题。</li></ul><p>流程：再DQN算法中，智能体会在与所处环境中<spanclass="math inline">\(environment\)</span>进行交互后，获得一个环境提供的状态<spanclass="math inline">\(s_t\)</span>。接受状态后，智能体会根据深度学习网络预测出在该状态下不同行动<spanclass="math inline">\(action\)</span>对应的Q值，并给出一个行动<spanclass="math inline">\(a_t\)</span>，当行动反馈给环境后，环境会给出对应的奖励<spanclass="math inline">\(r_t\)</span>、新的状态<spanclass="math inline">\(s_{t+1}\)</span>，以及是否触发终止条件<spanclass="math inline">\(done\)</span>，每一次交互完成后，DQN算法都会将<span class="math inline">\(s_t, a_t, r_t, s_{t+1},done\)</span>作为一条经验存储在经验池中，每次会从经验池中抽取一定量的经验作为输入数据训练神经网络。</p><p>具体流程：</p><ol type="1"><li>初始化经验池，随机初始化Q网络；</li><li>for episode=1,M do:</li><li>重置环境，获得第一个状态；</li><li>for t=1, T do:<ol type="1"><li><p>用 <span class="math inline">\(\epsilon-greedy\)</span>策略生成一个action；其中有 <spanclass="math inline">\(\epsilon\)</span>的概率会随机选择一个action，即为探索模式；其它情况下，则 <spanclass="math inline">\(a_t = max_a Q(s_t,a;\theta)\)</span>，选择在 <spanclass="math inline">\(s_t\)</span>状态下使得Q最大的action，即为经验模式；</p></li><li><p>根据动作与环境的交互，获得反馈的reward <spanclass="math inline">\(r_t\)</span>、下一个状态 <spanclass="math inline">\(s_{t+1}\)</span>和是否触发终止条件done。</p></li><li><p>将经验 <span class="math inline">\(s_t, a_t, r_t,s_{t+1},done\)</span> 存入经验池；</p></li><li><p>从经验池中随机获取一个minibatch的经验；</p></li><li><p><spanclass="math inline">\(\left.Qtarget_t=\left\{\begin{array}{c}r_t,\quadif done\\r_t+\gammamax_{a^{\prime}}Qtarget(s_{t+1},a^{&#39;};\theta),\quad if notdone\end{array}\right.\right.\)</span></p></li><li><p>根据<span class="math inline">\(Qpred_t\)</span>和 <spanclass="math inline">\(Qtarget_t\)</span>求loss，梯度下降法更新Q网络。</p></li></ol></li><li>end for</li><li>每隔固定个episode，更新Qtarget网络</li><li>end for</li></ol></li><li><p><strong>Actor-Critic</strong>：在REINFORCE算法中，每次需要根据一个策略采集一条完整的轨迹，并计算这条轨迹上的回报，这种采样的方式的方差比较大，学习效率比较低。可以借鉴时序差分学习的思路，使用<strong>动态规划</strong>来提高采样的效率，即从状态s开始的总回报可以通过当前动作的即时奖励<spanclass="math inline">\(r(s,a,s&#39;)\)</span>和下一个状态<spanclass="math inline">\(s&#39;\)</span>的值函数来近似估计。演员-评论家算法(Actor-Critic)是一种结合策略梯度和时序差分学习的强化学习算法，包括：演员(Actor)和评价者(Critic)，跟生成对抗网络(GAN)的流程类似：</p><ul><li>演员(Actor)是指策略函数 <spanclass="math inline">\(\pi_{\theta}(a|s)\)</span>，即学习一个策略来得到尽量高的回报，用于生成动作(Action)并和环境交互。</li><li>评论家(Critic)是指值函数<spanclass="math inline">\(V^{\pi}(s)\)</span>，对当前策略的值函数进行估计，即评估演员的好坏。用于评估Actor的表现，并指导Actor下一阶段的动作。</li></ul><p>借助于值函数，演员-评论家算法可以进行单步更新参数，不需要等到回合结束才进行更新。在Actor-Critic算法里，最知名的方法就是A3C(Asynchronous AdvantageActor-Critic)。</p><ul><li>如果去掉Asynchronous,只有AdvantageActor-Critic，就叫做<strong>A2C</strong>。</li><li>如果加上了Asynchronous，变成Asynchronous AdvantageActor-Critic，就变成<strong>A3C</strong>。</li></ul><p>Actor-Critic是Q-learning和PolicyGradient的结合，为了导出Actor-Critic算法，必须先了解PolicyGradient算法是如何一步步优化策略的。</p></li></ol><h3 id="参考链接">参考链接</h3><ul><li><code>https://paddlepedia.readthedocs.io/en/latest/tutorials/reinforcement_learning/index.html</code>,PaddlePaddle 强化学习</li></ul><figure><imgsrc="https://github.com/ZhangHandi/images-for-paddledocs/blob/main/images/reinforcement_learning/MDP.png?raw=true"alt="智能体-环境交互过程" /><figcaption aria-hidden="true">智能体-环境交互过程</figcaption></figure><figure><imgsrc="https://raw.githubusercontent.com/w5688414/paddleImage/main/rl_img/dqn_alg.png"alt="Reinforcement Learning" /><figcaption aria-hidden="true">Reinforcement Learning</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;强化学习&quot;&gt;强化学习&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;基本概念
&lt;ul&gt;
&lt;li&gt;智能体(agent)&lt;/li&gt;
&lt;li&gt;环境(environment)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Learning" scheme="https://young-eng.github.io/YoungBlogs/categories/Learning/"/>
    
    
    <category term="Reinforcement Learning" scheme="https://young-eng.github.io/YoungBlogs/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Softwares&#39; installations</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/07/17/Softwares-installations/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/07/17/Softwares-installations/</id>
    <published>2024-07-17T10:05:36.000Z</published>
    <updated>2024-09-03T11:54:34.864Z</updated>
    
    <content type="html"><![CDATA[<h3id="在服务器的个人账户下上安装cuda">在服务器的个人账户下上安装cuda</h3><ol type="1"><li><p>去<ahref="https://developer.nvidia.com/cuda-toolkit-archive">cudaarchive</a>里找到对应的cuda版本的runfile文件，通过shxxx.run来安装</p></li><li><p>安装的时候，需要去Options里更改toolkit和library的path，设置完后即可进行install</p></li><li><p>install完了之后，需要去bashrc里添加以下内容：</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="string">&quot;/xxx/cuda/bin:<span class="variable">$PATH</span>&quot;</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="string">&quot;/xxx/cuda/lib64:<span class="variable">$LD_LIBRARY_PATH</span>&quot;</span></span><br></pre></td></tr></table></figure><ol start="4" type="1"><li><p>然后source一下bashrc，再nvcc-V，如果显示版本号，则说明安装成功</p></li><li><p>遇到需要本地cuda的时候，可以</p></li></ol><p><code>export CUDA_HOME=="/xxx/cuda/"</code></p><h3 id="在服务器个人账户下安装gcc">在服务器个人账户下安装GCC</h3><ol type="1"><li><p>去清华源下载需要的gcc的版本，网址在这里:<code>https://mirrors.tuna.tsinghua.edu.cn/gnu/gcc/</code></p></li><li><p>解压完后，进入文件夹gcc-x.x.x，执行命令：<code>./contrib/download_prerequisites</code></p></li><li><p><code>make build</code>，新建一个文件夹，<code>cd build</code>，<code>../configure --prefix=/home/xxx/gcc-x.x.0 --enable-shared --enable-threads=posix --enable-languages=c,c++,fortran --disable-multilib</code></p></li><li><p><code>make -j10 &amp;&amp; make install</code></p></li><li><p>打开 <code>.bashrc</code>，添加一下内容：</p></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/path/to/install/gcc-5.4/bin:$PATH</span><br><span class="line">export LD_LIBRARY_PATH=/path/to/install/gcc-5.4/lib/:/path/to/install/gcc-5.4/lib64:$LD_LIBRARY_PATH`</span><br><span class="line"></span><br></pre></td></tr></table></figure><ol start="6" type="1"><li>source一下bashrc，然后<code>gcc --version</code>，如果显示版本号，则说明安装成功<span id="more"></span></li></ol><h3 id="参考链接">参考链接</h3><ul><li><code>https://blog.csdn.net/qq_35082030/article/details/110387800</code></li><li><code>https://blog.csdn.net/sdbyp/article/details/139853774</code></li><li><code>https://blog.csdn.net/Sihang_Xie/article/details/127347139</code></li></ul><p>非root环境下安装gcc -<code>https://blog.csdn.net/Fhujinwu/article/details/113786909</code> -<code>https://blog.csdn.net/qq_33278461/article/details/106357783?spm=1001.2014.3001.5502</code>-<code>https://blog.csdn.net/qq_38308388/article/details/127574517</code>- <code>https://www.cnblogs.com/TonyYPZhang/p/17614809.html</code></p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;在服务器的个人账户下上安装cuda&quot;&gt;在服务器的个人账户下上安装cuda&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;去&lt;a
href=&quot;https://developer.nvidia.com/cuda-toolkit-archive&quot;&gt;cuda
archive&lt;/a&gt;里找到对应的cuda版本的runfile文件，通过sh
xxx.run来安装&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;安装的时候，需要去Options里更改toolkit和library的path，设置完后即可进行install&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;install完了之后，需要去bashrc里添加以下内容：&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; PATH=&lt;span class=&quot;string&quot;&gt;&amp;quot;/xxx/cuda/bin:&lt;span class=&quot;variable&quot;&gt;$PATH&lt;/span&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; LD_LIBRARY_PATH=&lt;span class=&quot;string&quot;&gt;&amp;quot;/xxx/cuda/lib64:&lt;span class=&quot;variable&quot;&gt;$LD_LIBRARY_PATH&lt;/span&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ol start=&quot;4&quot; type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;然后source一下bashrc，再nvcc
-V，如果显示版本号，则说明安装成功&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;遇到需要本地cuda的时候，可以&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;export CUDA_HOME==&quot;/xxx/cuda/&quot;&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;在服务器个人账户下安装gcc&quot;&gt;在服务器个人账户下安装GCC&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;去清华源下载需要的gcc的版本，网址在这里:
&lt;code&gt;https://mirrors.tuna.tsinghua.edu.cn/gnu/gcc/&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;解压完后，进入文件夹gcc-x.x.x，执行命令：&lt;code&gt;./contrib/download_prerequisites&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;make build&lt;/code&gt;，新建一个文件夹，&lt;code&gt;cd build&lt;/code&gt;，&lt;code&gt;../configure --prefix=/home/xxx/gcc-x.x.0 --enable-shared --enable-threads=posix --enable-languages=c,c++,fortran --disable-multilib&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;make -j10 &amp;amp;&amp;amp; make install&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;打开 &lt;code&gt;.bashrc&lt;/code&gt;，添加一下内容：&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;export PATH=/path/to/install/gcc-5.4/bin:$PATH&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;export LD_LIBRARY_PATH=/path/to/install/gcc-5.4/lib/:/path/to/install/gcc-5.4/lib64:$LD_LIBRARY_PATH`&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ol start=&quot;6&quot; type=&quot;1&quot;&gt;
&lt;li&gt;source一下bashrc，然后&lt;code&gt;gcc --version&lt;/code&gt;，如果显示版本号，则说明安装成功</summary>
    
    
    
    <category term="methods" scheme="https://young-eng.github.io/YoungBlogs/categories/methods/"/>
    
    
  </entry>
  
  <entry>
    <title>绘图</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/07/01/%E7%BB%98%E5%9B%BE/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/07/01/%E7%BB%98%E5%9B%BE/</id>
    <published>2024-07-01T07:54:56.000Z</published>
    <updated>2024-07-01T07:58:44.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="matplotlib-绘图">Matplotlib 绘图</h3><ol type="1"><li>3D plot时候的需要注意的地方</li></ol><p>https://www.codenong.com/48442713/</p><p>https://www.coder.work/article/2032713#google_vignette</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;matplotlib-绘图&quot;&gt;Matplotlib 绘图&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;3D plot时候的需要注意的地方&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;https://www.codenong.com/48442713/&lt;/p&gt;
&lt;p&gt;h</summary>
      
    
    
    
    <category term="methods" scheme="https://young-eng.github.io/YoungBlogs/categories/methods/"/>
    
    
    <category term="tips" scheme="https://young-eng.github.io/YoungBlogs/tags/tips/"/>
    
  </entry>
  
  <entry>
    <title>大模型综述</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/06/29/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/06/29/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/</id>
    <published>2024-06-29T05:41:52.000Z</published>
    <updated>2024-06-29T05:41:54.000Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Object Tracking Survey</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/06/03/Object-Tracking-Survey/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/06/03/Object-Tracking-Survey/</id>
    <published>2024-06-03T12:41:20.000Z</published>
    <updated>2024-06-03T12:42:50.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="visual-objec-trackingvot">Visual Objec Tracking(VOT)</h3><h3 id="multiple-object-trackingmot">Multiple Object Tracking(MOT)</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;visual-objec-trackingvot&quot;&gt;Visual Objec Tracking(VOT)&lt;/h3&gt;
&lt;h3 id=&quot;multiple-object-trackingmot&quot;&gt;Multiple Object Tracking(MOT)&lt;/h3&gt;
</summary>
      
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Object Tracking" scheme="https://young-eng.github.io/YoungBlogs/tags/Object-Tracking/"/>
    
  </entry>
  
  <entry>
    <title>OC-SORT</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/06/02/OC-SORT/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/06/02/OC-SORT/</id>
    <published>2024-06-02T07:30:25.000Z</published>
    <updated>2024-06-20T13:25:52.000Z</updated>
    
    <content type="html"><![CDATA[<h3id="observation-centric-sort-rethinking-sort-for-robust-multi-object-tracking1">Observation-CentricSORT: Rethinking SORT for Robust Multi-ObjectTracking<sup>[1]</sup></h3><blockquote><p>作者是来自CMU、上海AI Lab和英伟达的Junkun Cao, Jiangmiao Pang,Xinshuo Weng, Rawal Khirodkar, Kris Kitani. 论文引用[1]:Cao, Jinkun etal. “Observation-Centric SORT: Rethinking SORT for Robust Multi-ObjectTracking.” 2023 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) (2022): 9686-9696.</p></blockquote><h3 id="time">Time</h3><ul><li>2023.Feb</li></ul><h3 id="key-words">Key Words</h3><ul><li>object observations</li><li><strong>O</strong>bservation-<strong>C</strong>entric<strong>SORT</strong>, Simple，Online, and Real-Time</li><li>occlusion and non-linear object motion</li></ul><span id="more"></span><h3 id="动机">动机</h3><ol type="1"><li><p>基于卡尔曼滤波的多目标追踪的方法是假设目标线性运动，这个假设对于很短的occlusion是可接受的，时间变长了的话，运动的线性估计会很不准确。Moreover，当这里没有可用的measurement来更新卡尔曼滤波的参数时，惯例是相信prioristate estimations for posterioriupdate。这会导致occlusion期间的误差累积，这个误差会在实践中造成严重的运动方向的多变。</p></li><li><p>旨在开发一个motion model-based multi-objecttracking(MOT)，对于occlusion和non-linear motion有很好的鲁棒性。</p></li><li><p>当occlusion和non-linear motion同时发生的时候，motion model-basedtracking方法的误差会发生。</p></li></ol><h3 id="总结">总结</h3><ol type="1"><li><p>基于卡尔曼滤波的多目标追踪的方法是假设目标线性运动，这个假设对于很短的occlusion是可接受的，时间变长了的话，运动的线性估计会很不准确。Moreover，当这里没有可用的measurement来更新卡尔曼滤波的参数时，惯例是相信prioristate estimations for posterioriupdate。这会导致occlusion期间的误差累积，这个误差会在实践中造成严重的运动方向的多变。本文中，作者展示了一个卡尔曼滤波当很好地处理了occlusion期间的噪声累加时，也能得到很好的效果。除了仅仅依赖于线性状态估计(estimation-centricapproach)，用object observations(measurements by objectdetector)来计算一个虚拟的轨迹 over occlusion period，来修复occlusion期间的滤波器的误差累加，提高了occlusion和non-linearmotion期间的robustness。取名为<strong>Observation-CentricSORT</strong>，</p></li><li><p>目前的现存的motion model-based算法，假设一个时间段内追踪的物体速度不变，称之为线性运动假设，这个假设在许多实际场景中不成立。但是时间间隔足够小的时候，仍然有效，物体的motion能够近似为线性。为了减小造成的副作用，重新思考当前的模型，然后认识到了一些局限。为了追踪的性能，提出解决的方法：<strong>especiallyin occlusion</strong>。</p></li><li><p>基于滤波器的方法(filtering-based)作为motion model-basedtracking，假设一个转换函数，来预测未来时刻的目标的状态，称之为 stateestimation；除了estimations，利用一个observationmodel，例如一个目标检测器，来推导一个目标物体的statemeasurements，也称之为observations。observation通常作为一个附属的信息来帮助更新滤波器的后验参数。轨迹能够被stateestimation来扩展。<strong>Among this line ofwork</strong>，用的最广的是SORT，用一个Kalmanfilter来估计目标的state，一个linear motion function最为<strong>trasitionfunction between time steps</strong>， 然而，当objectmotion是非线性、更新滤波器的后验参数的时候没有观察到observation，SORT的鲁棒性不够。</p></li><li><p>SORT的三个<strong>limitations</strong>；</p><ul><li>尽管高帧率能够近似物体的运动为线性的，它也放大了模型对于noise ofstateestimations的sensitivity。特别是，在高帧率视频的连续帧之间，物体位移的噪声可能与实际物体位移的大小相同，导致KF估计的物体速度有很大的方差。速度估计的噪声会通过transitionprocess累计进入到位置估计(position estimate)。</li><li>KF状态估计的噪声会随着时间累加，当在KF更新的阶段没有observation的时候。作者展示了相对于没有追踪到目标物体的时间，误差累加会非常快，在速度方向上的噪声的影响经常使得<strong>tracklost</strong>即使在re-association之后。</li><li>考虑到现在检测器的发展，object state by detections相比于滤波器里的固定的trainsition function得到的stateestimation，有很低的方差。然而，<strong>SORT是通过 state estimation而不是 observations来prolong object trajectories</strong>。</li></ul></li><li><p>为了减小上述限制的负面影响，两点创新：</p><ul><li><p>设计了一个module来用object state observations来减小track丢失的时候的累计的误差；准确地说，在传统的 <em>predict 和update</em>阶段之外，在一段untrack的时间之后，通过关联到一个observation，track被重新激活，trigger <em>re-update</em>，<em>re-update</em> 用 <strong>virtualobservation</strong> on the historical time steps to prevent erroraccumulation。 virtual observations 来自 a trajectory generated usingthe last-seen observatio before untracked and the latest observationre-activating this track as anchors. 称之为 <strong>Observation-centricRe-Update</strong>。</p></li><li><p>线性运动的假设提供了目标物体运动的方向，但这个cue很难用在SORTassociation上，因为directionestimation的大量的noise。提出了一个observation-centric方式，将tracks的方向一致性引入到cost matrix 里 for association。称之为<strong>Observation-Centric Momentum</strong></p></li></ul></li><li><p>Motion Models: 大部分的MOT算法用了motion models，这些motionmodels用贝叶斯估计通过最大化后验估计，来预测下一个状态。最为最经典的motionmodels，<strong>Kalman filter(KF)</strong>是一个recursive贝叶斯滤波器，跟随经i的那的 <em>predict-update</em>循环，measurements是来自隐马尔可夫模型的observations。给定一个先行运动假设限制KF，紧接着的一个工作是ExtendedKF和UnscentedKF,提出这两个是为了解决<strong>一阶和二阶泰勒估计的非线性motion</strong>；然而，它们还是依赖于KF假设的近似高斯先验，要求motionpattern assumption。另一个方面，粒子滤波通过sample-based后验估计来解决非线性motion，但是计算量是指数级的。因此，Kalmanfilter的变体和粒子滤波很少在视觉多目标追踪中采用，用的最多的motionmodel还是基于Kalman filter的。</p></li><li><p>Multi-object Tracking:多目标跟踪是从概率的角度进行的，现代的videoobject tracking通常建立在目标检测器上，<strong>SORT采用Kalman filter formotit ion-based multi-object tracking, given observations from deepdetectors</strong>，DeepSORT进一步将深度视觉特征引入object associationunder framework ofSORT，当场景拥挤，基于ReID的目标关联流行起来。最近，transformer被引入到MOT中，来学习深度特征from both visual information and objecttrajectories。然而，它们的性能和SOTA的tracking-by-detection的方法还有很大的差距，不管是精度还是时间效率。</p></li><li><p>Kalman filter是一个在时间域离散的动态系统的线性估计器。KF只要求前一时刻的stateestimations和当前的measurement来估计下一时刻的targetstate。这个滤波器维护两个变量: **后验状态估计 x, 后验估计协方差矩阵 P.在目标追踪任务，用一个状态变换模型F、观测模型H、过程噪声Q和观测噪声R来描述KF过程。在每个时刻t，给定观测z_t， KF在 predict和update两个阶段交替进行：</p><p><spanclass="math display">\[\begin{aligned}&amp;predict\begin{cases}\hat{\mathbf{x}}_{t|t-1}=\mathbf{F}_{t}\hat{\mathbf{x}}_{t-1|t-1}\\\mathbf{P}_{t|t-1}=\mathbf{F}_{t}\mathbf{P}_{t-1|t-1}\mathbf{F}_{t}^{\top}+\mathbf{Q}_{t}\end{cases},\\&amp;update\begin{cases}\mathbf{K}_{t}=\mathbf{P}_{t|t-1}\mathbf{H}_{t}^{\top}(\mathbf{H}_{t}\mathbf{P}_{t|t-1}\mathbf{H}_{t}^{\top}+\mathbf{R}_{t})^{-1}\\\hat{\mathbf{x}}_{t|t}=\hat{\mathbf{x}}_{t|t-1}+\mathbf{K}_{t}(\mathbf{z}_{t}-\mathbf{H}_{t}\hat{\mathbf{x}}_{t|t-1})\\\mathbf{P}_{t|t}=(\mathbf{I}-\mathbf{K}_{t}\mathbf{H}_{t})\mathbf{P}_{t|t-1}\end{cases}.\end{aligned}\]</span></p><ul><li>predict阶段是来推断下一时刻的state estimations。给定下一时刻的targetstates的measurement，update阶段旨在更新KF中的后验参数。因为measurement来自观测模型H，因此在很多场景中成为observation。</li><li>SORT是建立在KF上的MOT, SORT里的KF的状态x定义为 $ x=[u,v,s,r,,,] $,<span class="math inline">\((u,v)\)</span>是image里的目标的center的coordinates, <spanclass="math inline">\((s,r)\)</span>分别是bbox的 scale(area)和 aspectratio。aspect ratio假定为常数。其它的 <spanclass="math inline">\(\dot{u},\dot{v},\dot{s}\)</span>分别是相应的推导量。obervation是一个bbox<span class="math inline">\(z = [u,v,w,h,c]^T， (u,v)是object center,w,h分别为weight和height，c为detection confidence.\)</span>.SORT假设线性运动为过渡模型F，状态估计为： <spanclass="math display">\[u_{t+1}=u_t+\dot{u}_t\Delta t,\quadv_{t+1}=v_t+\dot{v}_t\Delta t.\]</span></li><li>为了利用SORT中的KF for visual MOT，predict阶段估计下一帧的objectposition。来自detection model的observation来更新stage。updatestage是来更新Kalman filter的参数，不直接edit tracking outcomes。</li><li>当两个step的时间差在transition中是常数，例如视频帧率是常数；当帧率很高，SORT表现很好即使objectmotion是非线性的，因为在短的时间内可以近似为线性的。然而实际中<strong>observation通常会缺失，例如目标被occluded</strong>,这种情况下，无法通过update阶段来更新KFparameters。SORT通常用先验的直接作为后验，称之为<strong>dummyupdate</strong>。</li><li><span class="math display">\[ \hat{x}_{t|t} = \hat{x}_{t|t-1},P_{t|t} = P_{t|t-1}\]</span></li></ul><p>称这个算法为<strong>estimation-centric</strong>。当遇到non-linear和occlusion的时候，这个方法就不太行。</p></li><li><p>很小的 <spanclass="math inline">\(\Delta{t}\)</span>会放大噪声。</p></li><li><p>Observation-Centric SORT, we introduce the proposed<em>observation-Centric SORT(OC-SORT)</em>,为了解决上述SORT的限制。用目标进入associtationstage的momentum来开发一个在occulsion和非线性motion时，less noise和morerobustness的pipeline。核心是将tracker设计为<strong>observation-centric</strong>而不是<strong>estimation-centric</strong>。如果一个track从untracked恢复，用一个<em>observation-centric Re-Update</em>来抵消untracked期间的累计错误。OC-SORT也在associationcost中加了Observation-Centric Momentum(OCM)。</p></li><li><p>Observation-Centric Re-Update(ORU):</p><ul><li>当一个track经过一段时间的untrack之后，又与observation关联起来(re-activation)，检查lost期间和重新更新KF的参数。参考untracked开始和结束时的observation，生成virtual trajectory。例如：untrack之前的last-seen observation为 <spanclass="math inline">\(z_t1\)</span>，triggeringre-association的observation为$z_t2，virtual trajectory为： <spanclass="math display">\[\tilde{\mathbf{z}}_t=Traj_{\text{virtual}}(\mathbf{z}_{t_1},\mathbf{z}_{t_2},t),t_1&lt;t&lt;t_2.\]</span>然后接着 run the loop of predict and re-update.</li></ul></li><li><p>Observation-Centric Momentum(OCM):</p><ul><li><p>时间很短的间隔中，将motion近似为线性的，线性motion假设要求持续的motiondirection。但是noise阻止了利用direction的一致性。更准确的说，为了决定motiondirection，需要目标state on two steps with a time difference <spanclass="math inline">\(\delta t\)</span>。如果 <spanclass="math inline">\(\delta t\)</span> 太小了，因为estimation对statenoise敏感，所以velocity noise很重要。如果 <spanclass="math inline">\(\delta t\)</span> 太大了，directionestimation的noise同样也是 significant，因为temporal errormagnification和failure of linear motion assumption。提出用observations而不是 estimations来降低motion directioncalculation的noise，引入consistency来帮助association。</p></li><li><p>给定 <span class="math inline">\(N\)</span>个存在的tracks和 <spanclass="math inline">\(M\)</span>个 detections on the new-coming timestep. Association Cost matrix是： <spanclass="math display">\[C(\hat{\mathbf{X}},\mathbf{Z})=C_{\mathrm{IoU}}(\hat{\mathbf{X}},\mathbf{Z})+\lambdaC_v(\mathcal{Z},\mathbf{Z}),\]</span></p></li></ul><p><span class="math inline">\(\hat{X} \in R^{N \times 7}\)</span> 是object state estimations的集合，<span class="math inline">\(Z \in R^{M\times 5}\)</span>是 observations在new time step的集合。</p></li><li></li></ol><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/643914ae0ad9dcc5868348e4d867374aedbf9531/4-Figure2-1.png"alt="pipleline" /> <span class="math inline">\(Fig.2^{[1]}\)</span>Pipeline of proposed OC-SORT，Red boxes are detections, orange boxes areactive tracks, blue boxes are untracked tracks, dashed boxes are theestimates from KF. During association, OCM is used to add the velocityconsistency cost. The target #1 is lost on the frame t+1 because ofocclusion. But on the next frame, it is recoverd by referring to itsobservation of the frame t by OCR. It being re-tracked triggers ORU fromt to t+2 for the parameters of its KF。</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;observation-centric-sort-rethinking-sort-for-robust-multi-object-tracking1&quot;&gt;Observation-Centric
SORT: Rethinking SORT for Robust Multi-Object
Tracking&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自CMU、上海AI Lab和英伟达的Junkun Cao, Jiangmiao Pang,
Xinshuo Weng, Rawal Khirodkar, Kris Kitani. 论文引用[1]:Cao, Jinkun et
al. “Observation-Centric SORT: Rethinking SORT for Robust Multi-Object
Tracking.” 2023 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) (2022): 9686-9696.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2023.Feb&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;object observations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;O&lt;/strong&gt;bservation-&lt;strong&gt;C&lt;/strong&gt;entric
&lt;strong&gt;SORT&lt;/strong&gt;, Simple，Online, and Real-Time&lt;/li&gt;
&lt;li&gt;occlusion and non-linear object motion&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Object Tracking" scheme="https://young-eng.github.io/YoungBlogs/tags/Object-Tracking/"/>
    
  </entry>
  
  <entry>
    <title>OSTrack</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/05/22/OSTrack/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/05/22/OSTrack/</id>
    <published>2024-05-22T14:38:59.000Z</published>
    <updated>2024-05-22T14:39:00.000Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Mixformer</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/05/22/Mixformer/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/05/22/Mixformer/</id>
    <published>2024-05-22T14:33:43.000Z</published>
    <updated>2024-05-30T16:11:26.000Z</updated>
    
    <content type="html"><![CDATA[<h3id="mixformer-end-to-end-tracking-with-iterative-mixed-attention1">MixFormer:End-to-End Tracking with Iterative Mixed Attention<sup>[1]</sup></h3><blockquote><p>作者是来自南大的Yutao Cui, Cheng Jiang, Limin Wang, Gangshan Wu.论文引用[1]:Cui, Yutao et al. “MixFormer: End-to-End Tracking withIterative Mixed Attention.” 2022 IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR) (2022): 13598-13608.</p></blockquote><h3 id="time">Time</h3><ul><li>2022.Mar</li></ul><h3 id="key-words">Key Words</h3><ul><li>compact tracking framework</li><li>unify the feature extraction and target integration solely with atransformer-based architecture</li></ul><h3 id="votmotsot的区别">VOT，MOT,SOT的区别</h3><ol type="1"><li>VOT是标首帧，MOT是多目标追踪，SOT是单目标追踪。</li></ol><h3 id="动机">动机</h3><ol type="1"><li><p>Tracking经常用多阶段的pipeline：<strong>featureextraction，target information integration，bounding boxestimation</strong>。为了简化这个pipeline，作者提出了一个紧凑的tracking框架，名为MixFormer。</p></li><li><p>target information integration解释： fuse the target and searchregion information</p></li></ol><span id="more"></span><h3 id="总结">总结</h3><ol type="1"><li><p>核心设计利用是attentation操作的灵活，提出了Mixed AttentionModule(MAM) for simultaneous feature extraction and target informationintegration. 这个即时的modeling scheme，能够抽取target-specificdiscriminative features以及在target和searcharea之间能够进行大量的communication。基于MAM,通过堆叠MAMs withprogressive patch embedding 和place a localization head ontop，来构建MixFormer追踪框架。</p></li><li><p>为了在onlinetracking的时候处理多个目标的template，在MAM中设计了一个非对称的注意力，来减小计算成本，提出了一个有效的scoreprediction module来选择高质量的template。</p></li><li><p>视觉目标追踪的仍然是有很多挑战，包括：scale variations, objectdeformations, occlusions, confusion from similar objects.</p></li><li><p>之前的流行的trackers通常包括几个部分：</p><ul><li>a backbone 来提取tracking target和 search area 的feature，</li><li>为了后续的target-aware的定位，一个集成的模块(integrationmodule)来使得tracking target和search area进行信息交流</li><li>task-specific heads来精确地定位目标和估计bounding box.</li></ul></li><li><p>集成模块(integration moduel) 是追踪算法的关键，它能够整合targetinformation，在通用特征提取和目标感知定位架起桥梁。传统的集成方法包括correlation-basedoperations(e.g., SiamFC, SiamRPN, CRPN, SiamFC++, 将Simaese网络和correlation操作结合起来，对target和search之间的全局依赖进行建模),online learning 算法(e.g., DCF, KCF, ATOM, FCOT，对于有区别的tracking，学习到独立于目标的model)，最近，多亏了全局和动态的modelingcapacity，引入了transformer来进行基于注意力的integration，有了很好的效果(STMTrack,STARK)，然而，这些基于transformer的trackers仍然基于CNN作为特征提取例如ResNet，仅仅用attention操作 in the latter high-level and abstract representation space.CNN是有限的，它们通常用来pre-trained for 通用目标提取，可能忽略finestructure information for tracking.CNN用局部卷积核，缺乏全局建模能力。</p></li><li><p>为了解决以上问题，在tracking frameworkdesign上，提出了新的思路。通用特征提取和target informationintegration集成到一个统一的框架，coupled processing paradigm有很多好的advantages:</p><ul><li>能够使得feature extraction to be more specific to the correspondingtracking target and capture more target-specific discriminativefeatures.</li><li>能够使得target information to be more extentively 集成到search area,然后更好地capture their correlation</li><li>得到一个更为紧凑的tracking pipelien with a single backbone andtracking head, without an explicit integration module.</li></ul></li><li><p>Attention module是一个非常灵活的架构，building block wiht dynamicand global modeling capacity, 提出了一个mixed attentionmodule(MAM)，能够同时进行特征提取和 mutual interaction of targettemplate and search area。在MAM中，设计了一个混合的interactionscheme，包括自注意力和cross-attention operations on tokens from templateand search area。自注意力能够抽取target or searcharea的自己的特征，然而cross-attention allows for communications betweenthem(search and target area) to mix the target and search areainformations。为了降低MAM的计算成本，能够使得多个templates 来处理obectdeformation，提出了一个定制的非对称的attentionscheme通过修建不必要的target-to-search area cross-attention.</p></li><li><p>MixFormer backone: 通过堆叠Patchembedding和MAM，最后放一个简单的localization head 来得到整个的trackingframework。 为了处理追踪过程中的目标变形，提出了基于target templateupdate机制的score，MixFormer能够容易地适应multiple target templateinputs.</p></li><li><p>最近的trackers引入了基于transformer的integrationmodule，来得到更复杂的依赖(dependencies),实现更好的效果。</p></li><li><p>MAM的输入使target template和searcharea。旨在提取long-range特征，融合它们之间的特征。和原始的MHA不同，MAM执行dualattention operations on two separate tokens sequences of target templateand search area. 对每个sequences上的tokens进行自注意力来得到target orsearch specific information；同时，对来自2个sequencnes的tokens进行crossattention来得到target template and searcharea的交互信息。通过拼接concatenated token sequences来进行mix attentionmechanism.</p></li><li><p>给定一个concatenated tokens of multiple targets andsearch，首先将其分成2个部分，然后reshape成一个2D的featuremap，为了实现additional modeling of local spatial context, 在每个featuremap上用一个可分离的depth-wise convolutional projection layer. target和search的每个feature map 然后通过linear project 来得到queries, keys,values of the attention operation. <span class="math inline">\(q_t, k_t,v_t\)</span>代表target, <span class="math inline">\(q_s, k_s,v_s\)</span>代表search region. mixed attention的定义如下: <spanclass="math display">\[\begin{aligned}&amp;k_{m}=\mathrm{Concat}(k_{t},k_{s}),\quadv_{m}=\mathrm{Concat}(v_{t},v_{s}),\\&amp;\mathrm{Attention}_{\mathrm{t}}=\mathrm{Softmax}(\frac{q_{t}k_{m}^{T}}{\sqrt{d}})v_{m},\\&amp;\mathrm{Attention}_{s}=\mathrm{Softmax}(\frac{q_{s}k_{m}^{T}}{\sqrt{d}})v_{m},\end{aligned}\]</span></p><ul><li><span class="math inline">\(Attention_s\)</span>和<spanclass="math inline">\(Attention_t\)</span>包含了featureextraction和信息融合，最后，targets token 和search token通过一个linearprojection 进行concatenated 和processed。</li></ul></li><li><p>非对称的mixed attention scheme：通过剪掉不必要的target-to-searcharea cross-attention，降低了MAM的计算成本，能够高效地用multipletemplates来处理目标变形的问题。</p><p><spanclass="math display">\[\begin{aligned}&amp;k_{m}=\mathrm{Concat}(k_{t},k_{s}),\quadv_{m}=\mathrm{Concat}(v_{t},v_{s}),\\&amp;\mathrm{Attention}_{\mathrm{t}}=\mathrm{Softmax}(\frac{q_{t}k_{m}^{T}}{\sqrt{d}})v_{m},\\&amp;\mathrm{Attention}_{s}=\mathrm{Softmax}(\frac{q_{s}k_{m}^{T}}{\sqrt{d}})v_{m},\end{aligned}\]</span></p></li><li><p>Corner-based localization head: 受STARK的启发，用了全卷积的cornerbased localization head来估计tracked object的boundingbox。通过计算corner probability distribution，来得到boundingbox。</p></li><li><p>Query based localization head:在最后一个stage的序列中，加了一个可学习的regressiontoken，用这个token作为anchor来aggregate整个目标和搜索区域的信息。最后一个3个fclayers的FFN来回归bounding box。不需要后处理。</p></li><li><p><strong>Training</strong>:用CVT模型对MAM进行预训练，然后再整个目标dataset上进行微调。</p></li><li><p><strong>Template Online Update</strong>:这个再得到时序信息和处理目标形变、外观变化上很重要，在追踪的时候，低质量的templates可能导致较差的追踪性能。引入了一个scoreprediction module(SPM)。SPM由2个attention blocks，three-layerperceptron组成。一个可学习的<em>score token</em>和search ROItokens进行attention，然后scoretoken与初始目标的所有positions进行attention, implicitly compare minedtarget with first target.最后，通过一个MLP和sigmoid来得到score，低于0.5的onlinetemplate被视为negative.</p></li></ol><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/b6eaec7917439d79ce840fa97bc371552e9b6685/1-Figure1-1.png"alt="Comparison" /> <span class="math inline">\(Fig.1^{[1]}\)</span>Comparison of tracking pipeline. (a) The dominant tracking frameworkcontains three components: a convolutional or transformer backbone, acarefully-designed integration module, and task-specific heads. (b) OurMixFormer is more compact and composed of two components: atarget-search mixed attention based backbone and a simple localizationhead.</p><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/b6eaec7917439d79ce840fa97bc371552e9b6685/3-Figure2-1.png"alt="MAM" /> <span class="math inline">\(Fig.2^{[2]}\)</span> MixedAttention Module (MAM) is a flexible attention operation that unifiesthe process of feature extraction and information integration for targettemplate and search area. This mixed attention has dual attentionoperations where self-attention is performed to extract features fromitself while cross-attention is conducted to communicate between targetand search. This MAM could be easily implemented with a concatenatedtoken sequence. To further improve efficiency, we propose an asymmetricMAM by pruning the target to-search cross attention (denoted by dashedlines).</p><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/b6eaec7917439d79ce840fa97bc371552e9b6685/4-Figure3-1.png"alt="MixFormer" /> <span class="math inline">\(Fig.3^{[3]}\)</span>MixFormer presents a compact end-to-end framework for tracking withoutexplicitly decoupling steps of feature extraction and target informationintegration. It is only composed of a single MAM backbone and alocalization head.</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/b6eaec7917439d79ce840fa97bc371552e9b6685/5-Figure4-1.png"alt="SPM" /><figcaption aria-hidden="true">SPM</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;mixformer-end-to-end-tracking-with-iterative-mixed-attention1&quot;&gt;MixFormer:
End-to-End Tracking with Iterative Mixed Attention&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自南大的Yutao Cui, Cheng Jiang, Limin Wang, Gangshan Wu.
论文引用[1]:Cui, Yutao et al. “MixFormer: End-to-End Tracking with
Iterative Mixed Attention.” 2022 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) (2022): 13598-13608.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2022.Mar&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;compact tracking framework&lt;/li&gt;
&lt;li&gt;unify the feature extraction and target integration solely with a
transformer-based architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;votmotsot的区别&quot;&gt;VOT，MOT,SOT的区别&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;VOT是标首帧，MOT是多目标追踪，SOT是单目标追踪。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;动机&quot;&gt;动机&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Tracking经常用多阶段的pipeline：&lt;strong&gt;feature
extraction，target information integration，bounding box
estimation&lt;/strong&gt;。为了简化这个pipeline，作者提出了一个紧凑的tracking框架，名为MixFormer。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;target information integration解释： fuse the target and search
region information&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Object Tracking" scheme="https://young-eng.github.io/YoungBlogs/tags/Object-Tracking/"/>
    
  </entry>
  
</feed>
