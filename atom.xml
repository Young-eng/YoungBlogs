<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Young&#39;s Blog</title>
  
  
  <link href="https://young-eng.github.io/YoungBlogs/atom.xml" rel="self"/>
  
  <link href="https://young-eng.github.io/YoungBlogs/"/>
  <updated>2024-08-29T05:58:17.880Z</updated>
  <id>https://young-eng.github.io/YoungBlogs/</id>
  
  <author>
    <name>Young</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>WOO</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/27/WOO/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/27/WOO/</id>
    <published>2024-08-27T07:23:26.000Z</published>
    <updated>2024-08-29T05:58:17.880Z</updated>
    
    <content type="html"><![CDATA[<h3id="watch-only-oncean-end-to-end-video-action-detection-framework1">WatchOnly Once：An End-to-end Video Action DetectionFramework<sup>[1]</sup></h3><blockquote><p>作者是来自港大的罗平老师组的Shoufa Chen、Peize Sun、EnzeXie等人。论文引用[1]:Chen, Shoufa et al. “Watch Only Once: An End-to-EndVideo Action Detection Framework.” 2021 IEEE/CVF InternationalConference on Computer Vision (ICCV) (2021): 8158-8167.</p></blockquote><h3 id="time">Time</h3><ul><li>2021.Oct</li></ul><h3 id="key-words">Key Words</h3><ul><li>end-to-end unified network</li><li>task-specific features</li></ul><h3 id="总结">总结</h3><ol type="1"><li>提出了一个端到端的pipeline for video actiondetection。<strong>当前的方法要么是将video action detection这个任务解耦成action localization和actionclassification这两个分离的阶段，要么在一个阶段里训练两个separatedmodels</strong>。相比之下，作者的方法将actor localization和actionclassification弄在了一个网络里。通过统一backbone网络，去掉很多认为的手工components，整个pipeline被简化了。<strong>WOO</strong>用一个unifiedvideo backbone来提取features for actor location 和actionlocalization,另外，引入了<font color=red>spatial-temporal actionembeddings</font>，设计了一个 spatial-temporal fusionmodule来得到更多的含有丰富信息的discriminative features，提升了actionclassification的性能。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>Video action detection 包含 <strong>actor bboxlocalization</strong>和 <strong>action typeclassification</strong>，当前方法的复杂性来自于actorlocalization和action classification之间的基本的困境。thatis，*用一个single key frame is "positive" for actor localization，但是"negative" for actionclassification，然而用多个frames有相反的影响。这是因为actionlocalization需要一个2D的检测模型同预测video clip的key frame上的actorbbox。在这个阶段，考虑clip中的相邻帧会带来额外的计算和存储成本，相比之下，actionclassification严重依赖于一个 3D video model来提取videosequence中的时序的信息，单帧图像有很少的temporal motion representationfor action classification。</p><p>之前提出了两种可能的替代方法来解决这个困境。第一个是用离线的persondetector，用来产生actor proposals，不是和actionclassification一起训练的，然后一个独立的video model用这些actorproposals和raw frames作为输入来预测action classes。单独的一个persondetector是已经足够复杂了，which is pre-trained on the ImageNet and COCOhuman keypoint detection，and further fine-tuned on the target actiondetection dataset。这个方法比较复杂，计算成本高，需要两个separatemodels和两个训练阶段。更进一步，<strong>separate optimization on twosub-problems leads to a sub-optimal solution</strong>。</p><p>第二种类型的方法，是将actor detection和action classificationmodels一起在单个trainingstage中进行联合训练。虽然训练的pipeline在某种程度上是简化了，两个模型仍然需要独立的从rawimages中提取特征。因此，整个框架仍然有很高的computation和memorycost。</p><p><strong>一个很自然的问题是：有没有可能设计一个简单的unified的网络来解决actorlocalization和action classification in a single end-to-endmodel</strong>。</p><p>本文提出了 <strong>Watch OnlyOnce(WOO)</strong>的框架，WOO直接预测actors的bbox和action classes from avideo clip。"watch" the clip onlyonce，就能够预测actor的位置和action的类别。方法主要包含3个keycomponents：<font color=red>unified backbone、a spatial-temporal actionembedding、a spatial-temporal knowledge fusion mechanism</font>。</p><p>首先，设计了一个简单的有效的module，使得单个backbone能偶提供task-specificfeature maps for actor localization head 和action classificationhead。这个module是轻量的，能够用来isolate keyframe features from allframes in the early stage of thebackbone。这个动机是，当model走深的时候，keyframe能够得到moreinteraction with neighboringframes，提出的module能够很容易地插入到现有的backbone中，例如I3D,X3D等。</p><p><strong>另外，注意到，同一个的架构tends to behave well for actorlocalization，但是对于action classification是有限的。这个困难是actiondetection主要lies in action classification。因此，怀疑单个backbone forboth tasks是否会bias towards localization，然后找到一个undesiredsolution，因此对actionclassification的性能造成了影响。基于这个观察，提出了spatial和tempoalaction embedding和interaction mechanism between them，来使得actionclassification features在spatial 和temporal perspectives更加的discriminative。</strong></p><p>第三，提出了一个spatial-temporal fusion module来汇聚spatial和temporalknowledge，这个spatial properties例如shape和pose，temporalproperties例如dynamic motion和temporal scale ofaction,结合在一起，通过spatial-temporal fusion module，来生成actionfeatures for actio classification。</p><p>主要贡献如下：</p><ol type="1"><li><p>提出了一个端到端的框架for video action detection，给定一个videoclip作为输入，能够直接产生bboxes 和action classes，不需要独立的persondetector(这个是在之前的工作中必不可少的).</p></li><li><p>提出了一个spatial-temporal embedding，一个embedding interactionmechanism，能够提高features的discriminativeness for actionclassificatin。一个spatial-temporal fusionmodule来进一步从spatial和temporal来汇聚features。</p></li></ol></li><li><p>相关工作：</p><ul><li><p><font color=red>two-stage, twobackbone</font>：当前的STAD的SOTA模型通常用了两阶段的pipeline，用了两个backbone。这些方法简单地把STAD任务划分成actorlocalization和action classification。具体地说，在第一阶段，在COCOkeypoint上预训练一个model，然后再目标STADdataset上进行微调；第二阶段，用video clip的keyframe作为第一阶段中得到的detection model的输入，来预测actorbboxes。然后将video clip和actor bboxes作为3Dbackbone的输入，来提取RoI区域的特征 for action classprediction。<strong>这些方法有很高的复杂度和很低的效率，因为sequentialtraining stage和separatemodel架构。另外，在两个独立的阶段的独立的优化可能导致一个sub-optimal的结果</strong>。</p></li><li><p><font color=red>One-stage,two-backbone</font>：<strong>YOWO和ACRN通过同时训练2D的actor检测网络和3D video model，来简化网络。然而，这里仍然有两个separatemodels来优化</strong>。<strong>以YOWO为例，它包含一个在Kinetics上预训练的一个3D model和 YOLO pre-trained on PASCAL VOC上的 2Dmodel，有很高的计算量和memoryburden</strong>。是虽然这个pipeline在一定程度上简化了。</p></li></ul><p>相比于这些方法，WOOrefreshing的简单：给定一个videoclip，直接预测actor bboxes和对应的action classes。</p><ul><li><p><font color=red>End-to-end objectdetection</font>：最近的端到端的目标检测框架，不需要任何手工设计的过程例如NMS，直接输出预测，实现了很好的性能。在这些工作中，DETR可以被视为端到端的目标检测方法，它采用了globalattention mechanism和双向matching between predictions和ground truthobjects。DETR抛弃了NMS步骤，实现了很好的性能。它在小目标上性能不太好，相比主流的检测器需要更长的训练时间。为了解决上述的问题，提出了<strong>Deformable-DETR</strong>，来将每个objectquery 限制在 a small set of crucial sampling points around referencepoints，而不是all points in the featuremap。Deformable-DETR是高效和快速收敛的；Sparse RCNN利用一组稀疏的learnedobject proposals，以iterative way来进行分类和定位。SparseRCNN和well-established的检测器相比，展示了精度、实时和训练收敛。在本次工作中，主要采用<strong>SparseRCNN的检测头来做定位</strong>。</p></li><li><p><font color=red>Attention mechanism for actionrecognition</font>：对于language相关的任务，注意力机制是一个流行的概念。对于actiorecognition，Non-local网络利用自注意力来得到不同时间或者空间features的<strong>dependencies</strong>。使得注意力机制applicablefor action classification，有人利用non-local block作为一个long-termfeature bank operator，使得video models能得到long-terminformation，提高了action detection的性能。</p></li></ul></li><li><p>Methods: <spanclass="math inline">\(X\quad\in\quad\mathbb{R}^{C\times T\times H\timesW}\)</span> 是一个layer的输入的spatial-temporal featuremap。跟随前人的工作，in this work，将key frame放在videoclip的中间，<span class="math inline">\(X_{t=\lfloorT/2\rfloor}\in\quad\mathbb{R}^{C\times H\timesW}\)</span>表示<strong>keyframe feature map</strong>。</p><ul><li><strong>union backbone</strong>：在之前的video backbone中，key framefeatures将会和相邻的frame features通过temporalpooling或者3D卷积(temporal kernel size 大于1，会给keyframe特征带来意想不到的disturbance)进行interact。为了克服这个问题，video backbone设计的时候，将keyframe和temporal interaction之前的早期的网络的features隔离开。</li></ul><p>和之前的backbon Slowfast(将spatial stride of res5设为1，用a dilationof 2 for its filters，来增加spatial resolution of res5 by <spanclass="math inline">\(2 \times\)</span>)不同，作者去掉了res5中的dialtedconv，采用FPN module来做<strong>keyframe</strong>特征提取。FPNmodule用res2,res3,res4,res5的输出的<strong>keyframefeature</strong>作为输入。进一步用FPN的输出的特征 for actorlocalization,res5输出的特征来做actionclassification。为了这个目的，一个统一的actionbackbone用来提供task-relevant features。</p><p>以上的设计有几个好处，首先，actor localizationhead采用层次化的feature representation作为sourcefeatures。对于目标检测是有好处的。第二，用于actor localization的keyframefeatures通过FPN结构，和所有的video frames的features隔离开，starting atthe early stage of thebackbone。这能减少邻近帧的干扰，因为keyframe会随着model的深入，和邻近帧有更多的interaction。第三，相比于存在的两个backbone的网络(用独立的backbone)for actor localization，作者仅用一个轻量的FPN module that tasks imagefeatures as input，减少了参数和FLOPS。</p><ul><li><p><strong>Acotr Localization Head</strong>：受最近的SparseRCNN的启发，设计了一个端到端的actor detection head for actorlocalization,detection head在得到hierarchical features fromFPN之后，能够预测bbox和对应的scores indicating model's confidence on thebox containing an actor。另外，person detector利用 set prediction lossfor optimal bipartite matching between prediciton和ground truth attrainingstage，在验证的阶段不需要post-process。不同于two-backbone的方法，不许哟啊额外的预训练，因为persondetector和action classifier共享一个backbone。</p></li><li><p><strong>Action Classification Head</strong>：给定有persondetector生成的 <span class="math inline">\(N\)</span>个actor proposalboxes。用RoIAlign来提取每个box的spatial和temporalfeatures，这两种类型的features然后融合，得到最终的action classprediction。细节如下：</p><ol type="1"><li><p><strong>Spatial Action Features</strong>：<spanclass="math inline">\(X_{5} \in \mathbb{R}^{C\times T\times H\timesW}\)</span>表示 res5得到的feature，在时间维度上进行一个<strong>globalaverage pooling</strong>，得到一个spatial feature map，<spanclass="math inline">\(f^{s} \in \mathbb{R}^{C\times1\times H\timesW}\)</span>，在 <span class="math inline">\(f^s\)</span> 上用RoIAlignwith <span class="math inline">\(N\)</span> 个actor proposals，得到<span class="math inline">\(N\)</span>个 spatial RoI features。<spanclass="math inline">\(f_{1}^{s},f_{2}^{s},\cdots,f_{N}^{s} \in\mathbb{R}^{C\times S\times S},\)</span>， <span class="math inline">\(S\times S\)</span>是 RoIAlign输出的spatial output size。</p></li><li><p><strong>Temporal Action Features</strong>：除了spatial actionfeatures，temporal properties也很重要，为了得到temporal motioninformation，从feature volume <spanclass="math inline">\(X_5\)</span>中的every frame提取temporalfeatures。因为这里主要关注temporal information，在spatialdimension上用一个global average pooling，来提取temporal RoIfeatures。temporal action feature表示为<spanclass="math inline">\(f_{1}^{t},f_{2}^{t},\cdots,f_{N}^{t}\in\mathbb{R}^{C\times T\times1\times1}\)</span>。</p></li><li><p><strong>Embedding Interaction</strong>：为了得到discriminativefeatures，为了增强instance的特性，引入了spatial 和temporal embedding tobe convolved with aforementioned spatial and temporal features。spatialembedding期望能够encoder spatial properties例如shape,pose等。temporalembedding能够encode temporal dynamicproperties，例如dynamics和action的temporalscale。注意到embedding是对于每个 <span class="math inline">\(N\)</span>features是exclusive的。定义 <span class="math inline">\(E^{s}\in\mathbb{R}^{N\times d},E^{t}\in \mathbb{R}^{N\times d}\)</span> forspatial and temporal embedding。<span class="math inline">\(E_n^s \in\mathbb{R}^d,E_n^t \in \mathbb{R}^d\)</span> are working for n-th RoIfeature。为了获得不同actors之间的relation 信息，构建了一个attentionmodule for all RoI features。因为每个actor RoI有自己的spatial和temporalembedding，embedding相比于featuremap更lighter，在对不同给的embedding之间而不是featuremaps之间采用attention mechanism for efficiency。这里给定一个queryelement和一系列key elements，多头注意力module能够根据attentionweights(measure compatibility of query-key pairs adaptively)来汇聚keycontents。最后，<span class="math inline">\(x = (x_,...,x_n)\)</span>表示 <span class="math inline">\(n\)</span>个input elements，输出 <spanclass="math inline">\(z= (z_1,...,z_n)\)</span>，<spanclass="math inline">\(z_i\)</span>是weighted sum of a linearlytransformed input：</p></li></ol><p><spanclass="math display">\[z_i=\sum_{j=1}^n\alpha_{ij}(x_jW^V).\]</span></p><p>weight coefficient <spanclass="math inline">\(\alpha_{ij}\)</span>是通过softmax计算出来的：<span class="math display">\[\alpha_{ij}=\frac{\expz_{ij}}{\sum_{k=1}^n\exp z_{ik}}, \text{where}z_ij=\frac{(x_iW^Q)(x_jW^K)^T}{\sqrt{d_z}}.\]</span> 将 <spanclass="math inline">\(E^s\)</span>和 <spanclass="math inline">\(E^t\)</span>送到 self-attentionmodule，得到对应的输出 <spanclass="math inline">\(\phi^s，\phi^t\)</span>，和 原始的embeddings <spanclass="math inline">\(E^s, E^t\)</span>有相同的shape，最后的actionfeature是： <spanclass="math display">\[f=\mathcal{G}(\mathcal{F}(f^s,\phi^s),\mathcal{F}(f^t,\phi^t)),\]</span></p><p><span class="math inline">\(F\)</span>是一个卷积操作 with parameters<span class="math inline">\(\phi\)</span>，<spanclass="math inline">\(G\)</span>是spatio-temporal fusion操作。初始化<span class="math inline">\(F\)</span> with <spanclass="math inline">\(1 \times 1\)</span> kernels for efficiency。</p><ol start="4" type="1"><li><strong>ObjectiveFunction</strong>：提出的model以端到端的方式解决了定位和分类，整个的目标函数由对应的两部分组成：<spanclass="math display">\[\mathcal{L}=\underbrace{\lambda_{cls}\cdot\mathcal{L}_{cls}+\lambda_{L1}\cdot\mathcal{L}_{L1}+\lambda_{giou}\cdot\mathcal{L}_{giou}}_{\text{setpredictionloss}}+\underbrace{\lambda_{act}\cdot\mathcal{L}_{act}}_{\text{action}}.\]</span></li></ol><p>第一部分是 <em>set prediciton loss</em>，produces an optimal<strong>bipartite matching between predictions and ground truthobjects</strong>。用 <span class="math inline">\(L_{cls}\)</span>表示cross-entropy loss over two classes(containing actor vs notcontaining actor)。<span class="math inline">\(L_{L1}\)</span> 和 <spanclass="math inline">\(L_{giou}\)</span>是box loss。<spanclass="math inline">\(\lambda_{cls},\lambda_{L1},\lambda_{giou}\)</span>是常量，平衡这些loss的contributions。对于第二部分， <spanclass="math inline">\(L_{act}\)</span>是一个binary cross entropy lossused for action classification，<spanclass="math inline">\(\lambda_{act}\)</span> 是对应的weight。</p></li></ul></li><li><p>实验：</p><ul><li>Spatial-temporal fusion：有不同的instantiations of fusing temporal和spatial action features：summation、concatenation和cross-attention(CA)，结果表明CA效果比另外两个好。</li></ul></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/1-Figure1-1.png"alt="Motivation of WOO" /><figcaption aria-hidden="true">Motivation of WOO</figcaption></figure><p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Motivation ofWOO. (a) Previous dominant video action detection methods usually adopttwo separate networks: an independent 2D detection model for actorlocalization from every key frames, and a 3D video model for actionclassification from video clips. (b) Our end-to-end unified frameworkuses a single backbone network to handle both 2D image detection and 3Dvideo classification (i.e.2D spatial dimensions plus a temporaldimension). This unified backbone only “watches” an input video once,and directly produces both actor localization and actionclassification</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/3-Figure2-1.png"alt="Comparison of Backbone" /><figcaption aria-hidden="true">Comparison of Backbone</figcaption></figure><p><span class="math inline">\(Figure \ 2^{[1]}\)</span>: Comparisons ofbackbone architecture. (a) Two separate backbones for actor localizationand action classification. Video backbone adopts res5 stage with dilatedconvolution (DC5). (b) A single union backbone which can providetask-specific features for actor localization and action classificationsimultaneously, enabling nearly cost-free feature extraction for actorlocalization compared to (a). Key frame features are illustrated inlight orange color. Here we purposely omit the res2 features for visualsimplicity</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/4-Figure3-1.png"alt="Action classification head" /><figcaption aria-hidden="true">Action classification head</figcaption></figure><p><span class="math inline">\(Figure \ 3^{[1]}\)</span>: Actionclassification head. Given the RoI feature of a specific box for Tframes, spatial and temporal action features are generated. Then,spatial and temporal embedding is used to make action featurerepresentation more discriminative through the interaction module.Finally, the multi-layer perceptron (MLP) takes as input the fusedspatial-temporal feature and predicts the action class logits. See textfor details</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/5-Figure4-1.png"alt="Structure of interaction module" /><figcaption aria-hidden="true">Structure of interactionmodule</figcaption></figure><p><span class="math inline">\(Figure \ 4^{[1]}\)</span>: Structure ofinteraction module. Here we plot spatial embedding interaction as anexample. ‘⊗’ denotes matrix multiplication, and ‘⊛’ denotes 1 × 1convolution</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;watch-only-oncean-end-to-end-video-action-detection-framework1&quot;&gt;Watch
Only Once：An End-to-end Video Action Detection
Framework&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自港大的罗平老师组的Shoufa Chen、Peize Sun、Enze
Xie等人。论文引用[1]:Chen, Shoufa et al. “Watch Only Once: An End-to-End
Video Action Detection Framework.” 2021 IEEE/CVF International
Conference on Computer Vision (ICCV) (2021): 8158-8167.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2021.Oct&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;end-to-end unified network&lt;/li&gt;
&lt;li&gt;task-specific features&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;提出了一个端到端的pipeline for video action
detection。&lt;strong&gt;当前的方法要么是将video action detection
这个任务解耦成action localization和action
classification这两个分离的阶段，要么在一个阶段里训练两个separated
models&lt;/strong&gt;。相比之下，作者的方法将actor localization和action
classification弄在了一个网络里。通过统一backbone网络，去掉很多认为的手工components，整个pipeline被简化了。&lt;strong&gt;WOO&lt;/strong&gt;用一个unified
video backbone来提取features for actor location 和action
localization,另外，引入了&lt;font color=red&gt;spatial-temporal action
embeddings&lt;/font&gt;，设计了一个 spatial-temporal fusion
module来得到更多的含有丰富信息的discriminative features，提升了action
classification的性能。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>YOWOv3</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/26/YOWOv3/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/26/YOWOv3/</id>
    <published>2024-08-26T06:35:34.000Z</published>
    <updated>2024-08-26T10:48:06.840Z</updated>
    
    <content type="html"><![CDATA[<h3id="yowov3-an-efficient-and-generalized-framework-for-human-action-detection-and-recognition1">YOWOv3:An Efficient and Generalized Framework for Human Action Detection andRecognition<sup>[1]</sup></h3><blockquote><p>作者是Nguyen Dang Duc Manh, Duong Viet Hang等人。论文引用[1]:Dang,Duc M et al. “YOWOv3: An Efficient and Generalized Framework for HumanAction Detection and Recognition.” (2024).</p></blockquote><h3 id="time">Time</h3><ul><li>2024.Aug</li></ul><h3 id="key-words">Key Words</h3><h3 id="总结">总结</h3><span id="more"></span><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/d02a8d30424422b99990cc394ca7d6526efc0a20/3-Figure2-1.png"alt="Structure" /><figcaption aria-hidden="true">Structure</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/d02a8d30424422b99990cc394ca7d6526efc0a20/4-Figure3-1.png"alt="CFAM" /><figcaption aria-hidden="true">CFAM</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;yowov3-an-efficient-and-generalized-framework-for-human-action-detection-and-recognition1&quot;&gt;YOWOv3:
An Efficient and Generalized Framework for Human Action Detection and
Recognition&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是Nguyen Dang Duc Manh, Duong Viet Hang等人。论文引用[1]:Dang,
Duc M et al. “YOWOv3: An Efficient and Generalized Framework for Human
Action Detection and Recognition.” (2024).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2024.Aug&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>YOWOv2</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/26/YOWOv2/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/26/YOWOv2/</id>
    <published>2024-08-26T06:35:29.000Z</published>
    <updated>2024-08-26T10:47:51.853Z</updated>
    
    <content type="html"><![CDATA[<h3id="yowov2-a-stronger-yet-efficient-multi-level-detection-framework-for-real-time-stad1">YOWOv2:A Stronger yet Efficient Multi-level Detection Framework for Real-timeSTAD<sup>[1]</sup></h3><blockquote><p>作者是来自哈工大的 Jianhuan Yang和Kun Dai，论文引用[1]:Yang, Jianhuaand Kun Dai. “YOWOv2: A Stronger yet Efficient Multi-level DetectionFramework for Real-time Spatio-temporal Action Detection.” ArXivabs/2302.06848 (2023): n. pag.</p></blockquote><h3 id="time">Time</h3><ul><li>2023.Feb</li></ul><h3 id="key-words">Key Words</h3><span id="more"></span><h3 id="总结">总结</h3><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/5f039c1410a764ad673dbb3336d360fad9e4e9e6/2-Figure1-1.png"alt="Structure" /><figcaption aria-hidden="true">Structure</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/5f039c1410a764ad673dbb3336d360fad9e4e9e6/3-Figure2-1.png"alt="channel encoder" /><figcaption aria-hidden="true">channel encoder</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/5f039c1410a764ad673dbb3336d360fad9e4e9e6/4-Figure3-1.png"alt="Coupled fusion head" /><figcaption aria-hidden="true">Coupled fusion head</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;yowov2-a-stronger-yet-efficient-multi-level-detection-framework-for-real-time-stad1&quot;&gt;YOWOv2:
A Stronger yet Efficient Multi-level Detection Framework for Real-time
STAD&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自哈工大的 Jianhuan Yang和Kun Dai，论文引用[1]:Yang, Jianhua
and Kun Dai. “YOWOv2: A Stronger yet Efficient Multi-level Detection
Framework for Real-time Spatio-temporal Action Detection.” ArXiv
abs/2302.06848 (2023): n. pag.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2023.Feb&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>TAAD</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/TAAD/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/TAAD/</id>
    <published>2024-08-24T08:53:52.000Z</published>
    <updated>2024-08-30T03:11:10.636Z</updated>
    
    <content type="html"><![CDATA[<h3id="spatio-temporal-action-detection-under-large-motion1">Spatio-TemporalAction Detection Under Large Motion<sup>[1]</sup></h3><blockquote><p>作者是来自ETHZ的Gurkirt Singh, Vasileios Choutas, Suman Saha, FisherYu和Luc Van Gool。论文引用[1]:Singh, Gurkirt et al. “Spatio-TemporalAction Detection Under Large Motion.” 2023 IEEE/CVF Winter Conference onApplications of Computer Vision (WACV) (2022): 5998-6007.</p></blockquote><h3 id="time">Time</h3><ul><li>2022.Oct</li></ul><h3 id="key-words">Key Words</h3><ul><li>track information for feature aggregation rather than tube fromproposals</li><li>3 motion categories: large motion、medium motion、small motion</li></ul><h3 id="总结">总结</h3><ol type="1"><li>当前的STAD的tubedetection的方法经常将一个给定的<strong>keyframe</strong>上的bboxproposal扩展成一个<strong>3D temporalcuboid</strong>，然后从邻近帧进行poolfeatures。如果actor的位置或者shape表现出了large 2D motion和variabilitythrough frames，这样的pooling不能够积累有意义的spaito-temporalfeatures。在这个工作中，作者旨在研究<strong>cuboid-aware featureaggregation in action detection under largeaction</strong>。进一步，提出了在<strong>largemotion</strong>的情况下，通过<strong>tracking actors和进行temporalfeature aggregation along the respective tracks</strong>增强actorfeature representation，定义了在不同的固定的time scales下的<strong>actormotion的IoU</strong>。有large motion的action会随着时间导致lowerIoU，slower actions会随着时间维持higher IoU。作者发现<strong>track-awarefeature aggregation持续地实现了很大的提升in actiondetection</strong>。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>目前的很多工作聚焦于通过复杂的上下文建模和更大的backbone，或者利用光流stream，来提升actiondetection的性能。上述方法用了cuboid-aware temporal pooling for featureaggregation。在这个工作中，旨在研究actioninstance的不同角度的情况下的cuboid-aware action detection。<strong>largeobject motion</strong>有不同的原因：例如fast camera motion，fastaction，body shape deformation due to pose change, or mixed camera andaction motions。所有的这些原因会造成sub-optimal featureaggregation，导致action classification的错误。</p></li><li><p>作者将actions分成了3类：largemotion、medium-motion和small-motion。这个区分是基于同一个actor overtime的boxes的IoU,可以用actors的ground truthtubes来计算。研究cuboid-aware的基线方法在不同motioncategories上的表现，不用context features或者long-termfeatures之类的。因为large-motion在很短的时间窗口中发生的很快。<strong>large-motioncases, IoU会比较小，因此一个3D cuboid-aware featureextractor不能获取actor location上的features throughtoutaction</strong>。为了解决large-motion，提出了随着时间来<strong>track theactor</strong>，用 <strong>Track-of-InterestAlign(TOI-Align)</strong>来extract features，resulting in <strong>TrackAware Action Detector(TAAD)</strong>。Further，研究了TOI-Alignedfeatures上的不同类型的 feature aggregation modules for TAAD网络。</p><p>为了这个目的，有以下的contributions：</p><ul><li>是第一个用evaluaiton metrics for each type ofmotion，来研究large-motion action detection。</li><li>提出了用 <strong>tube/track-aware feature aggregationmodules</strong>来处理largemotion，这种类型的module实现了很好的提升。</li></ul></li><li><p>Related work: Action recognition models提供了很强的videorepresentation models；action detection是一个在largemotion下的理解actions的问题。主要关注于STAD 问题，这里一个<strong>actioninstance被定义为随着时间的一系列的linked bboxes</strong>。最近的onlineactiondetection的方法的性能直追离线的STAD的方法。MultiSports数据集有更多的fine-grainedactionclass；有多个actors在同一个视频里执行不同类型的action。另外，MultiSports是一个<strong>denselyannotated</strong>。 很多的方法关注于keyframe，基于action detection onAVA，long-term feature banks起到了重要作用，获得了一些temporalcontext，但是没有actors之间的temporalassociations；另外也有人研究了actors和object之间的interactions。这些以上的方法都是用<strong>cuboid-awarepooling for local feature aggregation</strong>，作者们发现，当motion isquick和large，这不是最理想的方法。作者这里用SlowFast作为基线方法。</p><p><strong>Weinzaepfel</strong>是首个用<strong>tracking for actiondetection</strong>的工作，用一个tracker来解决tube generationpart的<strong>linking</strong>问题，给定tracks中的bboxesproposals，可以on a frame-by-frame basis做actionclassification。作者提出通过<strong>pooling features from within entiretracks</strong>来处理action detection。</p></li><li><p>Methodology：提出了处理large motions的方法，称之为Track AwareAction Detector(TAAD)，在视频中trackactor，同时，用了一个神经网络designed for videorecognition，来从每个clip中提取特征。用track boxes和video features，poolper-frame features with a RoI-Align operation，之后，<strong>TemporalFeature Aggregation module</strong>得到per-trackfeatures，计算单个feature vector，这里classifier预测最后的actionlabel。</p><ul><li><p><font color=red>Baseline ActionDetector</font>：选择Slowfast作为videobackbone，原因是它相对于大型的transformer模型仍然有竞争力 on the task ofSTAD；另外，Slowfast比其它的transformer的方法更高效。而且提供了不同temporalscales的features，有不同的temporalscales是重要的，特别是作者旨在处理fast/large motions,where a smallerscale是必须的；最后Slofast是MultiSports和UCF24datasets默认的backbone。用基于Slowfast的架构的pySlowFast with aResNet-50来执行baseline。首先，增加background frame,作为训练actiondetector的额外的negative samples。接下来，用一个multiclassclassifier来代替multi-label，switching from a binary cross entropy perclass to a cross entropy loss(CE-loss)。最后，加一个<strong>downward FPNblock</strong></p></li><li><p><font color=red>Tracker</font>：用YOLOv5-DeepSort的classagnostic版本作为tracker，这个是基于YOLOv5和TrochReID，fine-tuneYOlOv5的medium size作为detection model for personclasses。一个预训练的OsNet-x0-25被用作ReID模型。一个high recall、smallnumber of association的tracker对于提高action tubedetection的性能是重要的。<strong>微调detector</strong>也是重要的一步。<strong>Tracker可以被用作bboxesproposal filtering module</strong>,有时候detector产生多个high scoringdetections，有些会导致falsepositives,这些detections不和任何的tracks相匹配，因为它们时序上不是一致的。tracks产生的proposals能够在测试的时候用上。</p></li><li><p><font color=red>Temporal Feature Aggregation</font>：</p></li></ul><ol type="1"><li><strong>Track-of-Interest Align(TOI-Align)</strong>：SlowFast videobackbone处理输入的clip，产生 <span class="math inline">\(T \times H\times W\)</span> feature tensor，然而trackers返回一个 <spanclass="math inline">\(N_t \times T \times4\)</span>的array，包含物体附近的boxes。RoI-Align将这两个arrays作为输入，得到一个featurearray <span class="math inline">\(N_t \times T \times H \timesW\)</span>，one feature tube per track，在track的length小于inputclip的情况下，在时序上复制最后的available bbox。</li><li><strong>Featureaggregation</strong>：为了预测keyframe中的bbox的label，需要aggregatefeatures across time andspace。首先，在TOI-Align提取出来的features上，在空间维度上做一个averagepooling，然后执行Temporal Feature Aggregation的一个变体。<ul><li>Max-pooling over temporal axes(MaxPoo)</li><li>A sequence of temporal conv(TCN)</li><li>A temporal variant of Atrous Spatial PyramidPooling(ASPP)，修改Detectron2中的ASPP，用1D conv代替2D也尝试了ConvNeXt和VideoSwin的temporalversion，然而，这些导致不稳定的训练，即使调整学习率和其它的超参数，实验中，仅用了temporalconv for TCN module的一个layer。</li></ul></li></ol><ul><li><font color=red>Tube Construction</font>：<strong>Video-level tubedetection要求从per-frame detections中构建actiontubes。这个过程分为两步：首先将proposals连接起来形成tubehypotheses；然后trim这些proposals，得到有action的部分。可以将这两部视为trackingstep加上一个temporal action detection step</strong>。大多数的<strong>action tube detectionmethod</strong>在第一步用一个贪心的proposal连接算法；<strong>由于TAAD已经有tracks，所以不需要linkingstep</strong>。action tracks的temporal trimming是通过<strong>labelsmoothingoptimisation</strong>来做的，之前的很多工作都用到了，具体地，用了<strong>class-wisetemporal trimming</strong>。</li></ul></li><li><p>实验：</p></li><li><p>讨论和结论：在实验中发现，TAAD用<strong>tracking information forfeature aggregation，而不是从proposalboxes得到的tube</strong>,提高了性能，这不意味着没有了提高的空间，作者的方法对于<strong>tracker的性能比较敏感</strong>，因为这是pipeline的第一步。用更好的SOTA的tracker和persondetector，可以进一步提高性能。通过在<strong>TAAD中加上spatial/actorcontext modelling, long-term temporal context 或者一个transformer heador backbone</strong>，也能提高性能。 关于motion分类的定义可以说是不精确，不同于MS COCO中的object size类别，motion类别不容易定义。除了普遍的复杂的相机运动和quick actormotion外，必须特别注意错误标记。</p></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/978129084bd486157f3cc0997204554956a7cca3/4-Figure4-1.png"alt="Pipleline of TAAD" /><figcaption aria-hidden="true">Pipleline of TAAD</figcaption></figure><p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Proposed TrackAware Action Detector (TAAD): Given an input clip with T frames, weextract features using a video recognition network and Nt per-actortracks from a tracker. The TOI-Align operation extracts per-trackfeatures from the entire video sequence, using an RoI-Align operationand the track boxes, returning a <span class="math inline">\(N_t × T ×C\)</span> feature array. Next, the Temporal Feature Aggregation (TFA)module aggregates the features along the temporal dimension and passesthe resulting <span class="math inline">\(N_t × C\)</span> array to theaction classifier that predicts the action label.</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;spatio-temporal-action-detection-under-large-motion1&quot;&gt;Spatio-Temporal
Action Detection Under Large Motion&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自ETHZ的Gurkirt Singh, Vasileios Choutas, Suman Saha, Fisher
Yu和Luc Van Gool。论文引用[1]:Singh, Gurkirt et al. “Spatio-Temporal
Action Detection Under Large Motion.” 2023 IEEE/CVF Winter Conference on
Applications of Computer Vision (WACV) (2022): 5998-6007.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2022.Oct&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;track information for feature aggregation rather than tube from
proposals&lt;/li&gt;
&lt;li&gt;3 motion categories: large motion、medium motion、small motion&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;当前的STAD的tube
detection的方法经常将一个给定的&lt;strong&gt;keyframe&lt;/strong&gt;上的bbox
proposal扩展成一个&lt;strong&gt;3D temporal
cuboid&lt;/strong&gt;，然后从邻近帧进行pool
features。如果actor的位置或者shape表现出了large 2D motion和variability
through frames，这样的pooling不能够积累有意义的spaito-temporal
features。在这个工作中，作者旨在研究&lt;strong&gt;cuboid-aware feature
aggregation in action detection under large
action&lt;/strong&gt;。进一步，提出了在&lt;strong&gt;large
motion&lt;/strong&gt;的情况下，通过&lt;strong&gt;tracking actors和进行temporal
feature aggregation along the respective tracks&lt;/strong&gt;增强actor
feature representation，定义了在不同的固定的time scales下的&lt;strong&gt;actor
motion的IoU&lt;/strong&gt;。有large motion的action会随着时间导致lower
IoU，slower actions会随着时间维持higher IoU。作者发现&lt;strong&gt;track-aware
feature aggregation持续地实现了很大的提升in action
detection&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>TubeR</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/TubeR/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/TubeR/</id>
    <published>2024-08-24T08:53:46.000Z</published>
    <updated>2024-08-26T09:28:55.720Z</updated>
    
    <content type="html"><![CDATA[<h3 id="tuber-tubelet-transformer-for-video-action-detection1">TubeR:Tubelet Transformer for Video Action Detection<sup>[1]</sup></h3><blockquote><p>作者是来自阿姆斯特丹大学、罗格斯大学和AWS AI Labs的JiaojiaoZhao、Yanyi Zhang等人。论文引用[1]:Zhao, Jiaojiao et al. “TubeR: TubeletTransformer for Video Action Detection.” 2022 IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR) (2021): 13588-13597.</p></blockquote><h3 id="time">Time</h3><ul><li>2021.April</li></ul><h3 id="key-words">Key Words</h3><ul><li>tubelet queries, tubelet-attention</li><li>in sequence-to-sequence manner</li><li>scales well to longer video clips</li><li>end-to-end without person detectors, anchors or proposals</li></ul><h3 id="总结">总结</h3><ol type="1"><li>不同于现有的依赖于离线检测器或者人工设计的actor-positionalhypotheses like proposals or anchors，提出了一个通过同时进行actionlocalization和recognition from a singlerepresentation，直接检测视频里的actiontubelet的方法。TubeR学习一系列的tubelet queries，利用tubelet-attentionmodule来model video clip里的动态的spatio-tempralnature。相比于用actor-positional hypotheses in the spatio-temporalspace，它能够有效的强化模型的能力。对于包含transitional states或者scenechanges的视频，提出了一个context aware classificationhead，来利用short-term和long-term context to strengthen actionclassification，和一个action switch regression head来检测精确的时序上的行为范围。TubeR直接产生不同长度的actiontubelets，对于长的视频clips，也能保持一个比较好的结果。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>Actiondetection是一个复杂的任务，要求逐帧的任务的定位、将检测到的personinstances连接成actiontubes、预测action的类别。STAD中两种路径很流行：frame-level和video-level。<strong>frame-level的detection在每一帧上独立地进行检测和分类，然后将per-framedetections连接起来，形成连贯的actiontubes</strong>；为了弥补时序信息的缺失，一些方法简单地重复2D proposals或者离线的人物检测 over time，来得到时空特征。<strong>tubelet-leveldetection，直接生成spatio-temporal volumes from a videoclip，来获得连贯和动态的natures of actions。通常预测actionlocalization和classification jointly over spatio-temporalhypotheses</strong>。像 3D cuboid proposals。然而， 这些 3Dcuboids仅能够 capture a short period of time, when spatial location of aperson changes as soon as they move or due to cameramotion。Ideally，这些模型能够用灵活的spatio-temporal tubelets，能够trackthe person over a longer time。但是，large configuration space of such aparameterization限制了之前的方法只能用shortcuboids。这里，作者提出了一个tubelet-level的detectionapproach，<strong>能够同时定位和识别action tubelets in a flexiblemanner，使得tubelets能够随着时间改变size和location</strong>。这使得系统能够利用更长的tubelets，在更长的时间上汇聚人物和它们的行为的视觉信息。</p><p>从NLP中的sequence-to-sequence获得启发，特别是机器翻译和它在目标检测上的应用，DETR。DETR是一个frame-level的actiondetection。这里，用decoderqueries来表示整个视频序列上的人物和它们的行为，不限制tubelets是一个固定的cuboids。</p><p>提出了<font color=red>tubelet-transformer</font>：称之为<strong>TubeR for localizing and recognizing actions from a singlerepresentation</strong>。基于DETR的框架，TubeR学习一系列的 tubeletqueries，来从spatio-temporal video representation中pull action-specifictubelet-level features。TubeR的包括特别的 <strong>spatial and temporaltubelet attention</strong>，使得tubelets能够随着时间在它们的spatiallocation 和scale上没有限制。克服了之前对于cuboidsd限制。TubeR随着时间在一个 tubelet里 回归bboxes。考虑到tubelets之间的时序关联。汇聚visual features over thetubelet to classifyactions。这个涉及表现很好，但是并没有比用离线人物检测器的方法好很多。猜想是因为querybased features缺少全局上下文，only look at a singleperson的话，很难对涉及到的relationships 行为例如：listening-to或者talking-to进行分类。因此，提出了一个 <strong>contextaware classification head</strong>， along with the tubelet feature,利用完整的clipfeatures，分类头能够得出上下文信息。这个设计能够使得网络将persontubelet和完整的scene context(where tubeletappears)关联起来。这个设计的限制就是 *context feature仅能够从 tubelet占据的same clip中得到。包含long term contextualfeatures对于最后的行为分类很重要。因此受到要锁和存储tubelet附近的视频内容的contextual features的启发，引入了memorysystem。用相同的feature injection策略，将long term contextual memory给到分类头。</p><p>主要的贡献如下：</p><ol type="1"><li>提出来TubeR：一个tubelet-level的transformer 框架 for human actiondetection</li><li>tubelet query和attention basedformulation能够产生任意位置和尺寸的tubelets。</li><li>context aware classification head能够汇聚short-term和long-term上下文信息。</li></ol></li><li><p>相关工作：</p><ul><li><strong>frame-level action detection</strong>：用2D positionalhypothese(anchors) 或者离线的person detector on a keyframe来定位actors，然后更多地关注提高actionrecognition。通过利用光流分支来包含temporal patterns。其它的用 3DconvNet来获取时序信息来识别行为。不同于frame-level的方法，面向tubelet-levelvideo actiondetection，用一个统一的configuration，来进行定位和识别。</li><li><strong>Tubelet-level action detection</strong>：通过将tubelet作为一个representation unit来 detect actions变得流行了，有人重复 2Danchors per-frame来pooling ROI features，然后stack frame-wisefeatures来预测行为类别。有人依赖严格设计的 3D cuboidproposal，前者直接俄检测tubelets，后者逐步第refines 3D cuboid proposalsacross time。除了box/cuboid anchors，也有人通过centerposition假设，来检测tubeletinstances。基于假设的方法来处理长视频clips有很多困难。通过学习tubeletqueries的子集，来表示tubelets的动态nature。将action detection taskreformulate程一个 sequence-to-sequence学习的问题，在一个tubelet里显式地model temporal correlations。</li><li><strong>Transformer-based action detection</strong>：Girdhar提出来一个video action transformer network for detectingactions。用region-proposal-network forlocalization,通过汇聚actors附近的时空上下文信息，提高actionrecognition.</li></ul></li><li><p>TubeR：TubeR的输入是一个video clip，直接输出一个<strong>tubelet：a sequence of bboxes and the actionlabel</strong>，TubeR是受 DETR的启发，但是将transformer架构 reformulatefor sequence-to-sequence modeling in video。 给定一个video clip $IR^{T_{in} H W C} <span class="math inline">\(，\)</span>T_{in}, H, W,C$分别表示帧数，height,width和channel， TubeR首先用一个 3Dbackbone来提取 video feature <spanclass="math inline">\(F_{\mathrm{b}}\in\mathbb{R}^{T^{\prime}\timesH^{\prime}\times W^{\prime}\times C^{\prime}}\)</span>，<spanclass="math inline">\(T&#39;\)</span> 表示 temporal dimension， <spanclass="math inline">\(C&#39;\)</span>是 featuredimension。用一个transformer的encoder-decoder来transform视频的特征 intoa set of tubelet-specific feature <spanclass="math inline">\(F_{\mathrm{tub}}\in\mathbb{R}^{N\timesT_{\mathrm{out}}\times C^{\prime}}\)</span>，<spanclass="math inline">\(N\)</span>是 tubelets的数量。为了处理长的videoclips，用temporal 下采样，使得 $ T_{out} &lt; T' &lt; T_{in}$，减小memory的 requirement。TubeR 产生了稀疏的sparse，对于短的videoclips，去掉时序下采样，使得 <span class="math inline">\(T_{out} &lt;T&#39; &lt; T_{in}\)</span>，results in dense tubelets。Tubelet回归和associated action classification能够用一个separated task heads同时实现：</p><p><spanclass="math display">\[y_{\mathrm{coor}}=f(F_{\mathrm{tub}});y_{\mathrm{class}}=g(F_{\mathrm{tub}}),\]</span></p><p><span class="math inline">\(f\)</span>表示 tubelet 回归头， <spanclass="math inline">\(y_{coor} \in \mathbb{R}^{N\timesT_{\mathrm{out}}\times4}\)</span> 表示<spanclass="math inline">\(N\)</span>个 tubelets的坐标，each of which isacross <span class="math inline">\(T_{out}\)</span> frames(或者 <spanclass="math inline">\(T_{out}\)</span> sampled frames for longclips)。<span class="math inline">\(g\)</span>表示行为分类头，<spanclass="math inline">\(y_{\mathrm{class}}\in\mathbb{R}^{N\timesL}\)</span>表示 <span class="math inline">\(N\)</span>个 tubelets with<span class="math inline">\(L\)</span> 个可能的labels的行为分类。</p><ul><li><font color=red>TubeREncoder</font>：不同于普通的transformer的encoder，TubeR encoder用于处理3D 时空中的信息。每个 encoder layer由 self-attentionlayer(SA)、两个normalization layer和一个 FFN组成。core attentionlayers的公式如下：</li></ul><p><spanclass="math display">\[F_\mathrm{en}=\mathrm{Encoder}(F_\mathrm{b}),\]</span><spanclass="math display">\[\mathrm{SA}(F_\mathrm{b})=\mathrm{softmax}(\frac{\sigma_q(F_\mathrm{b})\times\sigma_k(F_\mathrm{b})^T}{\sqrt{C^{\prime}}})\times\sigma_v(F_\mathrm{b}),\]</span><spanclass="math display">\[\sigma(*)=\mathrm{Linear}(*)+\mathrm{Emb}_{\mathrm{pos}},\]</span></p><p><span class="math inline">\(F_b\)</span>是 backbone feature,<spanclass="math inline">\(F_{en} \in R^{T&#39;H&#39;W&#39; \timesC&#39;}\)</span>，<span class="math inline">\(C&#39;\)</span>表示dimensional encoded feature embedding。<spanclass="math inline">\(\sigma(*)\)</span> 是线性变换加上 positionalembedding。<span class="math inline">\(Emb_{pos}\)</span>是 3Dpositional embedding。optional temporal down-sampling 能够用在backbonefeatures上，来shrink 输入的sequence length to transformer for bettermemory efficiency。</p><ul><li><p><font color=red>TubeR Decoder</font>：</p><ul><li><p><strong>tubelet query</strong>：基于anchor假设来直接检测tubelets是相当有挑战的。tubelet space along thespatio-temporal dimension相比于single frame bbox space来说是巨大的。考虑FasterRCNN，requires for each position in a feature map with spatialsize <span class="math inline">\(H^{\prime}\times W^{\prime},K(=9)\)</span> anchors，总共有 <spanclass="math inline">\(KH&#39;W&#39;\)</span>个anchors。对于一个across<span class="math inline">\(T_{out}\)</span> frames的tubelet来说，需要<spanclass="math inline">\(KH&#39;W&#39;^{T_{out}}\)</span>个anchors，来保持同样的samplingin space-time。为了减小tubelet space，一些方法通过忽略短的videoclip中的action的空间位移，用 3Dcuboids来近似tubelets。然而，视频clip越长， 3Dcuboids代表的tubelet的精度越低。提出了学习tubelet queries小的子集，<span class="math inline">\(Q{=}\{Q_{1},...,Q_{N}\}\)</span>, <spanclass="math inline">\(N\)</span>是queries的数量。 第 <spanclass="math inline">\(i\)</span>个 tubelet query <spanclass="math inline">\(Q_{i}=\{q_{i,1},...,q_{i,T_{\mathrm{out}}}\}\)</span>包含 <span class="math inline">\(T_{out}\)</span> box query embeddings<span class="math inline">\(q_{i,t} \in R^{C&#39;}\)</span> across <spanclass="math inline">\(T_{out}\)</span> frames。学习一个tubeletquery表示dynamics of a tubelet, 而不是手工设计的 3D anchors。初始化boxembeddings identically for a tubelet query。</p></li><li><p><strong>tubelet attention</strong>：为了model tubeletqueries内的relations，提出来一个 tubelet-attention (TA)module，包含连个self-attention layers。首先有一个 <em>spatialself-attention layer</em>来处理一帧内的box queryembeddings的空间relations。这个layer的intuition是识别actions 受益于interactions between actors，或者between actos and objects in the sameframe。接下来有 <strong>temporal self-attentin layer</strong>来models同一个tubelet里的box query embeddings acrosstiem的correlations。这一层促使 TubeR query 去trackactors，然后产生action tubelets，聚焦于single actors而不是一个fixed areain the frame。TubeR decoder把tubelet attention module用在了 tubeletqueries <span class="math inline">\(Q\)</span>上，来产生 tubelet queryfeature <spanclass="math inline">\(F_{\mathfrak{a}}\in\mathbb{R}^{N\timesT_{\mathrm{out}}\times C^{\prime}}\)</span>：</p><p><span class="math inline">\(F_q = TA(Q)\)</span></p></li><li><p><strong>Decoder</strong>：decoder包含一个 tubele-attentionmodule和一个 cross-attention (CA) layer，用来decode tubelet-specificfeature <span class="math inline">\(F_{tub}\)</span> from <spanclass="math inline">\(F_{en}\)</span> to <spanclass="math inline">\(F_q\)</span>：</p></li></ul><p><spanclass="math display">\[\mathrm{CA}(F_{q},F_{\mathrm{en}})=\mathrm{softmax}(\frac{F_{q}\times\sigma_{k}(F_{\mathrm{en}})^{T}}{\sqrt{C^{\prime}}})\times\sigma_{v}(F_{\mathrm{en}}),\\F_{\mathrm{tub}}=\mathrm{Decoder}(F_{q},F_{\mathrm{en}}).\]</span></p><p><span class="math inline">\(F_\mathrm{tub}\in\mathbb{R}^{N\timesT_\mathrm{out}\times C^{\prime}}\)</span> 是tubelet specificfeatures。有temporal pooling的时候，<span class="math inline">\(T_{out}&lt; T_{in}\)</span>，TubeR产生 <strong>sparse tubelets</strong>，对于<span class="math inline">\(T_{out} = T{in}\)</span>，TubeR产生 densetubelets。</p></li></ul><p><font color=red>Task-Specific Heads</font>：对于每个tubelet，bbox和action classification可以用独立的task-specificheads来处理。这样的设计最大化的减小了计算量。</p><ul><li><strong>Context aware classificationhead</strong>：这个分类用一个简单的linear project就能实现。 <spanclass="math display">\[y_{\mathrm{class}}=\mathrm{Linear_c}(F_{\mathrm{tub}}),\]</span></li></ul><p><span class="math inline">\(y_{class} \in R^{N \timesL}\)</span>表示在 <spanclass="math inline">\(L\)</span>个可能的label上的分类的分数。one foreach tubelet。</p><ol type="1"><li><em>Short-term context head</em>:对于理解sequences，context是重要的。进一步提出利用spatio-temporal videocontext来帮助理解sequence。query the action specific feature <spanclass="math inline">\(F_{tub}\)</span> from some context feature <spanclass="math inline">\(F_{context}\)</span> to strengthen <spanclass="math inline">\(F_{tub}\)</span>，得到了 feature <spanclass="math inline">\(F_{c}\in R^{N \timesC&#39;}\)</span>，用于最后的分类： <spanclass="math display">\[F_\text{c}=\text{CA}(\text{Pool}_t(F_\text{tub}),\text{SA}(F_\text{context}))+\text{Pool}_t(F_\text{tub}).Eq.9\]</span></li></ol><p>这里设置 <span class="math inline">\(F_{context} = F_b\)</span> forutilizing the short-term context in the backbone feature。称之为<strong>Short-term context head</strong>，<spanclass="math inline">\(F_{context}\)</span> 首先用一个自注意力层，然后cross-attenion layer utilizes <spanclass="math inline">\(F_{tub}\)</span> to query from <spanclass="math inline">\(F_{context}\)</span>。<spanclass="math inline">\(F_{c}\)</span>经过线性层，用于最后的分类。</p><ol start="2" type="1"><li><em>Long-term context head</em>：为了利用long-range的时序信息，但是在有限的memory下，采用两阶段的decoder for long-termcontext compression。</li></ol><p><spanclass="math display">\[\mathrm{Emb}_{\mathrm{long}}=\mathrm{Decoder}(\mathrm{Emn}_{n1},\mathrm{Decoder}(\mathrm{Emb}_{n0},F_{\mathrm{long}}).\]</span></p><p>long-term context <spanclass="math inline">\(F_{\mathrm{long}}\quad\in\quad\mathbb{R}^{T_{\mathrm{long}}\timesH^{\prime}W^{\prime}\times C^{\prime}}\)</span> 是一个buffer，包含从<spanclass="math inline">\(2W\)</span>个在时间上concatenated的相邻的clips抽取出来的backbonefeature。为了将long-term video feature buffer压缩到 embedding <spanclass="math inline">\(Emb_{long}\)</span> with a lower temporaldimension，用了两个 stacked decoders with token <spanclass="math inline">\(Emn_{n0}\)</span> 和 <spanclass="math inline">\(Emn_{n1}\)</span>。首先用一个压缩的token <spanclass="math inline">\(Emb_{n0} (n0 &lt; T_{long})\)</span> to query<spanclass="math inline">\(F_{long}\)</span>中重要的信息，得到一个temporaldimension 为 <spanclass="math inline">\(n0\)</span>的中间压缩embedding。然后，进一步利用另外一个压缩的token<span class="math inline">\(Emb_{n1} (n1 &lt; n0)\)</span> to query from中间压缩的embedding，然后得到最后的压缩embedding <spanclass="math inline">\(Emb_{long}\)</span>。<spanclass="math inline">\(Emb_{long}\)</span>包含long-term的视频信息，但是有着 lower temporal dimension <spanclass="math inline">\(n1\)</span>，然后，对 <spanclass="math inline">\(F_b\)</span>和 <spanclass="math inline">\(Emb_{long}\)</span>采用cross-attentionlayer，来得到long-term context feature <spanclass="math inline">\(F_{\mathrm{lt}}\in\mathbb{R}^{T^{\prime}\timesH^{\prime}\times\bar{W}^{\prime}\times C^{\prime}}\)</span>：</p><p><spanclass="math display">\[F_{\mathrm{lt}}=\mathrm{CA}(F_{\mathrm{b}},\mathrm{Emb}_{\mathrm{long}}),\]</span></p><p>设置 <span class="math inline">\(F_{context} = F_{lt} inEq.9\)</span>，来利用 long-term context for classification。</p><p><font color=red>Action Switch regression head</font> <spanclass="math inline">\(T_{out}\)</span> bboxes in a tubelet是用一个 FClayer同时进行回归。</p><p><spanclass="math display">\[y_{\mathrm{coor}}=\mathrm{Linear}_{\mathrm{b}}(F_{\mathrm{tub}}),\]</span></p><p><span class="math inline">\(y_{\mathrm{coor}}\in\mathbb{R}^{N\timesT_{\mathrm{out}}\times4}\)</span>， <spanclass="math inline">\(N\)</span>是 action tubelet的数量， <spanclass="math inline">\(T_{out}\)</span>是一个action tubelet的temporallength。为了去掉tubelet里的non-action boxes。进一步用 FC layer来决定 abox 是否描述了tubelet里actor的行为。称之为 action switch。这个actionswitch 使得能够产生action tubelets witha more precise temporalextent。<span class="math inline">\(T_{out}\)</span> predicted boxes ina tubelet的概率是： <spanclass="math display">\[y_\mathrm{switch}=\mathrm{Linear}_\mathrm{s}(F_\mathrm{tub}),\]</span></p><p><span class="math inline">\(y_\mathrm{switch}\in\mathbb{R}^{N\timesT_\mathrm{out}}\)</span>，对于每个预测的tubelet, each of its <spanclass="math inline">\(T_{out}\)</span> bboxes 包含一个action switchscore。</p><p><font color=red> Losses</font>：4个loss的线性组合是： <spanclass="math display">\[\mathcal{L}=\lambda_{1}\mathcal{L}_{\mathrm{switch}}(y_{\mathrm{switch}},Y_{\mathrm{switch}})+\lambda_{2}\mathcal{L}_{\mathrm{class}}(y_{\mathrm{class}},Y_{\mathrm{class}})\\+\lambda_{3}\mathcal{L}_{\mathrm{box}}(y_{\mathrm{coor}},Y_{\mathrm{coor}})+\lambda_{4}\mathcal{L}_{\mathrm{iou}}(y_{\mathrm{coor}},Y_{\mathrm{coor}}),\]</span></p><p><span class="math inline">\(y\)</span>是模型的输出，<spanclass="math inline">\(Y\)</span>表示ground truth，action switch loss<span class="math inline">\(L_{switch}\)</span> 是一个binarycross-entropy loss，<span class="math inline">\(L_{class}\)</span> crossentropy loss，<span class="math inline">\(L_{box}\)</span> 和 <spanclass="math inline">\(L_{iou}\)</span> 表示per-frame bboxes matchingerror。当 <span class="math inline">\(T_{out} &lt; T_{in}\)</span>，tubelet是sparse，coordinate ground truth <spanclass="math inline">\(Y_{coor}\)</span>是来自对应的时序下采样的framesequence。用 匈牙利匹配，根据经验，设置参数 <spanclass="math inline">\(\lambda_{1}=1, \lambda_{2}=5, \lambda_{3}=2,\lambda_{4}=2\)</span>。</p></li><li><p>消融实验：</p><ul><li><strong>benefit of tubelet queries</strong>：在实验中发现了tubeletquery sets的好处，每个query set是由 <spanclass="math inline">\(T_{out}\)</span> per-frame query embeddings组成，能够在各自的frame上预测spatial location of the action。将其和single query embedding which represents a whole tubelet and must regress<span class="math inline">\(T_{out}\)</span> box locations for allframes in the clip.进行对比。结果是要好一些，证明了modeling actiondetection as a sequence-to-sequencetask，能够有效地利用transformer的架构。</li><li><strong>effect of tubelet attention</strong>：tubeletattention相比于典型的self-attention，能够节省memory。</li><li><strong>benefic of action switch</strong>：actionswitch能够精确地判断action的temporal start and end。没有actionswitch，TubeR会将transitional states误分类为actions。</li><li><strong>effect of short and long term contexthead</strong>：在AVA数据集上有很好的性能提升，网络能够<strong>seeingfull context of the clip</strong>。</li></ul></li><li><p>局限：</p><ul><li>3D backbone会占用很大的memory和计算量，限制了在长视频上应用TubeR。近期的工作是将transformer的encoder用于videoembedding，会占用较少的memory。</li><li>如果in one pass处理一个长视频，需要足够的queries来cover视频中per-person的不同的最多的行为数量。这会造成在自注意力层中，需要大量的queries，造成memory问题。一个可能的解决方法是产生persontubelets而不是 actiontubelets。因此当一个新的action发生的时候，不需要splittubelets。对于每个person instance，只需要一个query。</li></ul></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/e36c35f3b3d898898a1b63817ce70397555cef76/1-Figure1-1.png"alt="tube" /><figcaption aria-hidden="true">tube</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/e36c35f3b3d898898a1b63817ce70397555cef76/3-Figure2-1.png"alt="structure of TubeR" /><figcaption aria-hidden="true">structure of TubeR</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;tuber-tubelet-transformer-for-video-action-detection1&quot;&gt;TubeR:
Tubelet Transformer for Video Action Detection&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自阿姆斯特丹大学、罗格斯大学和AWS AI Labs的Jiaojiao
Zhao、Yanyi Zhang等人。论文引用[1]:Zhao, Jiaojiao et al. “TubeR: Tubelet
Transformer for Video Action Detection.” 2022 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (2021): 13588-13597.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2021.April&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;tubelet queries, tubelet-attention&lt;/li&gt;
&lt;li&gt;in sequence-to-sequence manner&lt;/li&gt;
&lt;li&gt;scales well to longer video clips&lt;/li&gt;
&lt;li&gt;end-to-end without person detectors, anchors or proposals&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;不同于现有的依赖于离线检测器或者人工设计的actor-positional
hypotheses like proposals or anchors，提出了一个通过同时进行action
localization和recognition from a single
representation，直接检测视频里的action
tubelet的方法。TubeR学习一系列的tubelet queries，利用tubelet-attention
module来model video clip里的动态的spatio-tempral
nature。相比于用actor-positional hypotheses in the spatio-temporal
space，它能够有效的强化模型的能力。对于包含transitional states或者scene
changes的视频，提出了一个context aware classification
head，来利用short-term和long-term context to strengthen action
classification，和一个action switch regression head
来检测精确的时序上的行为范围。TubeR直接产生不同长度的action
tubelets，对于长的视频clips，也能保持一个比较好的结果。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>EVAD</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/EVAD/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/EVAD/</id>
    <published>2024-08-24T08:51:19.000Z</published>
    <updated>2024-08-27T07:18:15.529Z</updated>
    
    <content type="html"><![CDATA[<h3id="efficient-video-action-detection-with-token-dropout-and-context-refinement1">EfficientVideo Action Detection with Token Dropout and ContextRefinement<sup>[1]</sup></h3><blockquote><p>作者是来自nju、蚂蚁集团、复旦和上海AI Lab的Lei Chen、ZhanTong、Yibing Song等人。论文引用[1]:Chen, Lei et al. “Efficient VideoAction Detection with Token Dropout and Context Refinement.” 2023IEEE/CVF International Conference on Computer Vision (ICCV) (2023):10354-10365.</p></blockquote><h3 id="time">Time</h3><ul><li>2023.Aug</li></ul><h3 id="key-words">Key Words</h3><ul><li>spatiotemporal token dropout</li><li>maintain all tokens in keyframe representing scene context</li><li>select tokens from other frames representing actor motions</li><li>drop out irrelavant tokens.</li></ul><h3 id="总结">总结</h3><ol type="1"><li>视频流clips with large-scale vieo tokens 阻止了ViTs for efficientrecognition，特别是在video actiondetection领域，这是需要大量的时空representations来精确地actoridentification。这篇工作，提出了端到端的框架 <strong>for efficient videoaction detection(EVAD) based on vanillaViTs</strong>。EVAD包含两个为视频行为检测的特殊设计。首先：提出来时空tokendropout from a keyframe-centric perspective. 在一个video clip中，mainall tokens from its keyframe，保留其它帧中和actormotions相关的tokens。第二：通过利用剩余的tokens，refine scene contextfor better recognizing actor identities。actiondetector中的RoI扩展到时间域。获得的时空actor identity representationsare refined via scene context in a decoder with the attentionmechanism。这两个设计使得EVAD高效的同时保持精度。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>image patches视为ViT输入的tokens for自注意力的计算。当识别一个video clip的时候，tokens来自每个frame，形成大规模的input for ViTs，这些videotokens会在训练和推理的时候引入很多计算，特别是在计算自注意力的时候。有一些尝试来减少visiontokens for fast computations。对于video action detection任务来说，平衡精度和效率是个挑战。这是因为在VAD中，需要定位每一帧中的actors，视频序列中的temporalmotion需要保持 for consistent identification。同时，scene context应该被保留以区别其它actors。大量的表示actor motions和scenecontext的video tokens会保持VAD的精度。这篇文章中，保留表示actormotions和scene context的video tokens，同时dropping out不相关的tokens。<strong>基于视频clips的时序连贯性，从keyframe-centric的角度，提出来时空tokendropout</strong>。<strong>对于每个视频clip,选择代表scenecontext的keyframe，这里的所有tokens将被maintained。同时，从其它代表actormotions的帧中选择tokens。另外，drop out 这个clip中的剩余的videotokens</strong>。</p><p><font color=red>通过一个keyframe-centric token pruning module withthe ViT encoder backbone，实现spatiotemporal tokendropout。这个keyframe要么是均匀采样得到的，要么人为在videoclips中定义的。默认是选择input clip中带有box annotations的middleframe</font>。另外，提取由这个keyframe增强的attention map。这个attentionmap在non-keyframe中知道tokedropout。在定位之后，需要分类每个定位好的bbox for actoridentification。当video tokens在non-keyframes中是不完整的时候，分类的性能在bbox regions (where tokenshave been droppedout)中是较差的。然而，视频的内在时序一致性使得能够refine bothactor和scene context from the remaining videotokens。扩展时域上的localized bboxes for RoIAlign，来获得与actormotion相关的tokenfeatures。然后，引入一个decoder，通过这个clip中的剩余videotokens的指导，来refine actor features。这个decoder concatenatesactor和token features，进行自注意力操作来产生丰富的actor features forbetter identification。在token dropout之后，退化的actionclassification能够用剩余的video tokens for contextrefinement来恢复。这个恢复的性能和用整个video tokens for actionclassification是一样的。通过这个context refinement，用减少了的videotokens保持了VAD的性能。</p></li><li><p>相关工作:</p><ul><li><strong>Spatio-temporal ActionDetection</strong>：当前的SOTA方法采用两个分开的backbones、2个stagespipelines。例如2D backbone for actor localization on keyframes，3Dbackbone for video featureextraction.之前的方法通过端到端的方式训练这两个backbone，来简化pipeline。这个会导致很高的复杂度和优化困难。最近的方法用统一的一个backbone来进行actiondetection。 VAT是一个transformer风格的actiondetector，来汇聚目标actor附近的时空上下文。WOO和TubeR是query-based的actiondetectors，来预测actor bboxes和actionclasses。STMixer是一个但阶段的query-based detector，来adaptively samplediscriminativefeatures。几个新的基于transformer的方法，用ViT的变种backbone，用两阶段的pipeline得到了一个比较好的结果。</li><li><strong>Spatio-temporal Redundancy</strong>：<ol type="1"><li>SpatialRedundancy：<strong>DynamicViT</strong>观察到，精确的图像识别是基于最富有信息的tokens的子集，设计了一个dynamictokensparsification的框架，来剪掉多余的tokens。<strong>EViT</strong>计算了classtoken to each token的attentiveness，确定了top-k tokens using theattentiveness value。<strong>ATS</strong>引入了一个differentiableadaptive token sampler for adaptively sampling 重要的tokens based onimage content。</li><li>Spatio-temporalredundancy：由于视频数据集的高冗余性，有很多的研究关注于开发高效的视频识别。最近的方法展示了高效的schemesspecialized for videotransformers。<strong>MAR</strong>提出了一个人为设计的masking策略，来丢弃 部分patches，减少videotokens。<strong>K-centered</strong>提出了patch-basedsampling，超越了传统的frame-basedsampling。<strong>STTS</strong>用transformer在时间和空间上sequentially选择tokens。</li></ol>为了和这些方法对比，考虑keyframe和邻近帧的correlation,来drop out冗余的tokens，然后执行端到端的基于transformer的action detector。</li></ul></li><li><p>Method：EVAD使encoder with token pruning来去掉多余的tokens，decoder来refine actor spatiotemporalfeatures。和<strong>WOO</strong>设置相同，利用encoder中的keyframe的多个中间的spatialfeature maps for actor localization，最后一个encoderlayer输出的spatiotemporal feature map 用来做action classification。</p><ul><li><strong>Keyframe-centric TokenPruning</strong>：相邻帧有着相似语义信息的video是高度冗余的，使得用一个比较高的dropoutrate on video transformers来做tokenpruning是可能的。这里，将所有的spatiotemporal tokens分成keyframetokens和non-keyframe tokens。keyframe中，保留所有的tokens for accurateactor localization。</li></ul><ol type="1"><li><font color=red>Non-keyframe token pruning</font>：用<strong>EViT</strong>中的方法，预先计算一个attentionmap来表示每个token的重要性，不需要额外的learnableparameters和计算开销。首先，average attentionmap的<em>num_heads</em>维度，来得到一个 <span class="math inline">\(N\times N\)</span>的matrix，表示tokens之间的<strong>attentiveness</strong>。因为没有classification token forvideo-level recognition，所以计算每个token的平均重要性分数 <spanclass="math inline">\(I_{j}=\frac{1}{N}\sum_{i=1}^{N}attn(i,j)\)</span>。然后从<span class="math inline">\(N_2\)</span> non-keyframetokens中，通过重要性分数的降序排列，找到 top(<spanclass="math inline">\(N \times \rho - N_1\)</span>) tokens，这里<spanclass="math inline">\(N, N_1, N_2\)</span>分别表示所有的tokens、keyframetokens和non-keyframe tokens。<span class="math inline">\(\rho\)</span>表示 token keepingrate，通常情况下，keyframe包含当前样本的最精确的语义信息，其它帧会带来信息bias。guidedby keyframe进行token pruning是符合实际的。为了这个目的，在acquiringattention map和计算non-keyframe token重要之间插入一个 <strong>KeyframeAttentiveness Enhancement</strong> step。用一个greater weight value tokeyframe queries，保留和keyframetokens高关联的tokens。每个token的重要性分数是这样更新的：</li></ol><p><span class="math display">\[I_j=\frac{1}{N}\sum_{i=1}^N\begin{cases} w_{tf} \cdot attn(i,j),&amp;i \in(0,N_1)\\\\attn(i,j),&amp;i \in (N_1,N)\end{cases}\]</span></p><p>假设first <span class="math inline">\(N_1\)</span>tokens属于keyframe，weigth value <spanclass="math inline">\(w_{kf}\)</span>是一个超参数。丢掉仅和non-keyframes 有highresponse的tokens，这些可能不是高质量的tokens。仅当non-keyframe变成了previousor next samples的keyframe的时候，这些highly responsivetokens才是高质量的。通过dropout这些冗余的tokens，进一步减少tokens的数量；做完tokenpruning之后，将这些保留的tokens给到之后的FFN。</p><p>第一次token pruning是在encoder layers 的1/3开始，之后，每个 1/4的total layers进行一次 token pruning，丢弃冗余的tokens，保留effectiveones。</p><ul><li><strong>Video Action Detection</strong>：</li></ul><ol type="1"><li><p><font color=red>Actor localization branch</font> 得到了keyframetokens，就能得到多个完整的keyframe feature maps。然后对这些featuremaps进行上采样或者下采样，来从palin ViT中产生hierarchicalfeatures。受Sparse RCNN的启发，引入了query-based actor localizationhead，来检测keyframe中的actors。actor localization branch的输出是 <spanclass="math inline">\(n\)</span>个 prediction boxes in thekeyframe和对应的actor confidence scores。</p></li><li><p><font color=red>Action Classification branch</font>不同于传统的特征提取，EVAD产生<spanclass="math inline">\(M\)</span>个离散的video tokens。需要恢复videofeature map的时空结构，然后进行location-related操作例如RoIAlign。初始化blank feature map shape as <spanclass="math inline">\((T/2,H/16,W/16)\)</span>，用保留的tokens来填充这个featuremap according to 它们对应的时空位置，剩余的用0进行pad。</p></li></ol><p>然后，用localization branch产生的boxes，通过3D RoIAlign来提取actorRoI features for subsequent action prediction。由于actormovement或者相机的变化，actor的空间位置是逐帧变化的，用keyframe box for3D RoIAlign不能得到partial actor feature deviated from thebox。直接扩展scope of the box来cover整个motiontrajectory，可能会引入背景或者其它的干扰信息，对actor featurerepresentation不利。然而，在EVAD featureextraction阶段，<strong>视频中的干扰项会被逐步去掉，因此可以扩展scope ofbox来增加deviated feature</strong>. <strong>观察发现：</strong>直接用vision classification的token pruning方法能减少计算中的tokens的数量，但是会对最后的检测性能有负面的影响。Videoaction detection要localizing和classifying actions of allactors，但是token pruning算法会导致时空上不连贯的actorfeatures**。在encoder中，pair-wise self-attention能够modeling globaldependency among tokens，actor regions内的dropouttokens的语义信息能够被incorporated into some preservedtokens，因此能够从保留的video tokens中恢复去掉的actorfeatures。为了这个目的，设计了一个<font color=red>context refinementdecoder</font>来refine actor的spatiotemporalrepresentation。具体地说：concatenate <spanclass="math inline">\(M\)</span>个video tokens的 <spanclass="math inline">\(n\)</span>个actor RoIfeatures，然后送到deocder中，guiding by 保留的tokens，actor features canenrich themselves with actor represntation and motion information fromother frames。没有token pruning，decoder会被用来作为relational modelingmodules，来得到inter-actor和actor-context的interaction information。</p><p>decoder输出的<span class="math inline">\(n\)</span>个refined actorfeatures are retrieved，然后通过一个classification layer，做最后的actionprediction。</p></li><li><p>实验：</p><ul><li><strong>RoI extension</strong>：由于人的largemotion，从keyframe中得到的box不能cover整个motiontrajectory，Intuitively，通过适当地扩展box scope来解决这个问题。pruningmechanism能够消除extension带来的interference information。结合了RoIextension的pruning能够在一定程度上消除human movements的影响。</li><li>EVAD能够实现实时的推理in an end-to-end manner。</li><li>之前流行的model是VideoMAE，是一个两阶段的model，需要一个离线的persondetector来pre-compute personproposals。现在EVAD，用同样的预训练的backbone，能够获得和VideoMAE相当的性能。和其它端到端的模型例如<strong>WOO</strong>和<strong>TubeR</strong>，用明显的性能提升；比基于CNN的model有更快的推理速度,more friendly to real-time action detection。比STMixer的性能略微好一点，<strong>STMixer是最近的端到端的模型，设计了一个decoder来采样discriminativefeatures。EVAD is deivsed for efficient video featureextraction，进一步结合这两个或许会得到更好的检测性能</strong>。</li></ul></li><li><p>Conclusion：受videosequence的transformer的大量的计算开销和视频检测中高度冗余时空信息的的启发，通过<strong>droping out spatiotemporal tokens and refining scene context toenable efficient transformer-based actiondetection</strong>,设计了EVAD的方法。 EVAD的局限是：它需要<strong>retraining once to take the benefits of reduced computations andfaster inference from removing redundancy</strong>。一个潜在的方法是探索transformer-adaptive token pruning algorithms。另外，follow 端到端的框架<strong>WOO</strong>来验证<strong>EVAD的效率和有效性</strong>，但是<strong>WOO</strong>是一个两阶段的pipeline，sequentially执行actor localization和action classificationmodules。在未来的工作中，旨在将这两个modules集成到一个 unifiedhead，能够减小通过 detector head的推理时间，能够amplifyEVAD的高效的特点。</p></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/f3ea68b1948c43518259dc22dc121b2c9730103a/3-Figure2-1.png"alt="Structure" /><figcaption aria-hidden="true">Structure</figcaption></figure><p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Pipeline ofEVAD. Our detector consists of three parts: an encoder withkeyframe-centric token pruning for efficient video feature extraction, aquery-based localization branch using multiscale features of thekeyframe for actor boxes prediction, and a classification branchconducting actor spatiotemporal feature refinement and relationalmodeling between actor RoI features and compact context tokens from ViTencoder.</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/f3ea68b1948c43518259dc22dc121b2c9730103a/4-Figure3-1.png"alt="Keyframe-centric token pruning" /><figcaption aria-hidden="true">Keyframe-centric tokenpruning</figcaption></figure><p><span class="math inline">\(Figure \ 2^{[1]}\)</span>: Structure ofkeyframe-centric token pruning.We conduct token pruning within theoriginal encoder layer.Specifically, we calculate the importance scoresof non-keyframe tokens using a keyframe-enhanced attention map.Then, wepreserve the top-k important non-keyframe tokens concatenated withkeyframe tokens as the results</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;efficient-video-action-detection-with-token-dropout-and-context-refinement1&quot;&gt;Efficient
Video Action Detection with Token Dropout and Context
Refinement&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自nju、蚂蚁集团、复旦和上海AI Lab的Lei Chen、Zhan
Tong、Yibing Song等人。论文引用[1]:Chen, Lei et al. “Efficient Video
Action Detection with Token Dropout and Context Refinement.” 2023
IEEE/CVF International Conference on Computer Vision (ICCV) (2023):
10354-10365.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2023.Aug&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;spatiotemporal token dropout&lt;/li&gt;
&lt;li&gt;maintain all tokens in keyframe representing scene context&lt;/li&gt;
&lt;li&gt;select tokens from other frames representing actor motions&lt;/li&gt;
&lt;li&gt;drop out irrelavant tokens.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;视频流clips with large-scale vieo tokens 阻止了ViTs for efficient
recognition，特别是在video action
detection领域，这是需要大量的时空representations来精确地actor
identification。这篇工作，提出了端到端的框架 &lt;strong&gt;for efficient video
action detection(EVAD) based on vanilla
ViTs&lt;/strong&gt;。EVAD包含两个为视频行为检测的特殊设计。首先：提出来时空token
dropout from a keyframe-centric perspective. 在一个video clip中，main
all tokens from its keyframe，保留其它帧中和actor
motions相关的tokens。第二：通过利用剩余的tokens，refine scene context
for better recognizing actor identities。action
detector中的RoI扩展到时间域。获得的时空actor identity representations
are refined via scene context in a decoder with the attention
mechanism。这两个设计使得EVAD高效的同时保持精度。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>YOWO</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/YOWO/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/YOWO/</id>
    <published>2024-08-24T08:51:02.000Z</published>
    <updated>2024-08-27T07:15:14.850Z</updated>
    
    <content type="html"><![CDATA[<h4id="you-only-watch-once-a-unified-cnn-architecture-for-real-time-spatiotemporal-action-localization1">YouOnly Watch Once: A Unified CNN Architecture for Real-Time SpatiotemporalAction Localization<sup>[1]</sup></h4><blockquote><p>作者是来自Technical Univ of Munich的Okan Kopuklu, Xiangyu Wei,Gerhard Rigoll。论文引用[1]:Köpüklü, Okan et al. “You Only Watch Once: AUnified CNN Architecture for Real-Time Spatiotemporal ActionLocalization.” ArXiv abs/1911.06644 (2019): n. pag.</p></blockquote><h4 id="time">Time</h4><ul><li>2019.Nov.15(v1)</li><li>2021.Oct.18(v5)</li></ul><h4 id="key-words">Key Words</h4><ul><li>single-stage with two branches</li></ul><h4 id="总结">总结</h4><ol type="1"><li>当前的网络抽取时序信息和keyframe的空间信息是用两个分开的网络，然后用一个额外的mechanism来融合得到detections。YOWO是一个单阶段的架构，有两个分支，来同时抽取当前的时序和空间信息，预测bboxes和action的概率 directly from video clips in oneevaluation。因为架构是统一的，因此可以端到端的优化。YOWO架构速度快，能够做到在16-framesinput clips上做到 34 frames-per-second，62 frames-per-second on 8-framesinput clips。是当前在STAD任务上最快的架构。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>和静态图像里的目标检测相比，时序信息很重要，受目标检测FasterRCNN的启发，SOTA的工作将经典两阶段的网络架构扩展到actiondetection，第一阶段产生proposals，在第二阶段进行分类和定位的refinement，然而，两阶段的STAD任务有3个主要的缺点：(1):action tube是由bboxes acrossframes组成的，它的产生比2Dcase更复杂和耗时。分类的性能极度依赖这些Proposals，然而detectedbboxes可能对于后续的分类任务是sub-optimal；(2)actionproposals只关注视频里的人物的features，忽略人和背景中的其它特征，而这这能够提供对于actionprediction相当重要的上下文信息。(3)训练RPN网络和分类网络是分开的，不能保证找到全局最优。只有局部最优from the combination of two stages can be found.训练的成本比单阶段的高，因此花费很多时间和存储。</p></li><li><p>YOWO克服了上述提到的缺点，YOWO的本能的idea是来自人类视觉认知系统，为了理解视频中的人物的行为，需要<strong>将当前帧的信息2Dfeatures from key frame</strong>与之前记忆里获得的知识(3D features fromclip)相关联，之后，两种features融合到一起，提供一个合理的结论。YOWO架构是一个单阶段的、有两个分支的网络。一个分支<strong>提取keyframe的spatial features via a 2D-CNN，另一个分支models spatiotemporalfeatures of the clip consisting of previous frames via a 3DCNN</strong>。YOWO是一个<strong>causal架构(因果架构)，就是没有利用future frames</strong>，能够operate onlineon incoming video streams。为了aggregate 2D CNN和3D CNN的featuressmoothly, 用了一个channel fusion和attention机制，get the utmost out ofinter-channel dependencies。最后，用融合的特征，产生frame-leveldetections，用一个<strong>linking 算法</strong>来产生action tubes.YOWO不局限于RGB的模态，其它的例如光流也是可以的；任何一个CNN的架构根据实时性能的要求，都可以用。YOWOoperates with maximum 16 frames input，因为short cliplengths对于实现STAD人物的实时性是必要的。然而，small clipsize是时序信息累计的限制因素。因此，利用<strong>long-term featurebank</strong>，通过训练好的3DCNN从整个视频中提取非重叠的8帧片段的特征。在推理的时候，averaged 3Dfeatures centering the key-frame.</p></li><li><p>主要贡献如下：</p><ul><li>提出了在视频流里的单阶段的STAD框架，能够端到端的训练。实现了在3DCNN和2D CNN上特征的bboxes 回归，同时，这两个特征对于彼此是互补的 forfinal bboxes 回归和分类。用了<strong>channel attention</strong>来汇聚两个branches的特征。实验证明：channel-wise attention机制，modelsinter-channel relationship within the concatenated featuremaps，提高了性能。</li></ul></li><li><p>Related work: 为了考虑时序信息，有twe-streamCNN来提取分别提取空间和时间特征，然后汇聚到一起；这样的工作大部分是基于光流的，很耗时和耗计算资源。然后就是3DCNN，用它来提取时空特征。为了resource efficiency，一些工作用2DCNN来学习2D 特征，然后用一个3DCNN来将它们融合到一起，学习时间特征。Attention是一个有效的mechanism来capturelong-range dependencies，用在了CNNs中来尝试提高图像分类的性能。Attentionmechanism在<strong>spatial-wise和channel-wise</strong>来执行。<strong>spatialattention解决inter-saptial relationship among features，channelattention增强最有意义的channels，弱化其它的</strong>。作为一个channel-wiseattention block，Squeeze-and-Excitationmoduel对于提高CNN的性能有益。另一方面，对于视频分类人物，non-localblock考虑时空信息，来学习acrossframes的特征的dependencies。可以视为自注意力策略。不同于之前的工作，YOWO只使用clip一次，检测keyframe里的对应的actions。为了避免光流的复杂计算，用keyframe的2Dfeatures和clip的3D features。之后，两种类型的features用<strong>attentionmechanism</strong>融合在一起，就能够考虑到丰富的上下文信息。</p></li><li><p>YOWO的架构主要分为4个部分：<strong>3D CNNbranch</strong>、<strong>2D CNN branch</strong>、<strong>CFAM</strong>和 <strong>bbox regression parts</strong>.</p><ul><li><font color=red>3DCNN</font>：因为上下文信息对于人类行为理解很重要，因此用3DCNN来提取时空特征。3DCNN能够获得运动信息(在空间和时间维度做卷积)，基本的3D CNN架构这里用的是3D-ResNext-101，对于所有的3DCNN架构，在最后的卷积层之后的所有层都会被丢弃。3D网络的输入是一个videoclip。是由时间上连续的视频帧组成，shape为 <span class="math inline">\(C\times D \times H \times W\)</span>，最后一个3DResNext-101的输出的feature map的shape为 <spanclass="math inline">\(C&#39; \times D&#39; \times H&#39; \timesW&#39;\)</span>，<span class="math inline">\(C=3, D&#39;=1, H&#39; =\frac{H}{32}, W&#39; = \frac{W}{32}\)</span>，<spanclass="math inline">\(D\)</span> 是输入的帧数。输出特征图的depth维度减小到1，以至于 output volume可以squeezed to <spanclass="math inline">\(C&#39; \times H&#39; \timesW&#39;\)</span>，为了和2D-CNN的输出匹配。</li><li><font color=red>2D CNN</font>：为了解决空间定位问题，keyframe的2D特征被extracted in parallel，用Darknet-19作为2DCNN的基本架构，因为它在精度和效率上取得了平衡。key frame with the shape<span class="math inline">\(C \times H \times W\)</span> 是输入clip的<strong>most recentframe</strong>，因此不需要一个额外的dataloader，Darknet-19的输出特征图的shape为<span class="math inline">\(C&#39;&#39; \times H&#39; \timesW&#39;\)</span>，<span class="math inline">\(C= 3\)</span>，<spanclass="math inline">\(C&#39;&#39;\)</span> 是输出的channels，<spanclass="math inline">\(H&#39;= \frac{H}{32}, W&#39; =\frac{W}{32}\)</span>，和3DCNN的情况类似。YOWO的另一个重要特性是，它的2D CNN和3DCNN的分支能够被任意的CNN架构代替，使其更灵活。需要注意的是：<strong>虽然YOWO有两个分支，但是它是一个统一的架构，能够端到端的训练</strong>。</li><li><font color=red>Feature aggregation: Channel Fusion and AttentionMechanism(CFAM) </font>：让3D和2D网络的输出有相同的shape in the last twodimensions，以至于两个featuremaps能够简单地融合。用concatenation来融合两个featuremaps。因此，融合的feature map能够 <strong>encoders both motion andappearanceinformatin</strong>，然后这个融合的特征输到CFAM模块里，这个模块是基于<font color=red>Gram matrix</font>来映射 <strong>inter-channeldependencies</strong>。虽然<strong>Grammatrix</strong>最初是用来做<strong>styletransfer</strong>，最近用在了segmentationtask，这样一个注意力机制对于来自不同sources的feature的融合是有益的。能够提高性能。concatenatedfeature map <span class="math inline">\(A \in R^{(C&#39; + C&#39;&#39;)\times H \times W}\)</span>，可以被视为3D 和2D 信息的一个 <strong>abruptcombination</strong>，这忽略了它们之间的<strong>interrelationship</strong>。因此，将<spanclass="math inline">\(A\)</span>输给两个conv layers来产生新的特征图<span class="math inline">\(B \in R^{C \times H&#39; \timesW&#39;}\)</span>，之后，在特征图<spanclass="math inline">\(B\)</span>上进行一些操作。$F R^{C N} $是 featuremap <span class="math inline">\(B\)</span>reshape之后的tensor， <spanclass="math inline">\(N=H \timesW\)</span>，意味着每个channel中的features被 <strong>vectorized to onedimension</strong>。然后 对 <span class="math inline">\(F \in R^{C\times N}\)</span> 和它的转置 <span class="math inline">\(F^T \in R^{N\times C}\)</span> 进行 矩阵乘法，来得到 <strong>Gram Matrix</strong><span class="math inline">\(G \in R^{C \timesC}\)</span>，这个矩阵能够表明不同channel之间的<strong>correlations</strong>。</li></ul><p><spanclass="math display">\[\begin{array}{rcl}\mathbf{G}&amp;=&amp;\mathbf{F}\cdot\mathbf{F}^\mathrm{T}&amp;with&amp;G_{ij}&amp;=&amp;\sum_{k=1}^{N}F_{ik}\cdotF_{jk}\end{array}\]</span></p><p><strong>Gram matrix G</strong>中的每个元素 <spanclass="math inline">\(G_{ij}\)</span>代表 <strong>vectorize feature mapsi and j</strong> 之间的<strong>inner product</strong>。</p><p>在计算完Gram matrix之后，用一个softmax layer来得到 <strong>channelattention map M</strong>, <span class="math inline">\(M \in R^{C \timesC}\)</span>，<span class="math inline">\(M_{ij}\)</span>表示<spanclass="math inline">\(j^{th}\)</span> channel对 <spanclass="math inline">\(i^{th}\)</span> channel之间的的影响。因此，<spanclass="math inline">\(M\)</span> summaries 给定featuremap的features的<strong>inter-channel dependency</strong>。为了performimpact of attention map to original features，对<spanclass="math inline">\(M\)</span>和<spanclass="math inline">\(F\)</span>做一个矩阵乘法，将结果shape到3维空间<span class="math inline">\(R^{C \times H \timesW}\)</span>，这将和输入的tensor有相同的shape。</p><p><span class="math display">\[\mathbf{F}^{\prime} =\mathbf{M}\cdot\mathbf{F}\\\mathbf{F}^{\prime}\in\mathbb{R}^{C\timesN}\xrightarrow{reshape}\mathbf{F}^{\prime\prime}\in\mathbb{R}^{C\timesH\times W}\]</span></p><p>channel attention module的输出 <span class="math inline">\(C \in R^{C\times H \times W}\)</span>是 <spanclass="math inline">\(F&#39;&#39;\)</span>和初始输入特征图 <spanclass="math inline">\(B\)</span> 的的结合，with a trainable scalarparameter <span class="math inline">\(\alpha\)</span>，用<strong>element-wise sum operation</strong>，<spanclass="math inline">\(\alpha\)</span> <strong>learns a weight from0</strong>。 <span class="math display">\[\mathbf{C} =\alpha\cdot\mathbf{F}^{\prime\prime}+\mathbf{B}\]</span></p><p>该方程表明：每个channel的最后的特征是 <strong>weighted sum of thefeatures of all channels and original features</strong>，这是对featuremaps之间的long-range的semantics dependencies的建模。最后 <spanclass="math display">\[\mathbf{C}\in\mathbb{R}^{C\times H^{\prime}\timesW^{\prime}}\]</span> 给到了两个conv layer，来得到 <em>CFAM module</em>的输出特征图 <spanclass="math display">\[\mathbf{D}\in\mathbb{R}^{C^{*}\timesH^{\prime}\times W^{\prime}}\]</span>，在CFAM模块的开始和结束的2 个convlayer很重要，因为它们 <strong>mix the features from differentdistributions</strong>，没有这些convlayers，CFAM模块的性能提升会有限。</p><p>这样的一个架构 promote feature representativeness in terms of<strong>inter-dependencies amongchannels</strong>，因此来自不同branches的features能够被reasonably andsmoothly汇聚到一起。另外，<strong>Gram matrix</strong> 考虑整个 featuremap。两个flattened feature vectors的<strong>点乘</strong> 展示了<strong>它们之间的relationinformation</strong>。一个比较大的product表明两个 channels的features<strong>more correlated</strong>，smallerproduct表明它们彼此不一样。对于一个给定的channel，<strong>allocate moreweights to other channels which are much correlated and have more impactto it</strong>。通过这种机制，上下文的关系被<strong>emphasized，features discriminability is enhanced</strong>。</p><ul><li><font color=red> Bounding box regression</font>：followYOLO相同的guidelines for bbox regression。最后一个conv layer with <spanclass="math inline">\(1 \times 1\)</span> kernels用来产生<strong>desired number of output channels。对于每个grid cel in <spanclass="math inline">\(H&#39; \times W&#39;\)</span>。用 k-means方法在对应的datasets上选择5个prior anchors, with </strong>NumCls classconditional action scores, 4 coordinates and confidencescore**。YOWO的最后的输出的size是 <spanclass="math inline">\([(5\times(NumCls+5))\times H&#39;\timesW&#39;]\)</span>。bboxes的回归然后基于这些anchors进行refined。</li></ul></li><li><p>在训练和测试阶段的输入分辨率都为 <span class="math inline">\(224\times 224\)</span>，用不同的分辨率进行multi-scaletraining在实验中没有发现有性能的提高。损失函数和原始的YOLOv2的网络中类似，除了这个采用了smooth L1 Loss with beta=1 for localization，</p><p><spanclass="math display">\[L_{1,smooth}(x,y)=\begin{cases}0.5(x-y)^2&amp;if|x-y|&lt;1\\\\|x-y|-0.5&amp;otherwise\end{cases}\]</span></p><p><span class="math inline">\(x,y\)</span>分别指prediction和groundtruth。L1 loss相比于MSE loss，对outliers不那么敏感，能够在某些情况阻止梯度爆炸。用MSE loss forconfidence scores.</p><p><span class="math display">\[L_{MSE}(x,y)=(x-y)^2\]</span></p><p>最后的detection loss是 individual coordiante losses forx,y,width,height和confidence score loss，</p><p><spanclass="math display">\[L_D=L_x+L_y+L_w+L_h+L_{conf}\]</span></p><p>用focal loss for classification:</p><p><span class="math display">\[L_{focal}(x,y)=y(1-x)^\gammalog(x)+(1-y)x^\gamma log(1-x)\]</span></p><p>x是 softmaxed network prediction, <span class="math inline">\(y \in{0,1}\)</span> is grouth truth class label。<spanclass="math inline">\(\gamma\)</span>是modulating factor，reduce loss ofsamples with high confidence(easy samples), increase the loss of sampleswith low confidence(hardsamples)。AVA数据集是一个多标签数据集，每个人执行一个poseaction和多个human-human or human-object interaction actions。因此，用<strong>softmax to pose classes and sigmoid to the interactionactions</strong>。另外，AVA是一个不平衡的数据集，modulating factor <spanclass="math inline">\(\gamma\)</span>不足以处理数据集的不平衡问题。因此用了一个focal loss的 <spanclass="math inline">\(\alpha -balanced variant\)</span>，对于 <spanclass="math inline">\(\alpha\)</span>，<strong>we have used exponentialof class sampleratios</strong>。最后的YOWO用的loss是检测的loss和分类的loss的和。</p><p><span class="math display">\[L_{final}=\lambdaL_D+L_{Cls}\]</span></p><p>这里 <span class="math inline">\(\lambda =0.5\)</span>在实验中表现最好。</p></li><li><p>分别初始化3D 和2D CNN网络：用在Kinetics上预训练的models来初始化3D CNN，用在PASCAL VOC上预训练的models来初始化2D CNN。虽然架构是由2DCNN和 3D CNN组成。这些参数能够一起更新。选择 <strong>mini-batch SGD withmomentum and weight decay</strong>来优化loss function。学习率初始为0.0001。在训练的时候，由于J-HMDB-21的样本数量少，冻结所有的 3D convnet的参数，因此收敛会更快，减小过拟合的风险。另外，在训练中用一些数据增强的方式例如flipping,random scaling等。</p></li><li><p><strong>linking strategy</strong>：在得到了frame-level的 actiondetection之后，下一步是将这些检测到的 bboxes 连接起来，构建<strong>action tubes in the whole video</strong>。利用连接算法，来找到最优的video-level action detections。 假设 <spanclass="math inline">\(R_t\)</span>和 <spanclass="math inline">\(R_{t+1}\)</span>是连续帧 <spanclass="math inline">\(t\)</span> 和 <spanclass="math inline">\(t+1\)</span>的两个区域。linking score for anaction class <span class="math inline">\(c\)</span> 定义为： <spanclass="math display">\[\begin{aligned}s_{c}(R_{t},R_{t+1})&amp;=\quad\psi(ov)\cdot[s_{c}(R_{t})+s_{c}(R_{t+1})\\&amp;+\alpha\cdots_{c}(R_{t})\cdot s_{c}(R_{t+1})\\&amp;+\beta\cdotov(R_{t},R_{t+1})]\end{aligned}\]</span></p><p><span class="math inline">\(s_{c}(R_{t})\)</span>和<spanclass="math inline">\(s_{c}(R_{t+1})\)</span> 是regions <spanclass="math inline">\(R_t\)</span>和 <spanclass="math inline">\(R_{t+1}\)</span>的 class specific socres。<spanclass="math inline">\(ov\)</span>是两个区域的IoU，如果overlap存在，则<spanclass="math inline">\(\psi(ov)\)</span>为1，否则为0.在linkingscore的定义上增加了一个额外的项：<span class="math inline">\(\alpha\cdots_{c}(R_{t})\cdots_{c}(R_{t+1})\)</span>。将两个连续帧的剧烈变化考虑进来了，能够提高videodetections的性能。在计算出所有的linking scores之后，用 <strong>Viterbialgorithm</strong>来找到最有的路径，生成 action tubes。</p></li><li><p><strong>long-term featurebank</strong>：虽然YOWO的推理是在线的和因果的 with small clipsize，但是16帧的输入限制了 temporal information required for actionunderstanding。因此，利用long-term featureback(LFB)，这个包含了不同的timestamps的来自3DCNN的features。在推理时，3D features centering the keyframe areaveraeged and resulting feature map用作输入，给到CFAM block，<strong>LFBfeatures are extracted for non-overlapping 8-frames clips using thepretrained 3D ResNeXt-101 backbone</strong>。用 8 features centering thekey-frame。因此在推理的时候，利用了总共64帧的数据。LFB增加了actionclassificatin的性能，类似于difference between clip accuracy and videoccuracy in video datasets。然后，LFB会导致一个非因果的架构，因为 future3D features在推理的时候用到了。</p></li></ol><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/08c936517b8e8b9fcab9a544c735be2909fab3ee/8-Figure2-1.png"alt="YOWO" /><figcaption aria-hidden="true">YOWO</figcaption></figure><p><span class="math inline">\(Figure \ \ 1^{[1]}\)</span>: The YOWOarchitecture. An input clip and corresponding key frame is fed to a 3DCNN and 2D-CNN to produce output feature volumes of <spanclass="math inline">\([C&#39;&#39; × H&#39; × W&#39;]\)</span> and <spanclass="math inline">\([C&#39; × H&#39; × W&#39;]\)</span>,respectively.These output volumes are fed to channel fusion and attention mechanism(CFAM) for a smooth feature aggregation. Finally, one last conv layer isused to adjust the channel number for final bounding boxpredictions.</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/08c936517b8e8b9fcab9a544c735be2909fab3ee/9-Figure3-1.png"alt="Channel Fusion" /><figcaption aria-hidden="true">Channel Fusion</figcaption></figure><p><span class="math inline">\(Figure \ \ 2^{[1]}\)</span>: Channelfusion and attention mechanism for aggregating output feature mapscoming from 2D-CNN and 3D-CNN branches</p>]]></content>
    
    
    <summary type="html">&lt;h4
id=&quot;you-only-watch-once-a-unified-cnn-architecture-for-real-time-spatiotemporal-action-localization1&quot;&gt;You
Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal
Action Localization&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自Technical Univ of Munich的Okan Kopuklu, Xiangyu Wei,
Gerhard Rigoll。论文引用[1]:Köpüklü, Okan et al. “You Only Watch Once: A
Unified CNN Architecture for Real-Time Spatiotemporal Action
Localization.” ArXiv abs/1911.06644 (2019): n. pag.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;time&quot;&gt;Time&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;2019.Nov.15(v1)&lt;/li&gt;
&lt;li&gt;2021.Oct.18(v5)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;key-words&quot;&gt;Key Words&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;single-stage with two branches&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;总结&quot;&gt;总结&lt;/h4&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;当前的网络抽取时序信息和keyframe的空间信息是用两个分开的网络，然后用一个额外的mechanism来融合得到detections。YOWO是一个单阶段的架构，有两个分支，来同时抽取当前的时序和空间信息，预测bboxes和action
的概率 directly from video clips in one
evaluation。因为架构是统一的，因此可以端到端的优化。YOWO架构速度快，能够做到在16-frames
input clips上做到 34 frames-per-second，62 frames-per-second on 8-frames
input clips。是当前在STAD任务上最快的架构。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Holistic Interaction Transformer</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/24/Holistic-Interaction-Transformer/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/24/Holistic-Interaction-Transformer/</id>
    <published>2024-08-24T01:24:04.000Z</published>
    <updated>2024-08-27T07:12:05.312Z</updated>
    
    <content type="html"><![CDATA[<h4id="holistic-interaction-transformer-network-for-action-detection1">HolisticInteraction Transformer Network for Action Detection<sup>[1]</sup></h4><blockquote><p>作者是来自国立清华大学和微软AI的Gueter Josmy Faure, Min-HungChen和Shang-Hong Lai.论文引用[1]:Faure, Gueter Josmy et al. “HolisticInteraction Transformer Network for Action Detection.” 2023 IEEE/CVFWinter Conference on Applications of Computer Vision (WACV) (2022):3329-3339.</p></blockquote><h3 id="time">Time</h3><ul><li>2022.Nov.18</li></ul><h3 id="key-words">Key Words</h3><ul><li>bi-modal structure</li><li>combine different interactions</li></ul><h3 id="总结">总结</h3><ol type="1"><li>行为是关于我们如何与环境互动的，包括其他人、物体和我们自己。作者提出了一个新的多模态的<strong>HolisticInteraction Transformer Network(HIT)</strong>，利用大量被忽略的、但是对人类行为重要的手部和姿态信息。HIT网络是一个全面的bi-modal框架，由一个RGBstream和pose stream组成。每个stream独立地建模person、object和handinteractions，对于每个子网络，用了一个<strong>Intra-Modality Aggregationmodule(IMA)</strong>，选择性地融合个体的交互。从每个模态的得到的features然后用一个<strong>AttentiveFusion Mechanism(AFM)</strong>进行融合，最后，从temporalcontext中提取cues，用cached memory来更好地分类存在的行为。</li></ol><span id="more"></span><ol start="2" type="1"><li><p>一个合理的时空行为检测的框架旨在正确地label帧里的每个人。应该在相邻帧之间keep a link来更好地理解带有连续性特点的活动例如："open"、"close"。最近，更多鲁棒的工作来考虑spatialentities之间的关系，因为如果两个人在同一帧，他们大概率会和彼此互动。然而，仅用personfeatures是不足以获得object-related action。其他人try to understand不仅是帧里的人物之间的关系，而且还有他们周围的物体。这些方法有两个主要的缺点：(1)他们仅依赖于highdetectionconfidence的目标，可能会导致忽略重要的没有检测到的目标；(2)这些模型努力检测没有出现在frame里的和目标相关的actions。例如：考虑到行为："pointto (an object)",actor所指向的object可能不在当前帧中。</p></li><li><p>HIT网络用细粒度的上下文，包括personpose，hands，objects，来构建一个<font color=red> bi-modal interactionstructure </font>，每个modality包括3个主要的components：<font color=red>person interaction </font>，<font color=red> object interaction</font>，<font color=red>hand interaction</font>，每个component学习有价值的local actionpatterns。在从相邻帧学习时序信息之前，用<font color=red>Attentive FusionMechanism </font>结合不同模态的信息，来帮助更好地检测发生在当前帧中的行为。主要的contributions有：</p><ul><li>提出了新的框架、<strong>结合RGB、pose和hand features for actiondetection</strong></li><li>引入了一个<strong>bi-modal HIT网络，结合不同的interaction in anintuitive and meaningful way</strong></li><li>提出了一个<strong>Attentive FusionModuel</strong>，作为一个选择性地过滤器，保持每个modal中信息量最多的features，<strong>Intra-ModalityAggregator</strong>用来学习modalities内有用的actionrepresentations.</li></ul></li><li><p>Related work:</p><ul><li><p>spatio-temporal actiondetection，不同于将整个视频分为1类，需要在空间上和时间上检测到行为。很多近期的spatio-temporalaction detection上的工作是用3D CNN作为backbone来提取视频特征，然后用ROIpooling或者ROI align来crop person features from videofeatures，这么做，抛弃了视频中潜在的有用信息。</p></li><li><p>时空行为检测任务实际是一个<strong>交互建模</strong>的任务，大多数的行为是在和环境互动。很多研究是用<strong>attentionmechanism</strong>，有人提出了<strong>Temporal RelationNetwork(TRN)</strong>，学习帧间的依赖，或者说，interaction betweenentities from相邻帧。其它的方法进一步，不仅建模temporal，也建模spatialinteractions between different entities from the same frame.然而，选择什么entities来model interactions也是因model而异的，不只用humanfeatures，也有人用background information来model interactions between theperson in the frame and context. 选择crop persons'features，但是不抛弃剩下的backgroundfeatures，这样提供了丰富的信息，但是，可能会引入很多噪音。也有人尝试bemore selective about the features to use. 有人首先pass the video framesthrough an object detecotr, crop both the object and personfeatures，model它们的interactions。interaction的额外的层，提供了更好的representationsthan 单独的human interaction modeling models，能够helps with classesrelated to objects such as "work on acomputer"，然而，当目标太小没有检测到或者没有出现在frame中的时候，这些方法会fallshort。</p></li><li><p>很多最近的action detection frameworks仅用RGBfeatures，也有人用光流来获取motion，有人用inception-likemodel来concatenates RGB和flow features at the <font color=red> Mixed4blayer </font>，然而也有人用I3D 网络来分别得到RGB和flowfeatures，然后在actionclassifier之前，concatenate两个模态。作者提出的bi-modal方法，用了<font color=red>visual and skeleton-based features</font>，每个modality计算一系列的interactions includingperson、object、and hands before being fused。一个temporal interactionmodule用在fused features上，来学习global information regardingneighboring frames.</p></li></ul></li><li><p>Methods: HIT由<strong>RGB和posesubnetwork组成</strong>,每个旨在学习persions interactions with theirsurroundings by focusing on the key entities that drive most of ouractions。在融合了两个sub-networks的输出之后，进一步通过<font color=red>lookingat cached features from past and future frames，model how actions evolveintime</font>。这样全面的活动的理解方案能够帮助实现更好的行为检测性能。几个步骤：<strong>entityselection</strong>、<strong>RGB modality</strong>、<strong>posemodality</strong>、<strong>Attentive FusionModule(AFM)</strong>、<strong>Temporal Interaction Module</strong>.</p><ul><li><strong>entity selection</strong>: HIT由两个mirroring modalitieswith distinct modules组成，来学习不同类型的interactions。Humanactions大部分基于它们的pose、hand movements和interactions with theirsurroundings。基于这些观察，选择human poses 和handsbboxes作为模型的entities along with object and personbboxes。用<strong>Detectron</strong> for human posedetection，然后create a bbox 包围location of the person's hands.用<font color=red> Faster-RCNN</font> 来计算<strong>object bboxproposals</strong>。 Video feature extractor是一个 3D CNN backbone。poseencoder是一个轻量的<font color=red>spatialtransformer</font>，用ROIAlign 来trim videofeatures，来抽取person、hands和object features。</li><li><strong>RGB branch</strong>：RGBbranch包含3个components，每个包含一系列的操作，来学习目标person的特定的信息。这个object和handsinteraction modules model person-object and person-handsinteraction。person interaction moduel学习当前帧之间的persons的interaction。在每个interaction unit的heart，是要给<font color=red>cross-atttentincomputation</font>，这个<strong>query是target person(or the output ofthe previous unit), key and value are derived from the objects, or handsfeatures,depending on which moduele we are at</strong>.<font color=blue> it is like asking "how can these particular featureshelp detect what the target person is doing?"</font>，公式如下： <spanclass="math display">\[F_{rgb}=(A(\mathcal{P})\to z_{r}\toA(\mathcal{O})\to z_{r}\to A(\mathcal{H})\toz_{r})\\A(*)=softmax(\frac{w_q(\widetilde{P})\timesw_k(*)}{\sqrt{d_r}})\times w_v(*)\\z_{r}=\sum_{b}A(b)\timessoftmax(\theta_{b}),b\in(\widetilde{P},\mathcal{O},\mathcal{H},\mathcal{M})\]</span></li></ul><p><span class="math inline">\(d_r\)</span> 代表RGB features的dimensionchannel,<font color=red> <span class="math inline">\(w_q\)</span>、<spanclass="math inline">\(w_k\)</span>、<spanclass="math inline">\(w_v\)</span> project their inputs into query,keyand value</font>，<em>A(*)</em>是cross-attention机制，only takes personfeatures as input when computing person interaction <spanclass="math inline">\(A(P)\)</span>，对于hand interaction(objectsinteraction):只有两个sets的输入：output of <spanclass="math inline">\(z_r\)</span> which serves as query(<spanclass="math inline">\(\bar{P}\)</span>)、hands features(object features)from which we obtain the key and values.</p><p><span class="math inline">\(Z_r\)</span>是所有interactionmodules的加权和，包括termporal interaction module <spanclass="math inline">\(TI\)</span>，<spanclass="math inline">\(Z_r\)</span>很重要，首先，它允许网络aggregate尽可能多的信息；另外可学习的参数<spanclass="math inline">\(\theta\)</span>帮助过滤不同sets的features，hand-pickingthe best each of them has to offer while discarding noisy andunimportant information。</p><ul><li><strong>Pose branch</strong>：pose model类似于它的RGBcounterpart，reuses most of its outputs。首先通过一个轻量的transformerencoder <span class="math inline">\(f\)</span>来抽取pose features <spanclass="math inline">\(K&#39;\)</span> <spanclass="math display">\[\mathcal{K}^{\prime}=f(\mathcal{K})\]</span></li></ul><p>然后通过mirroring RGB modality的不同的constituents来计算 <spanclass="math inline">\(F_pose\)</span>，然后reusing 对应的outputs，<spanclass="math inline">\(P&#39;\)</span>、<spanclass="math inline">\(O&#39;\)</span>、<spanclass="math inline">\(H&#39;\)</span>是对应的outputs of <spanclass="math inline">\(A(P),A(O),A(H)\)</span>。 <spanclass="math display">\[F_{pose}=(A(\mathcal{K}^{\prime},\mathcal{P}^{\prime})\toz_{p}\to A(\mathcal{O}^{\prime})\to z_{p}\to A(\mathcal{H}^{\prime})\toz_{p})\\A(\mathcal{K}^{\prime},\mathcal{P}^{\prime})=softmax(\frac{w_{q}(\mathcal{K}^{\prime})\timesw_{k}(\mathcal{P}^{\prime})}{\sqrt{d_{p}}})\timesw_{v}(\mathcal{P}^{\prime})\]</span></p><p><spanclass="math inline">\(A(\mathcal{K}^{\prime},\mathcal{P}^{\prime})\)</span>计算cross-attentionbetween pose features <spanclass="math inline">\(K&#39;\)</span>和增强的person interaction features<span class="math inline">\(P&#39;\)</span>，这样的cross-modal blendenforces the pose features by focusing on the key correspondingattributes of the RGB features。其它的components，<spanclass="math inline">\(A(O&#39;)\)</span>和<spanclass="math inline">\(A(H&#39;)\)</span>对 <spanclass="math inline">\(Z_p\)</span>做一个线性的projection as query whiletheir key-value pairs stem from <span class="math inline">\(A(O) andA(H)\)</span>，<spanclass="math inline">\(Z_p\)</span>是要给intra-modality aggregationcomponent for the pose model,类似于 <spanclass="math inline">\(Z_r\)</span>，它过滤和汇聚了每个interactionmodule的信息。</p><ul><li><p>Attentive Fusion Module(AFM)：在进入action分类器之前，RGB和posestream需要结合到一个set of features。出于这个目的，提出了AttentionFusion Module，用<font color=red>channel-wise concatenation of twofeatures sets followed by self-attention for featurerefinement</font>。通过用project matrix <spanclass="math inline">\(\Theta_{fused}\)</span>来减小输出特征的值。 <spanclass="math display">\[F_{fused}=\Theta_{fused}(SelfAttention(F_{rgb},F_{pose}))\]</span></p></li><li><p>Temporal Interaction Unit：fusion module之后，就是一个temporalinteraction block <span class="math inline">\(TI\)</span>，humanactions是连续发生的，因此，long-term context对于理解行为是重要的，alongwith <span class="math inline">\(F_fused\)</span>，这个Module receives压缩的 memory data <span class="math inline">\(M\)</span> with length<span class="math inline">\(2S+1\)</span>，memory cache包含了videobackbone得到的person features. <spanclass="math inline">\(F_fused\)</span> inquirs <spanclass="math inline">\(M\)</span> as to which of the neighboring framescontains informative features, 然后absorb。<spanclass="math inline">\(TI\)</span>是另一个 cross-attention module where<span class="math inline">\(F_fused\)</span>是query,memory <spanclass="math inline">\(M\)</span>两个不同的projections构成了key-valuepairs. <spanclass="math display">\[F_{cls}=TI(F_{fused},\mathcal{M})\]</span>最后，分类头 <span class="math inline">\(g\)</span>是有两个 feed-forwardlayers with relu activation 和output layer组成的， <spanclass="math display">\[\hat{y}=g(F_{cls})\]</span></p></li></ul></li><li><p>实验：</p></li></ol><ul><li><p>person和object detector:从数据集中的每个视频抽取keyframes，然后用detected person bbox from(YOWO[16]) fo inference。作为一个object detector，用FasterRCNN withResNet-50-FPN作为backbone，模型是在ImageNet上进行的预训练，然后再MSCOCO上微调。</p></li><li><p>Keypoints Detection and Processing：对于keypoints detection，采用<span class="math inline">\(Detectron\)</span>中的posemodel，用在ImageNet for object detection上预训练 and fine-tuned onMSCOCO keypoints using precomputed RPN proposals的ResNet-50-FPN，targetdataset中的每个keyframe通过model，输出17个keypoints for each detectedperson corresponding to the COCO format.进一步后处理这些检测到的posecoordinates，因此match GT person bboxes(during training) and bboxes from<a href="during%20testing">16</a>。对于person handslocation，仅对人的手腕的部位的keypoints感兴趣，因此，对这个keypoints做一个bboxes，来highlight人物的hands和everything in between。</p></li><li><p>用SlowFast网络作为视频的backbone</p></li><li><p><strong>Limitations</strong>: 该框架依赖于离线的detector和poseestimator，因此detector和poseestimator的精度可能会对这个方法有影响。similar-lookingclasses例如："throw"和"catch"看起来很像；第二个是部分的occlusion。</p></li></ul><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/ea3763cbc7bc7b502dd50fe0a54b7d06d8d8163b/3-Figure2-1.png"alt="HIT Network" /><figcaption aria-hidden="true">HIT Network</figcaption></figure><p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Overview ofour HIT Network. On top of our RGB stream is a 3D CNN backbone which weuse to extract video features. Our pose encoder is a spatial transformermodel. We parallelly compute rich local information from bothsub-networks using person, hands, and object features. We then combinethe learned features using an attentive fusion module before modelingtheir interaction with the global context</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/ea3763cbc7bc7b502dd50fe0a54b7d06d8d8163b/3-Figure3-1.png"alt="Illustration of the Interaction module" /><figcaption aria-hidden="true">Illustration of the Interactionmodule</figcaption></figure><p><span class="math inline">\(Figure \ 2^{[1]}\)</span>: Illustrationof the Interaction module. ∗ refers to the module-specific inputs whilePe refers to the person features in <spanclass="math inline">\(A(P)\)</span> or the output of the module thatcomes before <span class="math inline">\(A(∗)\)</span></p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/ea3763cbc7bc7b502dd50fe0a54b7d06d8d8163b/3-Figure4-1.png"alt="Illustration of the Intra-Modality" /><figcaption aria-hidden="true">Illustration of theIntra-Modality</figcaption></figure><p><span class="math inline">\(Figure \ 3^{[1]}\)</span>: Illustrationof the Intra-Modality Aggregator. Features from one unit to the next arefirst augmented with contextual cues then filtered</p>]]></content>
    
    
    <summary type="html">&lt;h4
id=&quot;holistic-interaction-transformer-network-for-action-detection1&quot;&gt;Holistic
Interaction Transformer Network for Action Detection&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自国立清华大学和微软AI的Gueter Josmy Faure, Min-Hung
Chen和Shang-Hong Lai.论文引用[1]:Faure, Gueter Josmy et al. “Holistic
Interaction Transformer Network for Action Detection.” 2023 IEEE/CVF
Winter Conference on Applications of Computer Vision (WACV) (2022):
3329-3339.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2022.Nov.18&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;bi-modal structure&lt;/li&gt;
&lt;li&gt;combine different interactions&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;行为是关于我们如何与环境互动的，包括其他人、物体和我们自己。作者提出了一个新的多模态的&lt;strong&gt;Holistic
Interaction Transformer Network
(HIT)&lt;/strong&gt;，利用大量被忽略的、但是对人类行为重要的手部和姿态信息。HIT网络是一个全面的bi-modal框架，由一个RGB
stream和pose stream组成。每个stream独立地建模person、object和hand
interactions，对于每个子网络，用了一个&lt;strong&gt;Intra-Modality Aggregation
module(IMA)&lt;/strong&gt;，选择性地融合个体的交互。从每个模态的得到的features然后用一个&lt;strong&gt;Attentive
Fusion Mechanism(AFM)&lt;/strong&gt;进行融合，最后，从temporal
context中提取cues，用cached memory来更好地分类存在的行为。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Action Detection" scheme="https://young-eng.github.io/YoungBlogs/tags/Action-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Video Understanding</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/20/Video-Understanding/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/20/Video-Understanding/</id>
    <published>2024-08-20T06:12:07.000Z</published>
    <updated>2024-08-23T08:38:57.194Z</updated>
    
    <content type="html"><![CDATA[<h4id="视频理解及分析的计算机视觉任务">视频理解及分析的计算机视觉任务</h4><ol type="1"><li><p>之前看的时候，不管是论文还是一些博客，感觉都不是很清晰和全面，大家的定义不全面，特别是英文的名称上，这里写一下我的理解：</p></li><li><p>几个任务：</p><ul><li><strong>行为识别(Action Recognition)</strong>:实质是对视频的分类任务，可以类别图像领域的分类任务</li><li><strong>时序动作定位(Temporal Action Localization)</strong>:在时间上对视频进行分类，给出动作的起止时间和类别</li><li><strong>时空行为检测(Spatio-Temporal Action Detection)</strong>:不仅识别出动作出现的<strong>区间</strong>和<strong>类别</strong>，还要在空间范围内用一个boundingbox标记处目标的<strong>位置</strong>。</li><li><strong>还有人提出了时空动作定位(Spatio-temporal Actionlocalization)</strong>：和上一个是一样的</li><li>Action Detection在Paperswithcode上的定义： aims to find both whereand when an action occurs within a video clip and classify what theaction is taking place. Typically results are given in the form of<strong>action tublets</strong>, which are action bounding boxes linkedacross time in the video. <em>This is related to temporal localization,which seeks to identify the start and end frame of an action, and actionrecognition, which seeks only to classify which action is taking placeand typically assumes a trimmed video.</em></li><li>论文里还提到了<strong>temporal action segmentation</strong>：针对细粒度的actions和videos with dense occurrence of actions to predictaction label labels at every frame of the video.</li></ul></li><li><p>时空行为检测的算法：之前的论文都是都是基于行为识别(actionrecognition)的，很多都是基于早期的Slowfast的那个检测的方式：需要一个额外的检测器，实现行为检测。也就是在行为识别的基础上，再进行时空行为检测。但这并不是我理想中的方式，所以很多行为识别的算法，在AVA上也能上榜；最近看VideoMAE看了之后，就一直在看这个，没有去看看其它的。</p></li><li><p>Action Detection数据集：</p><ul><li>J-HMDB</li><li>UCF101-24</li><li>MultiSports</li><li>AVA</li><li>其中，JHMDB和UCF101-24是密集标注数据集(每一帧都标注，25fps)，这类数据集每个视频只有一个动作，大部分视频是单人做一些语义简单的重复动作；AVA为代表的稀疏标注数据集(隔一段时间标注一帧，1fps)，没有给出明确的动作边界</li></ul></li></ol><span id="more"></span><h4id="deep-learnign-based-action-detection-in-untrimmed-videos-a-survey">DeepLearnign-based Action Detection in Untrimmed Videos: A Survey</h4><blockquote><p>作者是来自纽约城市大学的Elahe Vahdani and YingliTian,论文引用[1]:Vahdani, Elahe and Yingli Tian. “Deep Learning-BasedAction Detection in Untrimmed Videos: A Survey.” IEEE Transactions onPattern Analysis and Machine Intelligence 45 (2021): 4302-4320.</p></blockquote><ol type="1"><li><p>很多action recognition 算法是在untrimmedvideo里，真实世界中的视频大部分是漫长的和untrimmed with sparse segmentsof interest. temporal activitydetection在没有剪辑的视频里的任务是定位行为的时间边界和分类行为类别。spatio-temporalactiondetection：action在temporal和spatial维度上都进行定位，还需识别行为的类别。因为长的未修剪的视频的标注费时费力，所以<strong>action detection with limited supervision</strong>是一个重要的研究方向。</p></li><li><p><strong>Temporal Action Detection</strong> 旨在 在untrimmedvideo里找到精确的时间边界和行为实例的label。依赖于训练集的标注的availability，可以分为：</p><ul><li>全监督 action detection: 时间边界和labels of action instances areavailable</li><li>弱监督action detection：only the video-level labels of actioninstances are available, the order of action labels can be provided ornot.</li><li>unsupervised action detection: no annotations for actioninstances</li><li>semi-supervised action detection: 数据被划分为小的子集 <spanclass="math inline">\(S_1\)</span> 和大的子集 <spanclass="math inline">\(S_2\)</span>，<spanclass="math inline">\(S_1\)</span> 中的视频是全标注的，<spanclass="math inline">\(S_2\)</span>中的视频没有标注(as infully-supervised)或者only annotated with video-level labels(asweakly-supervised)</li><li>self-supervised action detection:用一个代理任务从数据中抽取信息，然后用于提高性能，例如一般的自监督预训练，然后有监督微调。</li><li>Temporal action detection：或者说temporal action localization,思路和图像中的目标检测类似，会用到 proposals、RoIpooling这些类似的思路。</li></ul></li><li><p>Untrimmedvideos通常很长，由于计算资源的限制，很难直接把整个视频给到visual encoder来提取特征。通常的做法是把视频划分为相同大小的temporal intervals called<em>snippets</em>，然后对每个snippet都用visual encoder。</p></li><li><p><strong>Spatio-temporal Action Detection</strong>:有frame-levelaction detection和clip-level action detection</p><ul><li><strong>frame-level action detection</strong>:早期的方式是基于滑动窗口的一些扩展方法，要求一些很强的假设：例如cuboidshape，一个actor的跨帧的固定的空间范围。图像的目标检测启发了识别人类行为at frame level; 第一阶段，通过region proposal 或者 densely sampledanchors 产生action proposals，然后第二阶段proposals用于actionclassification和localization refinement. 在检测frames里的actionregions之后，一些方法，用光流来获取运动信息，<strong>用linkingalgorithm来连接frame-level bounding box into spatio-temporal actiontubes</strong>；有人用dynamic programming approach来连接 resultingper-frame detection，这个costfunction是基于boxes的检测分数和连续帧之间的重叠；也有人用tracking-by-detection方法来代替linkingalgorithm；另外一组是依赖于actionness measure, 例如pixel-wiseprobability of containing any action.为了估计actionness，它们用low-level cues例如光流；通过thresholding theactionness scores来抽取action tubes，这个输出是action的roughlocalization。这些方法的主要的缺点是没有完全利用视频的时序信息，检测是在每一帧上独立做的，<strong>有效的时序建模是很重要，因为当时序上下文信息是可用的时候，大量的actions才是可识别的。</strong></li><li><strong>Clip-level action detection</strong>: 通过在cliplevel执行action detection来利用时序信息。Kalogeiton提出了action tubeletdetector(ACT-detector)，输入为一系列的frames，输出action来别和回归的tubelets：系列的带有associatedscores的bounding box。tubelets被连接，来构造actiontubes。Gu等人进一步通过用longer clips和利用I3D pre-trained on thelarge-scale video dataset，展示了时序信息的重要性。为了生成actionproposals，把2D region proposals扩展到3D，假定spatialextent在一个clip内是固定的。随着时间的推移，用较大空间唯一的actiontubes将要违反假设，特别是当clip是很长，涉及actors或者camera的快速移动。</li><li><strong>modeling spatio-temporal dependencies</strong>:理解人类行为要求理解它们身边的人和物体。一些方法用图结构的网络、注意力机制来汇集视频中的物体和人的上下文信息。时空关系通过多层图结构的自注意力来学习，这个能够连接连续clips的entities，因此考虑long-rangespatial and temporal dependencies。</li><li>Metrics for Spatio-temporal action detection:<strong>frame-AP</strong>: measures the area under the precision-recallcurve of the detections for each frame. frame中的IoU大于某个阈值且actionlabel是正确的，则detection 是正确的。<strong>video-AP</strong>: measuresthe area under the precision-recall curve of the action tubespredictions。如果整个视频帧中，mean per frame IoU大于某个阈值且actionlabel预测正确，则tube 是正确的.</li></ul></li></ol><h4id="a-survey-on-deep-learning-based-spatio-temporal-action-detection"><strong>ASurvey on Deep Learning-based Spatio-temporal ActionDetection</strong></h4><blockquote><p>作者是来自浙大和蚂蚁集团的Peng Wang, Fanwei Zeng和YuntaoQian，论文引用[2]:Wang, Peng et al. “A Survey on Deep Learning-basedSpatio-temporal Action Detection.” ArXiv abs/2308.01618 (2023): n.pag.</p></blockquote><ol type="1"><li><p>Spatio-temporal action detection(STAD)旨在对视频中出现的行为进行分类，然后在空间和时间上进行定位。传统的STAD方式涉及到了滑动窗口，例如deformablepart models, branch and bound approach.模型主要划分为2类：frame-level和clip-level；<em>frame-level</em>预测 2Dbounding box for a frame; <em>clip-level</em>预测 3D spatio-temporaltubelets for a clip.</p></li><li><p><strong>Frame-level</strong>:目标检测做的很成功，研究人员将目标检测的模型泛化到STAD领域，直接的思路是：把STAD in video视为 2D image检测的集合。具体地说，在每一帧上用action detector来检测得到<strong>frame-level 2D bounding box</strong>。然后用<strong>linking ortracking算法</strong>关联这些frame-level detection results，生成<strong>3D action proposals</strong>。作者从<strong>Temporalcontext</strong>、<strong>3D CNN</strong>、<strong>High efficiency andreal-time speed</strong>、<strong>Visual RelationModeling</strong>这几个角度给出了相关的算法。</p><p>有些借鉴了RCNN、FasterRCNN的思路，用了RPN网络，然后用两个分支分别处理RGB和光流；然后融合外观和运动信息，Linkedup里得到 class-specific action tubes. 也有基于actionnessmaps的方法。<strong>actionness是指在图像的特定位置包含一般的actioninstance的可能性</strong>。上述这些STAD方法独立的对待frame，忽视了时序上下文关系。为了克服这个问题，有人提出了<strong>cascadeproposal and location anticipationmodel(CPLA)</strong>的方法，能够推理发生在两帧之间的运动趋势。用<em>frame <span class="math inline">\(I_t\)</span> 上检测到的bbox来推理<span class="math inline">\(I_{t+k}\)</span> frame上对应的 bbox。<spanclass="math inline">\(k\)</span>是 anticipation gap</em>.除了通过光流来获取视频里的运动特性之外，可以用 <font color=red> 3D CNN</font>来从多个相邻帧提取运动信息。后续的还有用<strong>X3D</strong>网络、<strong>ACDnet</strong>、将<strong>光流和RGB</strong>嵌入到一个单流网络中，利用光流来modulateRGB特征、用<strong>SSD</strong>作为检测器、借鉴YOLO的<strong>YOWO</strong>：3DCNN来提取时空信息，2Dmodel来提取空间信息、<strong>WOO</strong>：单个统一的网络，只用一个backbone来做actorlocalization和actionclassification、<strong>SE-STAD</strong>用FCOS作为目标检测器、<strong>EVAD</strong>用ViTs，通过dropout non-kkeyframe tokens减小计算开销，refine scenencontext来增强模型性能。</p></li><li><p>frame-level的方法没有完全利用时序的信息，将视频帧视为独立的图像，因此提出了clip-level的STAD方法，将一系列的frames作为输入，直接输出检测到的<font color=red>tubelet proposals(short sequence of bounding boxes)</font></p></li><li><p><strong>Clip-level</strong>: 输入一个video clip，模型输出一个3Dspatio-temporal tubelet proposals。3D tubelet proposals是由一系列的bboxes that tightly bound the actions of interest形成。然后这些tubelet proposals在successiveclips连接在一起，形成完整的action tubes。作者从几个<strong>Largemotion</strong>、<strong>Progressive learning</strong>、<strong>Anchorfree</strong>、<strong>Visual RelationModeling</strong>这几个角度给出了相关的算法。为了克服 3D anchors的 fixedspatial exntet的问题，有人提出了 <font color=red> two-framemicor-tubes</font>的方法。为了避免3D cuboid anchor，也有人提出了<font color=red> 通过frame-level actor detection，然后将detectedbboxes连接起来形成class-independent action tubelets,然后给到temporalunderstanding module来做行为分类 </font>。还有<strong>sparse-to-dense</strong>的方法。在<em>progressivelearning</em>方面，通过progressive learning 方法，反复修正proposalstowards actions over a fewsteps。有人提出了<strong>PCSC</strong>框架，以迭代的方式，用一个stream(RGB/Flow)里的regionproposals和features来帮助另一个stream(RGB/Flow)提高action localizationresults。计算anchor是一个比较费劲的事情，提出了一些<strong>anchor-free</strong>的方法：有人把每个actioninstance 视为moving points的轨迹。</p><ul><li><strong>MovingCenterDetector(MOC-detector)</strong>，它由3个branches组成：center-branch forinstance center detection and action recognition; movement branch formovement estimation;box branch for spatial extent detection.</li><li><strong>VideoCapsuleNet</strong>：用3D conv along withcapsules来学习必要的语义信息 for action detection and recognition。有一个定位的component，利用capsules得到的action representation for apixel-wise localization of actions.</li><li><strong>TubeR</strong>：直接检测视频里的actiontubelet，同时执行action localization和recognition from a singlerepresentation. 设计了一个tubelet-attention moduel 来model dynamicspatio-temporal nature of a video clip. TubeR学习了tubeletqueries的集合，输出actio tubelets.</li></ul></li></ol><p>在<strong>Visual Relation Modeling</strong>方面，clip-level 的visualrelations也被探索了，来增强STAD模型；有人提出了 long short-term relationnetwork(LSTR)，获取short-term 和long-term relations in videos。具体地说： LSTR先产生3D bboxes(tubelets) in eachvideo。然后通过<strong>spatio-temporal attention mechanism</strong> ineach clip 来建模human-context interactions。推理long-term temporaldynamics across video clips via <em>graph ConvNet in a cascadedmanner</em>。 actor tubelets和objectproposals的特征然后被用于构建关系图，建模human-object manipulations andhuman-human interaction actions。</p><ol start="4" type="1"><li><p><strong>Linking up the Detection Results</strong>:actions会持续一段时间，通常跨很多帧和clips。在frame-level或者clip-level检测结果得到之后，很多方法用一个<strong>linkingalgorithm</strong>来detections across frames orclips连接起来，形成video-level的action tubes。</p><ul><li><strong>linking up frame-level detection boxes</strong>：第一个frame-level action detection linking算法是由<strong>Gkioxari</strong>提出的，他们假设两个相邻regionproposals(bboxes)的空间范围有很好的重叠，且scores很高，有很大的可能性belinked。计算两个region proposals的linking score的公式为： <spanclass="math display">\[s_c(R_t,R_{t+1})=s_c(R_t)+s_c(R_{t+1})+\lambda\cdotov(R_t,R_{t+1}), Eq.(1)\]</span></li></ul><p><span class="math inline">\(s_c\)</span>(R_i)是region proposalR_i的class specific score，<spanclass="math inline">\(ov(R_i,R_j)\)</span> 是<spanclass="math inline">\(R_i\)</span>和<spanclass="math inline">\(R_j\)</span>的 IoU(overlap)。<spanclass="math inline">\(\lambda\)</span>是一个超参数，对IoU项进行加权，有些模型输出的bbox是带有actionnessscores，这里就用actionness scores代替class-specificscores。计算出所有的linking scores之后，<font color=red>最优的path</font>通过这个来搜索： <span class="math display">\[\barR_c^*=\underset{\barR}{\text{argmax}}\frac{1}{T}\sum_{t=1}^{T-1}s_c(R_t,R_{t+1}),Eq.(2)\]</span></p><p><span class="math inline">\(\bar{R}_{c} =[R_{1},R_{2},\ldots,R_{T}]\)</span> 是action class <spanclass="math inline">\(c\)</span>的一系列的linked region.通过维特比算法来解这个优化问题。找到最有的path之后，region proposals in<span class="math inline">\(\bar{R}_{c}\)</span> 从 set of regionproposals中去掉，然后再继续解该方程，直到set of regionproposals是空的。从Eq.(2)中计算得到的path被称为 <font color=red> actiontube</font>。 action tube <spanclass="math inline">\(\bar{R}_{c}\)</span> 定义为：<spanclass="math inline">\(S_{c}(\bar{R}_{c})=\frac1T\sum_{t=1}^{T-1}s_{c}(R_{t},R_{t+1}).\)</span></p><ul><li>基于Gkioxari的思路，Peng等人提出了在 Eq.(1)增加一个阈值函数，linkingscore between two region proposals变成了： <spanclass="math display">\[s_{c}(R_{t},R_{t+1})=s_{c}(R_{t})+s_{c}(R_{t+1})+\lambda\cdotov(R_{t},R_{t+1})\cdot\psi(ov)\]</span></li></ul><p><span class="math inline">\(\psi(ov)\)</span>是一个阈值函数，当<spanclass="math inline">\(ov\)</span>大于<spanclass="math inline">\(\tau\)</span>时，<spanclass="math inline">\(\psi(ov)=1\)</span>，否则<spanclass="math inline">\(\psi(ov)=0\)</span>。Peng在实验中发现，有了这个阈值函数，linkingscore比之前更好了更robust了。Kopuklu进一步扩展了这个linkingscore的定义：</p><p><span class="math display">\[\begin{aligned}s_{c}\left(R_{t},R_{t+1}\right)=&amp;\psi(ov)\cdot[s_{c}\left(R_{t}\right)+s_{c}\left(R_{t+1}\right) \\&amp;+\alpha\cdot s_{c}\left(R_{t}\right)\cdot s_{c}\left(R_{t+1}\right)\\&amp;+\beta\cdot ov\left(R_{t},R_{t+1}\right)] ,\end{aligned}\]</span></p><p>其中<span class="math inline">\(\alpha\)</span>和<spanclass="math inline">\(\beta\)</span>是超参数，<spanclass="math inline">\(\alpha \cdot s_c(R_t)\cdots_c(R_{t+1})\)</span>项将两个连续帧之间的dramatic change考虑进去，提高video detection的性能</p><ul><li>在<strong>Temporal trimming</strong>中，上述的linking算法得到了action tubes 横跨整个video duration，然而，human actions通常只占很小一部分。为了决定一个actioninstance的<strong>时间范围</strong>。有一些temporaltrimming的工作。Saha限制了consecutive proposals来得到smooth actionnessscores。通过动态规划解一个energymaximization的问题。Peng等人依赖一个高校的maximum subarray方法：给定一个video-level action tube <spanclass="math inline">\(\bar{R}\)</span>, 它的理想的时间范围是从frame<span class="math inline">\(s\)</span> to frame <spanclass="math inline">\(e\)</span>，满足下列公式：</li></ul><p><spanclass="math display">\[s_{c}(\bar{R}_{(s,e)}^{\star})=\underset{(s,e)}{\operatorname*{argmax}}\{\frac{1}{L_{(s,e)}}\sum_{i=s}^{e}s_{c}(R_{i})-\lambda\frac{|L_{(s,e)}-L_{c}|}{L_{c}}\},\]</span></p><p><span class="math inline">\(L_{(s,e)}\)</span>是 action tube的长度，<span class="math inline">\(L_c\)</span>是 class <spanclass="math inline">\(c\)</span>在训练集中的平均时长。</p><ul><li><p>在<strong>online action tubegeneration</strong>中，在视频的第一帧，用 <spanclass="math inline">\(n\)</span>个detected bboxes来初始化 <spanclass="math inline">\(n\)</span> action tubes for each class <spanclass="math inline">\(c\)</span>。然后，actiontubes通过增加frame中的box扩大或者在<spanclass="math inline">\(k\)</span>个连续帧之后没有匹配的boxes，就会终结。最后，每个更新的tube通过执行binarylabeling using a online Viterbi算法来进行temporally trimmed。</p></li><li><p><strong>Linking up Clip-level DetectionResults</strong>:clip-level tubelet linking算法旨在 <strong>associate asequence of clip-level tubelets into video-level actiontubes</strong>。它们通常是从frame-levelbox中得到。一个tubelet内的内容应该获取一个action，在任何两个连续的clips连接的tubelets应该有一个大的<font color=red> temporal overlap</font>，因此，他们定义tubelet's的linking score是这样的： <spanclass="math display">\[S=\frac{1}{m}\sum_{i=1}^{m}Actionness_{i}+\frac{1}{m-1}\sum_{j=1}^{m-1}Overlap_{j,j+1}\]</span></p></li></ul><p><span class="math inline">\(Actionness_i\)</span> 表示 第<spanclass="math inline">\(i\)</span>个clip的tubelet的actionness score。<spanclass="math inline">\(Overlap_{j,j+1}\)</span>表示来自第<spanclass="math inline">\(j\)</span>和第<spanclass="math inline">\(j+1\)</span>个clip的两个proposals的Overlap。<spanclass="math inline">\(m\)</span>是videoclips的总数，两个tubelets之间的overlap是基于<strong>第<spanclass="math inline">\(j\)</span>个tubelet的最后一帧和<spanclass="math inline">\(j+1\)</span>个tubelet的第一帧</strong>来计算的。在计算出了tubelets'分数之后；另外，有人把frame linking的算法扩展到tubelet linking来构建action tubes。核心的idea是这样的： -初始化：在视频的第一帧，对每个tubelet开始一个new link，这里a link 指asequence of linked tubelets - Linking: 给定一个new frame <spanclass="math inline">\(f\)</span>，扩展存在的links with one o the tubeletcandidates starting at this frame. 选择tubeletcandidate的标准如下：没有被其它links选择；有最高的actionscore；和要被扩展的link的overlap高于给定的阈值。 -终止：对于一个存在的Link，如果这个标注在<spanclass="math inline">\(K\)</span>个连续帧之后没有被满足，这个link就会终止，<spanclass="math inline">\(K\)</span>是一个给定的超参数。</p><p>由于它的简单和高效，tubelet linking算法被很多最近的工作采用。</p><p>在<strong>temporal trimming</strong>方面。tubelet linking算法，初始和终止决定了actiontubes的时间范围，有人发现它不能彻底的解决transition state产生的temporallocation error。定义为ambiguous states，但不属于targetactions。为了解决这个问题，有人提出了transition-awareclassifier：能够区分transitional states和realactions；后续也有人通过引入一个action switch regressionhead：决定一个boxprediction是否描述了一个执行actions的actor。这个regressionhead给出了一个tubelet每个bbox的action switchscore。如果这个score高于给定的阈值，这个box就包含这个action。这个actionswitch regression head能够有效减小transitional states的误分类。</p></li><li><p>数据集：STAD中经常用到的数据集有：</p><ul><li><strong>Weizmann</strong>：在一个统一的背景中用一个静态相机记录，包含90个videoclips grouped 10 action classes, performed by 9 diffferentsubjects，每个video clip 包含多个单一行为的实例，空间分辨率为$180$，每个clip是从1-5s。</li><li><strong>CMU Crowded Videos</strong>：包含5个actions，每个action有5个training videos和48个testvideos。所有的video被缩放，空间分辨率为<span class="math inline">\(120\times 160\)</span>，testvideos是5-37秒(166-1115帧)，这个数据集是在一个凌乱的和动态的环境中记录的，以至于这个数据集上的actiondetection更加有挑战性。数据集是denselyannotated，提供时间和空间坐标(x,y,height,width,start,end andframes)。</li><li><strong>MSR Action I andII</strong>：是微软研究组弄的，II是I的扩展，Action I包含62个actioninstances in 16个video sequences。II包含203 instances in 54videos，每个video包含不同个体执行的多个actions。所有的视频是32-76秒，每个actioninstance的时间和空间标注是提供的。包含3个action 类别。</li><li><strong>J-HMDB</strong>：是joint-annotatedHMDB数据集，HMDB包含5个action categories，每个category包含至少101个视频片段，数据集包含6849个视频片段，分布在51个actioncategories中。J-HMDB包含从HMDB数据集中宣导的21类视频，选择的视频涉及单个任务的动作，每个actionclass有36-55个clips，每个clip包含15-40帧，总共928个clips，每个clip被裁剪了，第一帧和最后一帧对应一个action的开始和终止。frame的分辨率是<span class="math inline">\(320 \times 240\)</span>, frame rate 是30fps。</li><li><strong>UCF Sports</strong>：包含体育领域的10个actions，所有的视频包含相机运动和复杂背景，包含150个clips，每个clip的framerate是10 fps，空间分辨率是 <span class="math inline">\(480 \times360\)</span> to <span class="math inline">\(720 \times576\)</span>，持续时间是 2.2 - 14.4秒，平均6.39秒。</li><li><strong>UCF101-24</strong>：UCF101的数据来自Youtube，包含101行为类别，总共13320个视频。对于行为检测任务，包含24个行为类别的3207个视频子集提供了密集标注，这个子集称为UCF101-24，不同于UCFSports和J-HMDB，视频是被剪过的，UCF101-24是没有被剪过的。</li><li><strong>THUMOS andMultiTHUMOS</strong>：THMOS系列数据集包含4个数据集：THUMOS13，THUMOS14,THUMOS15和MultiTHUMOS，所有的视频是来自UCF101，THUMOS数据集包含24个actionclasses，视频的时长从几秒到几分钟不等。数据集包含13000个被剪过的视频，超过1000个没有剪过的视频，超过2500个negativesamplevideo。这些视频可能包含none、one、或者单个行为或者多个行为的实例。MultiTHUMOS是一个THUMOS的增强的版本，是一个dense、multi-class、frame-wiselabeled video dataset with 400 videos of 30hours和65个类别的38690个标注。平均每帧有1.5个标注，每个视频10.5个行为类别。</li><li><strong>AVA</strong>：来自Youtube的430个movies，每个movie提供了第15到30分钟的这个clip，每个clip分成897个重叠3s的segmentswith a stride of 1second。对于每个segment，中间帧被选为keyframe，在每个keyframe，每个人都用bbox和actions标注，430个movies分成235个training，64个validation和131test movies，差不多是55:15:30的比例，包含80个原子行为，60个actions用来evaluation。</li><li><strong>MultiSports</strong>：这个视频来自Youtube上奥林匹克和世界杯的竞赛，包含4个运动，66个行为类别，每个运动800个clips，共3200个clips；包含37701个actioninstances with902k个bboxes，每个行为类别的instance从3个到3477个不等，显示了自然的长尾分布。每个视频被多个行为类别的多个实例标注，视频的平均长度是750帧，每个行为的segment比较短，平均24帧。</li></ul></li><li><p>评估指标：主要是两个：frame mAP和video-mAP</p><ul><li><strong>Frame mAP</strong>: area under the PR curve of bboxdetections at each frame.<strong>如果和GTbbox的IoU大于给定的阈值且actionlabel是正确的，则detection是对的，阈值设为0.5</strong>。Frame-mAP能够独立于linkingstrategy来比较检测精度。</li><li><strong>Video-mAP</strong>：area under PR curve of action tubepredictions。<strong>如果和GT tube的IoU大于给定的阈值且actionlabel是正确的，则tubedetection是对的</strong>，两个tubes之间的IoU被定义为时序上的IoU，<strong>multipliedby the average of the IoU between boxes averaged over all overlappingframes</strong>。 video-mAP的阈值通常设为0.2、0.5、0.75，and0.5:0.95。对应于average video-mAP for thresholds with step 0.05 in thisrange.然而frame-mAP衡量的是单帧里的分类和空间检测的能力，video-mAP能够进一步评估时序检测的能力。</li></ul></li><li><p>未来的方向：</p><ul><li><strong>Lable-efficient learning forSTAD</strong>：STAD需要密集的标注，然而密集的标注是昂贵的。</li><li><strong>Online real-timeSTAD</strong>：STAD有很多的在线的应用，必须基于过去的数据来给出当前帧的预测。这要求模型必须是轻量和高效的，还有很长的路。</li><li><strong>STAD under largemotion</strong>：在真实场景中，很多行为由于fast actordisplacement，camera motion,actions有很大的motion。</li><li><strong>Multimodal learning for STAD</strong>：actionvideo包含多个模态，包括视觉、声音甚至语言，因此，通过多模态学习，有潜力实现比单个模态更好的检测精度。另一方面，actions可以通过多种传感器得到，例如深度相机，红外相机，Lidar等，STAD或者可以从多个模态数据中学到的融合表征受益。</li><li><strong>Diffusion models forSTAD</strong>：扩散模型作为一类生成模型，从 sample in随机分布开始，通过逐步地去噪恢复样本数据。尽管它们属于生成模型，它们对于表征的感知任务(例如目标检测和时序动作定位)，表现有效,输入随机的spatialboxes(temporal proposals)，基于扩散的模型能够精确地产生目标框(actionproposals)，自从STAD视为目标检测和时序动作定位(temporal actionlocation)和结合体，有一些工作展示了利用diffusionmodels来解决STAD任务。</li></ul></li></ol><h4 id="参考链接">参考链接：</h4><ul><li>https://0809zheng.github.io/2021/07/15/stad.html：时空行为检测的比较好的介绍</li></ul><p>一些STAD相关的解释和算法图示：</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/2217f65a3fade532f079dbefe1230b47063417b7/8-Figure7-1.png"alt="Spatio-Temporal Action Detection Task" /><figcaption aria-hidden="true">Spatio-Temporal Action DetectionTask</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/2-Figure1-1.png"alt="Spatio-temporal action detection" /><figcaption aria-hidden="true">Spatio-temporal actiondetection</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/2-Table1-1.png"alt="Comparision" /><figcaption aria-hidden="true">Comparision</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/2-Figure2-1.png"alt="illustration" /><figcaption aria-hidden="true">illustration</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/3-Figure3-1.png"alt="Taxonomy of STAD models" /><figcaption aria-hidden="true">Taxonomy of STAD models</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/3-Figure4-1.png"alt="RPN for STAD" /><figcaption aria-hidden="true">RPN for STAD</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/4-Figure5-1.png"alt="I3D for STAD" /><figcaption aria-hidden="true">I3D for STAD</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/4-Figure6-1.png"alt="YOWO" /><figcaption aria-hidden="true">YOWO</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/5-Figure7-1.png"alt="ARCN" /><figcaption aria-hidden="true">ARCN</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/6-Figure8-1.png"alt="CFAD" /><figcaption aria-hidden="true">CFAD</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/10-Table2-1.png"alt="STAD datasets" /><figcaption aria-hidden="true">STAD datasets</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/12-Table3-1.png"alt="Performance on J-HMDB and UCF101-24" /><figcaption aria-hidden="true">Performance on J-HMDB andUCF101-24</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/13-Table4-1.png"alt="Performance on UCF Sports and MultiSports datasets" /><figcaption aria-hidden="true">Performance on UCF Sports and MultiSportsdatasets</figcaption></figure><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/13-Table5-1.png"alt="Performance on AVA" /><figcaption aria-hidden="true">Performance on AVA</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;h4
id=&quot;视频理解及分析的计算机视觉任务&quot;&gt;视频理解及分析的计算机视觉任务&lt;/h4&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;之前看的时候，不管是论文还是一些博客，感觉都不是很清晰和全面，大家的定义不全面，特别是英文的名称上，这里写一下我的理解：&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;几个任务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;行为识别(Action Recognition)&lt;/strong&gt;:
实质是对视频的分类任务，可以类别图像领域的分类任务&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时序动作定位(Temporal Action Localization)&lt;/strong&gt;:
在时间上对视频进行分类，给出动作的起止时间和类别&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时空行为检测(Spatio-Temporal Action Detection)&lt;/strong&gt;:
不仅识别出动作出现的&lt;strong&gt;区间&lt;/strong&gt;和&lt;strong&gt;类别&lt;/strong&gt;，还要在空间范围内用一个bounding
box标记处目标的&lt;strong&gt;位置&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;还有人提出了时空动作定位(Spatio-temporal Action
localization)&lt;/strong&gt;：和上一个是一样的&lt;/li&gt;
&lt;li&gt;Action Detection在Paperswithcode上的定义： aims to find both where
and when an action occurs within a video clip and classify what the
action is taking place. Typically results are given in the form of
&lt;strong&gt;action tublets&lt;/strong&gt;, which are action bounding boxes linked
across time in the video. &lt;em&gt;This is related to temporal localization,
which seeks to identify the start and end frame of an action, and action
recognition, which seeks only to classify which action is taking place
and typically assumes a trimmed video.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;论文里还提到了&lt;strong&gt;temporal action segmentation&lt;/strong&gt;：
针对细粒度的actions和videos with dense occurrence of actions to predict
action label labels at every frame of the video.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;时空行为检测的算法：之前的论文都是都是基于行为识别(action
recognition)的，很多都是基于早期的Slowfast的那个检测的方式：需要一个额外的检测器，实现行为检测。也就是在行为识别的基础上，再进行时空行为检测。但这并不是我理想中的方式，所以很多行为识别的算法，在AVA上也能上榜；最近看VideoMAE看了之后，就一直在看这个，没有去看看其它的。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Action Detection数据集：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;J-HMDB&lt;/li&gt;
&lt;li&gt;UCF101-24&lt;/li&gt;
&lt;li&gt;MultiSports&lt;/li&gt;
&lt;li&gt;AVA&lt;/li&gt;
&lt;li&gt;其中，JHMDB和UCF101-24是密集标注数据集(每一帧都标注，25fps)，这类数据集每个视频只有一个动作，大部分视频是单人做一些语义简单的重复动作；AVA为代表的稀疏标注数据集(隔一段时间标注一帧，1fps)，没有给出明确的动作边界&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Video Understanding" scheme="https://young-eng.github.io/YoungBlogs/tags/Video-Understanding/"/>
    
  </entry>
  
  <entry>
    <title>ollama 大模型部署</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/19/ollama-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/19/ollama-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/</id>
    <published>2024-08-19T13:22:22.000Z</published>
    <updated>2024-08-20T02:49:57.242Z</updated>
    
    <content type="html"><![CDATA[<h3id="记录一下用ollama和openwebui来部署几个大模型qwen2llama3和llava">记录一下用ollama和openwebui来部署几个大模型：Qwen2、LLaMa3和LLaVa</h3><h4 id="安装ollama-及pull-model">安装Ollama 及pull model</h4><ol type="1"><li><p>去ollama的官网下载安装ollama</p></li><li><p>更改变量：windows中添加环境变量: OLLAMA_MODELS:XXXXpath，linux需要到systemd中找到ollama的哪个文件，然后进行修改，这样ollamapull 模型的时候，就会安装到指定的路径</p></li><li><p>ollama安装完成后，可以用ollama pullqwen2:7b这样来下载模型，也可以下载模型的GGUF文件，然后需要写一个配置文件，如config.txt，内容如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">FROM &quot;path/to/llama3-8b-cn-q6/Llama3-8B-Chinese-Chat.q6_k.GGUF&quot;</span><br><span class="line"></span><br><span class="line">TEMPLATE &quot;&quot;&quot;&#123;&#123;- if .System &#125;&#125;</span><br><span class="line">&lt;|im_start|&gt;system &#123;&#123; .System &#125;&#125;&lt;|im_end|&gt;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">&#123;&#123; .Prompt &#125;&#125;&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">SYSTEM &quot;&quot;&quot;&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">PARAMETER stop &lt;|im_start|&gt;</span><br><span class="line">PARAMETER stop &lt;|im_end|&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p></li></ol><span id="more"></span><p>这个From后面的路径需要修改，然后使用<code>ollama create llama3-cn -f ./config.txt</code>导入模型，导入成功后，可以用<code>ollama list</code>查看，使用<code>ollama run xxx</code>运行，使用<code>/exit</code>退出</p><h4 id="openwebui">OpenWebUI</h4><ol type="1"><li><p>运行之前需要先安装docker</p></li><li><p>运行命令:<code>docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</code>，通过Docker运行OpenWebUI，这里输入<code>localhost:3000</code>就能访问，如果没有看到模型，需要修改<code>OLLAMA_HOSTS=0.0.0.0</code>，然后重新启动ollama的服务</p></li><li><p>docker的相关命令：</p><ul><li><p><code>docker ps</code>：查看正在运行的docker容器</p></li><li><p><code>docker kill container_id</code>：杀死正在运行的docker容器</p></li><li><p><code>docker rm container_id</code>：删除已经停止的docker容器</p></li></ul></li></ol><h4id="llava及相关视觉语言模型vlm的微调">LLaVa及相关视觉语言模型(VLM)的微调</h4><h4 id="参考链接">参考链接</h4><ul><li><p>https://www.cnblogs.com/obullxl/p/18295202/NTopic2024071001</p></li><li><p>https://blog.csdn.net/u010522887/article/details/140651584</p></li><li><p>https://liaoxuefeng.com/blogs/all/2024-05-06-llama3/index.html</p></li><li><p>https://www.cnblogs.com/obullxl/p/18295202/NTopic2024071001</p></li><li><p>https://github.com/qianniucity/ollama-doc/blob/main/ollama/docs/ollama%20%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E7%AD%94.md</p></li><li><p>https://www.cnblogs.com/ting1/p/18358286</p></li><li><p>https://github.com/open-webui/open-webui</p></li><li><p>https://github.com/ollama/ollama/blob/main/docs/faq.md</p></li><li><p>https://blog.csdn.net/joeyoj/article/details/136427362</p></li><li><p>https://blog.csdn.net/2401_85328934/article/details/139749167(用lobe chat代替openWebUI)</p></li><li><p>https://cuterwrite.top/p/ollama/ (用Continue 插件部署自己的 codecopliot)</p></li><li><p>https://mp.weixin.qq.com/s/vt1EXVWtwm6ltZVYtB4-Tg</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;记录一下用ollama和openwebui来部署几个大模型qwen2llama3和llava&quot;&gt;记录一下用ollama和openwebui来部署几个大模型：Qwen2、LLaMa3和LLaVa&lt;/h3&gt;
&lt;h4 id=&quot;安装ollama-及pull-model&quot;&gt;安装Ollama 及pull model&lt;/h4&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;去ollama的官网下载安装ollama&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;更改变量：windows中添加环境变量: OLLAMA_MODELS:
XXXXpath，linux需要到systemd中找到ollama的哪个文件，然后进行修改，这样ollama
pull 模型的时候，就会安装到指定的路径&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ollama安装完成后，可以用ollama pull
qwen2:7b这样来下载模型，也可以下载模型的GGUF文件，然后需要写一个配置文件，如config.txt，内容如下：
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;FROM &amp;quot;path/to/llama3-8b-cn-q6/Llama3-8B-Chinese-Chat.q6_k.GGUF&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;TEMPLATE &amp;quot;&amp;quot;&amp;quot;&amp;#123;&amp;#123;- if .System &amp;#125;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;lt;|im_start|&amp;gt;system &amp;#123;&amp;#123; .System &amp;#125;&amp;#125;&amp;lt;|im_end|&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;#123;- end &amp;#125;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;lt;|im_start|&amp;gt;user&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;#123; .Prompt &amp;#125;&amp;#125;&amp;lt;|im_end|&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;lt;|im_start|&amp;gt;assistant&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SYSTEM &amp;quot;&amp;quot;&amp;quot;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;PARAMETER stop &amp;lt;|im_start|&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;PARAMETER stop &amp;lt;|im_end|&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Tools" scheme="https://young-eng.github.io/YoungBlogs/categories/Tools/"/>
    
    
    <category term="LLM" scheme="https://young-eng.github.io/YoungBlogs/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>强化学习</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/08/14/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/08/14/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</id>
    <published>2024-08-14T09:51:44.000Z</published>
    <updated>2024-08-14T09:51:44.872Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Softwares&#39; installations</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/07/17/Softwares-installations/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/07/17/Softwares-installations/</id>
    <published>2024-07-17T10:05:36.000Z</published>
    <updated>2024-07-17T10:15:22.000Z</updated>
    
    <content type="html"><![CDATA[<h3id="在服务器的个人账户下上安装cuda">在服务器的个人账户下上安装cuda</h3><ol type="1"><li><p>去<ahref="https://developer.nvidia.com/cuda-toolkit-archive">cudaarchive</a>里找到对应的cuda版本的runfile文件，通过shxxx.run来安装</p></li><li><p>安装的时候，需要去Options里更改toolkit和library的path，设置完后即可进行install</p></li><li><p>install完了之后，需要去bashrc里添加以下内容：</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="string">&quot;/xxx/cuda/bin:<span class="variable">$PATH</span>&quot;</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="string">&quot;/xxx/cuda/lib64:<span class="variable">$LD_LIBRARY_PATH</span>&quot;</span></span><br></pre></td></tr></table></figure><ol start="4" type="1"><li><p>然后source一下bashrc，再nvcc-V，如果显示版本号，则说明安装成功</p></li><li><p>遇到需要本地cuda的时候，可以</p></li></ol><p><code>export CUDA_HOME=="/xxx/cuda/"</code></p><span id="more"></span><h3 id="参考链接">参考链接</h3><ul><li>https://blog.csdn.net/qq_35082030/article/details/110387800</li><li>https://blog.csdn.net/sdbyp/article/details/139853774</li><li>https://blog.csdn.net/Sihang_Xie/article/details/127347139</li></ul>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;在服务器的个人账户下上安装cuda&quot;&gt;在服务器的个人账户下上安装cuda&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;去&lt;a
href=&quot;https://developer.nvidia.com/cuda-toolkit-archive&quot;&gt;cuda
archive&lt;/a&gt;里找到对应的cuda版本的runfile文件，通过sh
xxx.run来安装&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;安装的时候，需要去Options里更改toolkit和library的path，设置完后即可进行install&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;install完了之后，需要去bashrc里添加以下内容：&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; PATH=&lt;span class=&quot;string&quot;&gt;&amp;quot;/xxx/cuda/bin:&lt;span class=&quot;variable&quot;&gt;$PATH&lt;/span&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; LD_LIBRARY_PATH=&lt;span class=&quot;string&quot;&gt;&amp;quot;/xxx/cuda/lib64:&lt;span class=&quot;variable&quot;&gt;$LD_LIBRARY_PATH&lt;/span&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ol start=&quot;4&quot; type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;然后source一下bashrc，再nvcc
-V，如果显示版本号，则说明安装成功&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;遇到需要本地cuda的时候，可以&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;export CUDA_HOME==&quot;/xxx/cuda/&quot;&lt;/code&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="methods" scheme="https://young-eng.github.io/YoungBlogs/categories/methods/"/>
    
    
  </entry>
  
  <entry>
    <title>绘图</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/07/01/%E7%BB%98%E5%9B%BE/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/07/01/%E7%BB%98%E5%9B%BE/</id>
    <published>2024-07-01T07:54:56.000Z</published>
    <updated>2024-07-01T07:58:44.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="matplotlib-绘图">Matplotlib 绘图</h3><ol type="1"><li>3D plot时候的需要注意的地方</li></ol><p>https://www.codenong.com/48442713/</p><p>https://www.coder.work/article/2032713#google_vignette</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;matplotlib-绘图&quot;&gt;Matplotlib 绘图&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;3D plot时候的需要注意的地方&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;https://www.codenong.com/48442713/&lt;/p&gt;
&lt;p&gt;h</summary>
      
    
    
    
    <category term="methods" scheme="https://young-eng.github.io/YoungBlogs/categories/methods/"/>
    
    
    <category term="tips" scheme="https://young-eng.github.io/YoungBlogs/tags/tips/"/>
    
  </entry>
  
  <entry>
    <title>大模型综述</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/06/29/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/06/29/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/</id>
    <published>2024-06-29T05:41:52.000Z</published>
    <updated>2024-06-29T05:41:54.000Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Object Tracking Survey</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/06/03/Object-Tracking-Survey/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/06/03/Object-Tracking-Survey/</id>
    <published>2024-06-03T12:41:20.000Z</published>
    <updated>2024-06-03T12:42:50.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="visual-objec-trackingvot">Visual Objec Tracking(VOT)</h3><h3 id="multiple-object-trackingmot">Multiple Object Tracking(MOT)</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;visual-objec-trackingvot&quot;&gt;Visual Objec Tracking(VOT)&lt;/h3&gt;
&lt;h3 id=&quot;multiple-object-trackingmot&quot;&gt;Multiple Object Tracking(MOT)&lt;/h3&gt;
</summary>
      
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Object Tracking" scheme="https://young-eng.github.io/YoungBlogs/tags/Object-Tracking/"/>
    
  </entry>
  
  <entry>
    <title>OC-SORT</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/06/02/OC-SORT/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/06/02/OC-SORT/</id>
    <published>2024-06-02T07:30:25.000Z</published>
    <updated>2024-06-20T13:25:52.000Z</updated>
    
    <content type="html"><![CDATA[<h3id="observation-centric-sort-rethinking-sort-for-robust-multi-object-tracking1">Observation-CentricSORT: Rethinking SORT for Robust Multi-ObjectTracking<sup>[1]</sup></h3><blockquote><p>作者是来自CMU、上海AI Lab和英伟达的Junkun Cao, Jiangmiao Pang,Xinshuo Weng, Rawal Khirodkar, Kris Kitani. 论文引用[1]:Cao, Jinkun etal. “Observation-Centric SORT: Rethinking SORT for Robust Multi-ObjectTracking.” 2023 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) (2022): 9686-9696.</p></blockquote><h3 id="time">Time</h3><ul><li>2023.Feb</li></ul><h3 id="key-words">Key Words</h3><ul><li>object observations</li><li><strong>O</strong>bservation-<strong>C</strong>entric<strong>SORT</strong>, Simple，Online, and Real-Time</li><li>occlusion and non-linear object motion</li></ul><span id="more"></span><h3 id="动机">动机</h3><ol type="1"><li><p>基于卡尔曼滤波的多目标追踪的方法是假设目标线性运动，这个假设对于很短的occlusion是可接受的，时间变长了的话，运动的线性估计会很不准确。Moreover，当这里没有可用的measurement来更新卡尔曼滤波的参数时，惯例是相信prioristate estimations for posterioriupdate。这会导致occlusion期间的误差累积，这个误差会在实践中造成严重的运动方向的多变。</p></li><li><p>旨在开发一个motion model-based multi-objecttracking(MOT)，对于occlusion和non-linear motion有很好的鲁棒性。</p></li><li><p>当occlusion和non-linear motion同时发生的时候，motion model-basedtracking方法的误差会发生。</p></li></ol><h3 id="总结">总结</h3><ol type="1"><li><p>基于卡尔曼滤波的多目标追踪的方法是假设目标线性运动，这个假设对于很短的occlusion是可接受的，时间变长了的话，运动的线性估计会很不准确。Moreover，当这里没有可用的measurement来更新卡尔曼滤波的参数时，惯例是相信prioristate estimations for posterioriupdate。这会导致occlusion期间的误差累积，这个误差会在实践中造成严重的运动方向的多变。本文中，作者展示了一个卡尔曼滤波当很好地处理了occlusion期间的噪声累加时，也能得到很好的效果。除了仅仅依赖于线性状态估计(estimation-centricapproach)，用object observations(measurements by objectdetector)来计算一个虚拟的轨迹 over occlusion period，来修复occlusion期间的滤波器的误差累加，提高了occlusion和non-linearmotion期间的robustness。取名为<strong>Observation-CentricSORT</strong>，</p></li><li><p>目前的现存的motion model-based算法，假设一个时间段内追踪的物体速度不变，称之为线性运动假设，这个假设在许多实际场景中不成立。但是时间间隔足够小的时候，仍然有效，物体的motion能够近似为线性。为了减小造成的副作用，重新思考当前的模型，然后认识到了一些局限。为了追踪的性能，提出解决的方法：<strong>especiallyin occlusion</strong>。</p></li><li><p>基于滤波器的方法(filtering-based)作为motion model-basedtracking，假设一个转换函数，来预测未来时刻的目标的状态，称之为 stateestimation；除了estimations，利用一个observationmodel，例如一个目标检测器，来推导一个目标物体的statemeasurements，也称之为observations。observation通常作为一个附属的信息来帮助更新滤波器的后验参数。轨迹能够被stateestimation来扩展。<strong>Among this line ofwork</strong>，用的最广的是SORT，用一个Kalmanfilter来估计目标的state，一个linear motion function最为<strong>trasitionfunction between time steps</strong>， 然而，当objectmotion是非线性、更新滤波器的后验参数的时候没有观察到observation，SORT的鲁棒性不够。</p></li><li><p>SORT的三个<strong>limitations</strong>；</p><ul><li>尽管高帧率能够近似物体的运动为线性的，它也放大了模型对于noise ofstateestimations的sensitivity。特别是，在高帧率视频的连续帧之间，物体位移的噪声可能与实际物体位移的大小相同，导致KF估计的物体速度有很大的方差。速度估计的噪声会通过transitionprocess累计进入到位置估计(position estimate)。</li><li>KF状态估计的噪声会随着时间累加，当在KF更新的阶段没有observation的时候。作者展示了相对于没有追踪到目标物体的时间，误差累加会非常快，在速度方向上的噪声的影响经常使得<strong>tracklost</strong>即使在re-association之后。</li><li>考虑到现在检测器的发展，object state by detections相比于滤波器里的固定的trainsition function得到的stateestimation，有很低的方差。然而，<strong>SORT是通过 state estimation而不是 observations来prolong object trajectories</strong>。</li></ul></li><li><p>为了减小上述限制的负面影响，两点创新：</p><ul><li><p>设计了一个module来用object state observations来减小track丢失的时候的累计的误差；准确地说，在传统的 <em>predict 和update</em>阶段之外，在一段untrack的时间之后，通过关联到一个observation，track被重新激活，trigger <em>re-update</em>，<em>re-update</em> 用 <strong>virtualobservation</strong> on the historical time steps to prevent erroraccumulation。 virtual observations 来自 a trajectory generated usingthe last-seen observatio before untracked and the latest observationre-activating this track as anchors. 称之为 <strong>Observation-centricRe-Update</strong>。</p></li><li><p>线性运动的假设提供了目标物体运动的方向，但这个cue很难用在SORTassociation上，因为directionestimation的大量的noise。提出了一个observation-centric方式，将tracks的方向一致性引入到cost matrix 里 for association。称之为<strong>Observation-Centric Momentum</strong></p></li></ul></li><li><p>Motion Models: 大部分的MOT算法用了motion models，这些motionmodels用贝叶斯估计通过最大化后验估计，来预测下一个状态。最为最经典的motionmodels，<strong>Kalman filter(KF)</strong>是一个recursive贝叶斯滤波器，跟随经i的那的 <em>predict-update</em>循环，measurements是来自隐马尔可夫模型的observations。给定一个先行运动假设限制KF，紧接着的一个工作是ExtendedKF和UnscentedKF,提出这两个是为了解决<strong>一阶和二阶泰勒估计的非线性motion</strong>；然而，它们还是依赖于KF假设的近似高斯先验，要求motionpattern assumption。另一个方面，粒子滤波通过sample-based后验估计来解决非线性motion，但是计算量是指数级的。因此，Kalmanfilter的变体和粒子滤波很少在视觉多目标追踪中采用，用的最多的motionmodel还是基于Kalman filter的。</p></li><li><p>Multi-object Tracking:多目标跟踪是从概率的角度进行的，现代的videoobject tracking通常建立在目标检测器上，<strong>SORT采用Kalman filter formotit ion-based multi-object tracking, given observations from deepdetectors</strong>，DeepSORT进一步将深度视觉特征引入object associationunder framework ofSORT，当场景拥挤，基于ReID的目标关联流行起来。最近，transformer被引入到MOT中，来学习深度特征from both visual information and objecttrajectories。然而，它们的性能和SOTA的tracking-by-detection的方法还有很大的差距，不管是精度还是时间效率。</p></li><li><p>Kalman filter是一个在时间域离散的动态系统的线性估计器。KF只要求前一时刻的stateestimations和当前的measurement来估计下一时刻的targetstate。这个滤波器维护两个变量: **后验状态估计 x, 后验估计协方差矩阵 P.在目标追踪任务，用一个状态变换模型F、观测模型H、过程噪声Q和观测噪声R来描述KF过程。在每个时刻t，给定观测z_t， KF在 predict和update两个阶段交替进行：</p><p><spanclass="math display">\[\begin{aligned}&amp;predict\begin{cases}\hat{\mathbf{x}}_{t|t-1}=\mathbf{F}_{t}\hat{\mathbf{x}}_{t-1|t-1}\\\mathbf{P}_{t|t-1}=\mathbf{F}_{t}\mathbf{P}_{t-1|t-1}\mathbf{F}_{t}^{\top}+\mathbf{Q}_{t}\end{cases},\\&amp;update\begin{cases}\mathbf{K}_{t}=\mathbf{P}_{t|t-1}\mathbf{H}_{t}^{\top}(\mathbf{H}_{t}\mathbf{P}_{t|t-1}\mathbf{H}_{t}^{\top}+\mathbf{R}_{t})^{-1}\\\hat{\mathbf{x}}_{t|t}=\hat{\mathbf{x}}_{t|t-1}+\mathbf{K}_{t}(\mathbf{z}_{t}-\mathbf{H}_{t}\hat{\mathbf{x}}_{t|t-1})\\\mathbf{P}_{t|t}=(\mathbf{I}-\mathbf{K}_{t}\mathbf{H}_{t})\mathbf{P}_{t|t-1}\end{cases}.\end{aligned}\]</span></p><ul><li>predict阶段是来推断下一时刻的state estimations。给定下一时刻的targetstates的measurement，update阶段旨在更新KF中的后验参数。因为measurement来自观测模型H，因此在很多场景中成为observation。</li><li>SORT是建立在KF上的MOT, SORT里的KF的状态x定义为 $ x=[u,v,s,r,,,] $,<span class="math inline">\((u,v)\)</span>是image里的目标的center的coordinates, <spanclass="math inline">\((s,r)\)</span>分别是bbox的 scale(area)和 aspectratio。aspect ratio假定为常数。其它的 <spanclass="math inline">\(\dot{u},\dot{v},\dot{s}\)</span>分别是相应的推导量。obervation是一个bbox<span class="math inline">\(z = [u,v,w,h,c]^T， (u,v)是object center,w,h分别为weight和height，c为detection confidence.\)</span>.SORT假设线性运动为过渡模型F，状态估计为： <spanclass="math display">\[u_{t+1}=u_t+\dot{u}_t\Delta t,\quadv_{t+1}=v_t+\dot{v}_t\Delta t.\]</span></li><li>为了利用SORT中的KF for visual MOT，predict阶段估计下一帧的objectposition。来自detection model的observation来更新stage。updatestage是来更新Kalman filter的参数，不直接edit tracking outcomes。</li><li>当两个step的时间差在transition中是常数，例如视频帧率是常数；当帧率很高，SORT表现很好即使objectmotion是非线性的，因为在短的时间内可以近似为线性的。然而实际中<strong>observation通常会缺失，例如目标被occluded</strong>,这种情况下，无法通过update阶段来更新KFparameters。SORT通常用先验的直接作为后验，称之为<strong>dummyupdate</strong>。</li><li><span class="math display">\[ \hat{x}_{t|t} = \hat{x}_{t|t-1},P_{t|t} = P_{t|t-1}\]</span></li></ul><p>称这个算法为<strong>estimation-centric</strong>。当遇到non-linear和occlusion的时候，这个方法就不太行。</p></li><li><p>很小的 <spanclass="math inline">\(\Delta{t}\)</span>会放大噪声。</p></li><li><p>Observation-Centric SORT, we introduce the proposed<em>observation-Centric SORT(OC-SORT)</em>,为了解决上述SORT的限制。用目标进入associtationstage的momentum来开发一个在occulsion和非线性motion时，less noise和morerobustness的pipeline。核心是将tracker设计为<strong>observation-centric</strong>而不是<strong>estimation-centric</strong>。如果一个track从untracked恢复，用一个<em>observation-centric Re-Update</em>来抵消untracked期间的累计错误。OC-SORT也在associationcost中加了Observation-Centric Momentum(OCM)。</p></li><li><p>Observation-Centric Re-Update(ORU):</p><ul><li>当一个track经过一段时间的untrack之后，又与observation关联起来(re-activation)，检查lost期间和重新更新KF的参数。参考untracked开始和结束时的observation，生成virtual trajectory。例如：untrack之前的last-seen observation为 <spanclass="math inline">\(z_t1\)</span>，triggeringre-association的observation为$z_t2，virtual trajectory为： <spanclass="math display">\[\tilde{\mathbf{z}}_t=Traj_{\text{virtual}}(\mathbf{z}_{t_1},\mathbf{z}_{t_2},t),t_1&lt;t&lt;t_2.\]</span>然后接着 run the loop of predict and re-update.</li></ul></li><li><p>Observation-Centric Momentum(OCM):</p><ul><li><p>时间很短的间隔中，将motion近似为线性的，线性motion假设要求持续的motiondirection。但是noise阻止了利用direction的一致性。更准确的说，为了决定motiondirection，需要目标state on two steps with a time difference <spanclass="math inline">\(\delta t\)</span>。如果 <spanclass="math inline">\(\delta t\)</span> 太小了，因为estimation对statenoise敏感，所以velocity noise很重要。如果 <spanclass="math inline">\(\delta t\)</span> 太大了，directionestimation的noise同样也是 significant，因为temporal errormagnification和failure of linear motion assumption。提出用observations而不是 estimations来降低motion directioncalculation的noise，引入consistency来帮助association。</p></li><li><p>给定 <span class="math inline">\(N\)</span>个存在的tracks和 <spanclass="math inline">\(M\)</span>个 detections on the new-coming timestep. Association Cost matrix是： <spanclass="math display">\[C(\hat{\mathbf{X}},\mathbf{Z})=C_{\mathrm{IoU}}(\hat{\mathbf{X}},\mathbf{Z})+\lambdaC_v(\mathcal{Z},\mathbf{Z}),\]</span></p></li></ul><p><span class="math inline">\(\hat{X} \in R^{N \times 7}\)</span> 是object state estimations的集合，<span class="math inline">\(Z \in R^{M\times 5}\)</span>是 observations在new time step的集合。</p></li><li></li></ol><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/643914ae0ad9dcc5868348e4d867374aedbf9531/4-Figure2-1.png"alt="pipleline" /> <span class="math inline">\(Fig.2^{[1]}\)</span>Pipeline of proposed OC-SORT，Red boxes are detections, orange boxes areactive tracks, blue boxes are untracked tracks, dashed boxes are theestimates from KF. During association, OCM is used to add the velocityconsistency cost. The target #1 is lost on the frame t+1 because ofocclusion. But on the next frame, it is recoverd by referring to itsobservation of the frame t by OCR. It being re-tracked triggers ORU fromt to t+2 for the parameters of its KF。</p>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;observation-centric-sort-rethinking-sort-for-robust-multi-object-tracking1&quot;&gt;Observation-Centric
SORT: Rethinking SORT for Robust Multi-Object
Tracking&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自CMU、上海AI Lab和英伟达的Junkun Cao, Jiangmiao Pang,
Xinshuo Weng, Rawal Khirodkar, Kris Kitani. 论文引用[1]:Cao, Jinkun et
al. “Observation-Centric SORT: Rethinking SORT for Robust Multi-Object
Tracking.” 2023 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) (2022): 9686-9696.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2023.Feb&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;object observations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;O&lt;/strong&gt;bservation-&lt;strong&gt;C&lt;/strong&gt;entric
&lt;strong&gt;SORT&lt;/strong&gt;, Simple，Online, and Real-Time&lt;/li&gt;
&lt;li&gt;occlusion and non-linear object motion&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Object Tracking" scheme="https://young-eng.github.io/YoungBlogs/tags/Object-Tracking/"/>
    
  </entry>
  
  <entry>
    <title>OSTrack</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/05/22/OSTrack/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/05/22/OSTrack/</id>
    <published>2024-05-22T14:38:59.000Z</published>
    <updated>2024-05-22T14:39:00.000Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Mixformer</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/05/22/Mixformer/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/05/22/Mixformer/</id>
    <published>2024-05-22T14:33:43.000Z</published>
    <updated>2024-05-30T16:11:26.000Z</updated>
    
    <content type="html"><![CDATA[<h3id="mixformer-end-to-end-tracking-with-iterative-mixed-attention1">MixFormer:End-to-End Tracking with Iterative Mixed Attention<sup>[1]</sup></h3><blockquote><p>作者是来自南大的Yutao Cui, Cheng Jiang, Limin Wang, Gangshan Wu.论文引用[1]:Cui, Yutao et al. “MixFormer: End-to-End Tracking withIterative Mixed Attention.” 2022 IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR) (2022): 13598-13608.</p></blockquote><h3 id="time">Time</h3><ul><li>2022.Mar</li></ul><h3 id="key-words">Key Words</h3><ul><li>compact tracking framework</li><li>unify the feature extraction and target integration solely with atransformer-based architecture</li></ul><h3 id="votmotsot的区别">VOT，MOT,SOT的区别</h3><ol type="1"><li>VOT是标首帧，MOT是多目标追踪，SOT是单目标追踪。</li></ol><h3 id="动机">动机</h3><ol type="1"><li><p>Tracking经常用多阶段的pipeline：<strong>featureextraction，target information integration，bounding boxestimation</strong>。为了简化这个pipeline，作者提出了一个紧凑的tracking框架，名为MixFormer。</p></li><li><p>target information integration解释： fuse the target and searchregion information</p></li></ol><span id="more"></span><h3 id="总结">总结</h3><ol type="1"><li><p>核心设计利用是attentation操作的灵活，提出了Mixed AttentionModule(MAM) for simultaneous feature extraction and target informationintegration. 这个即时的modeling scheme，能够抽取target-specificdiscriminative features以及在target和searcharea之间能够进行大量的communication。基于MAM,通过堆叠MAMs withprogressive patch embedding 和place a localization head ontop，来构建MixFormer追踪框架。</p></li><li><p>为了在onlinetracking的时候处理多个目标的template，在MAM中设计了一个非对称的注意力，来减小计算成本，提出了一个有效的scoreprediction module来选择高质量的template。</p></li><li><p>视觉目标追踪的仍然是有很多挑战，包括：scale variations, objectdeformations, occlusions, confusion from similar objects.</p></li><li><p>之前的流行的trackers通常包括几个部分：</p><ul><li>a backbone 来提取tracking target和 search area 的feature，</li><li>为了后续的target-aware的定位，一个集成的模块(integrationmodule)来使得tracking target和search area进行信息交流</li><li>task-specific heads来精确地定位目标和估计bounding box.</li></ul></li><li><p>集成模块(integration moduel) 是追踪算法的关键，它能够整合targetinformation，在通用特征提取和目标感知定位架起桥梁。传统的集成方法包括correlation-basedoperations(e.g., SiamFC, SiamRPN, CRPN, SiamFC++, 将Simaese网络和correlation操作结合起来，对target和search之间的全局依赖进行建模),online learning 算法(e.g., DCF, KCF, ATOM, FCOT，对于有区别的tracking，学习到独立于目标的model)，最近，多亏了全局和动态的modelingcapacity，引入了transformer来进行基于注意力的integration，有了很好的效果(STMTrack,STARK)，然而，这些基于transformer的trackers仍然基于CNN作为特征提取例如ResNet，仅仅用attention操作 in the latter high-level and abstract representation space.CNN是有限的，它们通常用来pre-trained for 通用目标提取，可能忽略finestructure information for tracking.CNN用局部卷积核，缺乏全局建模能力。</p></li><li><p>为了解决以上问题，在tracking frameworkdesign上，提出了新的思路。通用特征提取和target informationintegration集成到一个统一的框架，coupled processing paradigm有很多好的advantages:</p><ul><li>能够使得feature extraction to be more specific to the correspondingtracking target and capture more target-specific discriminativefeatures.</li><li>能够使得target information to be more extentively 集成到search area,然后更好地capture their correlation</li><li>得到一个更为紧凑的tracking pipelien with a single backbone andtracking head, without an explicit integration module.</li></ul></li><li><p>Attention module是一个非常灵活的架构，building block wiht dynamicand global modeling capacity, 提出了一个mixed attentionmodule(MAM)，能够同时进行特征提取和 mutual interaction of targettemplate and search area。在MAM中，设计了一个混合的interactionscheme，包括自注意力和cross-attention operations on tokens from templateand search area。自注意力能够抽取target or searcharea的自己的特征，然而cross-attention allows for communications betweenthem(search and target area) to mix the target and search areainformations。为了降低MAM的计算成本，能够使得多个templates 来处理obectdeformation，提出了一个定制的非对称的attentionscheme通过修建不必要的target-to-search area cross-attention.</p></li><li><p>MixFormer backone: 通过堆叠Patchembedding和MAM，最后放一个简单的localization head 来得到整个的trackingframework。 为了处理追踪过程中的目标变形，提出了基于target templateupdate机制的score，MixFormer能够容易地适应multiple target templateinputs.</p></li><li><p>最近的trackers引入了基于transformer的integrationmodule，来得到更复杂的依赖(dependencies),实现更好的效果。</p></li><li><p>MAM的输入使target template和searcharea。旨在提取long-range特征，融合它们之间的特征。和原始的MHA不同，MAM执行dualattention operations on two separate tokens sequences of target templateand search area. 对每个sequences上的tokens进行自注意力来得到target orsearch specific information；同时，对来自2个sequencnes的tokens进行crossattention来得到target template and searcharea的交互信息。通过拼接concatenated token sequences来进行mix attentionmechanism.</p></li><li><p>给定一个concatenated tokens of multiple targets andsearch，首先将其分成2个部分，然后reshape成一个2D的featuremap，为了实现additional modeling of local spatial context, 在每个featuremap上用一个可分离的depth-wise convolutional projection layer. target和search的每个feature map 然后通过linear project 来得到queries, keys,values of the attention operation. <span class="math inline">\(q_t, k_t,v_t\)</span>代表target, <span class="math inline">\(q_s, k_s,v_s\)</span>代表search region. mixed attention的定义如下: <spanclass="math display">\[\begin{aligned}&amp;k_{m}=\mathrm{Concat}(k_{t},k_{s}),\quadv_{m}=\mathrm{Concat}(v_{t},v_{s}),\\&amp;\mathrm{Attention}_{\mathrm{t}}=\mathrm{Softmax}(\frac{q_{t}k_{m}^{T}}{\sqrt{d}})v_{m},\\&amp;\mathrm{Attention}_{s}=\mathrm{Softmax}(\frac{q_{s}k_{m}^{T}}{\sqrt{d}})v_{m},\end{aligned}\]</span></p><ul><li><span class="math inline">\(Attention_s\)</span>和<spanclass="math inline">\(Attention_t\)</span>包含了featureextraction和信息融合，最后，targets token 和search token通过一个linearprojection 进行concatenated 和processed。</li></ul></li><li><p>非对称的mixed attention scheme：通过剪掉不必要的target-to-searcharea cross-attention，降低了MAM的计算成本，能够高效地用multipletemplates来处理目标变形的问题。</p><p><spanclass="math display">\[\begin{aligned}&amp;k_{m}=\mathrm{Concat}(k_{t},k_{s}),\quadv_{m}=\mathrm{Concat}(v_{t},v_{s}),\\&amp;\mathrm{Attention}_{\mathrm{t}}=\mathrm{Softmax}(\frac{q_{t}k_{m}^{T}}{\sqrt{d}})v_{m},\\&amp;\mathrm{Attention}_{s}=\mathrm{Softmax}(\frac{q_{s}k_{m}^{T}}{\sqrt{d}})v_{m},\end{aligned}\]</span></p></li><li><p>Corner-based localization head: 受STARK的启发，用了全卷积的cornerbased localization head来估计tracked object的boundingbox。通过计算corner probability distribution，来得到boundingbox。</p></li><li><p>Query based localization head:在最后一个stage的序列中，加了一个可学习的regressiontoken，用这个token作为anchor来aggregate整个目标和搜索区域的信息。最后一个3个fclayers的FFN来回归bounding box。不需要后处理。</p></li><li><p><strong>Training</strong>:用CVT模型对MAM进行预训练，然后再整个目标dataset上进行微调。</p></li><li><p><strong>Template Online Update</strong>:这个再得到时序信息和处理目标形变、外观变化上很重要，在追踪的时候，低质量的templates可能导致较差的追踪性能。引入了一个scoreprediction module(SPM)。SPM由2个attention blocks，three-layerperceptron组成。一个可学习的<em>score token</em>和search ROItokens进行attention，然后scoretoken与初始目标的所有positions进行attention, implicitly compare minedtarget with first target.最后，通过一个MLP和sigmoid来得到score，低于0.5的onlinetemplate被视为negative.</p></li></ol><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/b6eaec7917439d79ce840fa97bc371552e9b6685/1-Figure1-1.png"alt="Comparison" /> <span class="math inline">\(Fig.1^{[1]}\)</span>Comparison of tracking pipeline. (a) The dominant tracking frameworkcontains three components: a convolutional or transformer backbone, acarefully-designed integration module, and task-specific heads. (b) OurMixFormer is more compact and composed of two components: atarget-search mixed attention based backbone and a simple localizationhead.</p><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/b6eaec7917439d79ce840fa97bc371552e9b6685/3-Figure2-1.png"alt="MAM" /> <span class="math inline">\(Fig.2^{[2]}\)</span> MixedAttention Module (MAM) is a flexible attention operation that unifiesthe process of feature extraction and information integration for targettemplate and search area. This mixed attention has dual attentionoperations where self-attention is performed to extract features fromitself while cross-attention is conducted to communicate between targetand search. This MAM could be easily implemented with a concatenatedtoken sequence. To further improve efficiency, we propose an asymmetricMAM by pruning the target to-search cross attention (denoted by dashedlines).</p><p><imgsrc="https://d3i71xaburhd42.cloudfront.net/b6eaec7917439d79ce840fa97bc371552e9b6685/4-Figure3-1.png"alt="MixFormer" /> <span class="math inline">\(Fig.3^{[3]}\)</span>MixFormer presents a compact end-to-end framework for tracking withoutexplicitly decoupling steps of feature extraction and target informationintegration. It is only composed of a single MAM backbone and alocalization head.</p><figure><imgsrc="https://d3i71xaburhd42.cloudfront.net/b6eaec7917439d79ce840fa97bc371552e9b6685/5-Figure4-1.png"alt="SPM" /><figcaption aria-hidden="true">SPM</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;mixformer-end-to-end-tracking-with-iterative-mixed-attention1&quot;&gt;MixFormer:
End-to-End Tracking with Iterative Mixed Attention&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自南大的Yutao Cui, Cheng Jiang, Limin Wang, Gangshan Wu.
论文引用[1]:Cui, Yutao et al. “MixFormer: End-to-End Tracking with
Iterative Mixed Attention.” 2022 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) (2022): 13598-13608.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2022.Mar&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;compact tracking framework&lt;/li&gt;
&lt;li&gt;unify the feature extraction and target integration solely with a
transformer-based architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;votmotsot的区别&quot;&gt;VOT，MOT,SOT的区别&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;VOT是标首帧，MOT是多目标追踪，SOT是单目标追踪。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;动机&quot;&gt;动机&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Tracking经常用多阶段的pipeline：&lt;strong&gt;feature
extraction，target information integration，bounding box
estimation&lt;/strong&gt;。为了简化这个pipeline，作者提出了一个紧凑的tracking框架，名为MixFormer。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;target information integration解释： fuse the target and search
region information&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Object Tracking" scheme="https://young-eng.github.io/YoungBlogs/tags/Object-Tracking/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu with Nvidia Drivers</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/05/22/Ubuntu-with-Nvidia-Drivers/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/05/22/Ubuntu-with-Nvidia-Drivers/</id>
    <published>2024-05-22T10:30:43.000Z</published>
    <updated>2024-05-22T10:38:38.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="在ubuntu-20.04里安装-nvidia-rtx-3060显卡的驱动">在Ubuntu20.04里安装 Nvidia RTX 3060显卡的驱动</h3><ol type="1"><li><p>之前照着网上的教程弄过一次，记得是通过命令行来弄的，结果搞得黑屏，好不容易解决了黑屏的问题，进入桌面之后，显示不了Wifi和蓝牙，好像缺了很多东西，搞得很狂躁。这两天跑Tracking，大部分是Ubuntu环境下的，就趁这次机会重装一下系统，然后找个新的教程</p></li><li><p>找到了两个方式：</p><ul><li>通过 Ubuntu 自带的Software &amp; updates里头的additionaldrivers里，能看到有Nvidia的显卡驱动，勾一个合适的就行，很简单。。全程没有什么bug。。害得我上次弄了好久。</li><li>去nvidia官网上下载驱动，名称一般是Nvidia-Linux-xxx.run，运行的时候需要先禁用掉 nouveau，在哪个文件里加上:blacklist nouveau, options nouveaumodeset=0，然后重启，看看lsmod一下，看看nouveau有没有被禁用掉。然后运行.run文件，在运行.run文件的时候：提示可以用Ubuntu里 additionaldrivers来安装。</li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;在ubuntu-20.04里安装-nvidia-rtx-3060显卡的驱动&quot;&gt;在Ubuntu
20.04里安装 Nvidia RTX 3060显卡的驱动&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;之前照着网上的教程弄过一次，记得是通过命令行来弄的，</summary>
      
    
    
    
    <category term="methods" scheme="https://young-eng.github.io/YoungBlogs/categories/methods/"/>
    
    
  </entry>
  
  <entry>
    <title>DropMAE</title>
    <link href="https://young-eng.github.io/YoungBlogs/2024/05/21/DropMAE/"/>
    <id>https://young-eng.github.io/YoungBlogs/2024/05/21/DropMAE/</id>
    <published>2024-05-21T14:28:25.000Z</published>
    <updated>2024-05-21T16:22:12.000Z</updated>
    
    <content type="html"><![CDATA[<h3id="masked-autoencoders-with-saptial-attention-dropout-for-tracking-tasks1">MaskedAutoencoders with Saptial-Attention Dropout for TrackingTasks<sup>[1]</sup></h3><blockquote><p>作者是来自CityU、IDEA、Tecent AI Lab、CUHK(SZ)的 QiangqiangWu、Tianyu Yang、Ziquan Liu、Baoyuan Wu、Ying Shan、Antoni B.Chan.论文引用[1]:Wu, Qiangqiang et al. “DropMAE: Masked Autoencoders withSpatial-Attention Dropout for Tracking Tasks.” 2023 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR) (2023):14561-14571.</p></blockquote><h3 id="time">Time</h3><ul><li>2023.Apr</li></ul><h3 id="key-words">Key Words</h3><ul><li>masked autoencoder</li><li>temporal matching-based</li><li>spatial-attention dropout</li></ul><h3 id="动机">动机</h3><ol type="1"><li>将MAE应用到下游任务如: visual object tracking(VOT) and video objectsegmentation(VOS). 简单的扩展MAE是mask out frame patches in videos andreconstruct the frame pixels.然而作者发现这个会严重依赖于spatial cues,当进行frame reconstruction的时候忽略temporal relations,这个导致sub-optimal temporal matching representations for VOT andVOS.</li></ol><span id="more"></span><h3 id="总结">总结</h3><ol type="1"><li><p>采用adaptively spatial-attention dropout in the framereconstruction，来促进temporal correspondence learning in videos.作者展示了dropMAE是一个很强的、efficient temporal matching learner.实现了better fine-tuning results on matching-based tasks.作者还发现motion diversity in pre-training videos比scenediversity更重要。 预训练的dropMAE能够直接用现有的ViT-based trackers forfine-tuning without further modifications.</p></li><li><p>在VOT中最近的两个工作，SimTrack和OSTrack，都是用MAE pre-trainedViT model作为trackign backbone, 不需要复杂的trackingpipelines，实现了很好的效果。它们的MAE是在ImageNet上的。没有先验的时序信息能在静态图像上学习到。之前的追踪方法表明：temporalcorrespondence learning 对于develop一个robusttracker是关键的。因此develop MAE framework specifically formatching-based videos tasks 是一个机会。</p></li><li><p>Extend MAE to videos 是随机mask out frame patches in a videoclip, then reconstruct video clip. 像baselineTwinMAE，在重建patches的时候，会严重依赖 spatially neighbouring patcheswithin the same frame, which implies a heavy co-adaptation of spatialcues(within-frame tokens) for reconstruction, 可能对于matching-based下游任务，造成learning of sub-optimal temporal representation.</p></li><li><p>为了解决这个问题，dropMAE能够adaptively performsspatial-attention dropout to <strong>break up co-adaptation betweenspatial cues(within-frame tokens) during the framereconstruction</strong>, 因此encouraging more temporal interaction andfacilitating temporal correspondence learning.</p></li><li><p>VOT：标首帧，然后预测之后的目标的boundingbox; VOS: 标首帧的binarymask，预测之后的target masks。之前VOT的方法，correlation filter-basedapproaches 占主导，因为它们能够很好地对target appearancevariation进行建模。深度学习的发展，Siamese网络引入到了VOT,SiamFC用template和search images 作为target localization的输入。</p></li><li><p>自监督的方法中，有很多人工设计的代理任务(pretext tasks) forpre-training.例如：image colorization、jigsaw puzzle solving、futureframe prediction、rotation prediction、Contrastive learning approaches.然而，这些方法都对type 和strength of applied dataaugmentation敏感，使得它们很难训练。</p></li><li><p>不同于已有的VideoMAE，这个是用来做video actionrecognition的，在预训练的时候用一个long video clip(16frames)。为了和VOT/VOS保持一致，在通常的objecttracking中，从一个视频中采样2个frames作为TwinMAE的输入，来做预训练。</p></li><li><p>Given a query token， 基本想法是<strong>adaptively drop a portionof its with-frame cues in order to facilitate the model to learn morereliable temporal correspondence</strong>.就是<strong>限制同一个frame里的 query token和tokens之间的interaction,encourage more interactions with tokens in the otherframe</strong>，这样模型会去学到更好的temporal matchingability.</p></li><li><p>用在VOT上的流程：</p><ul><li>cropped template 和search images are firstly serialized intosequences and concatenated together，然后，总的sequence is added withthe positional embeddings and input to the ViT backbone for jointfeature extraction and interaction. 最后，updated search features areinput to a prediction head to predict the target bounding box.</li><li>在fine-tuning的时候，用预训练的DropMAE来初始化OSTracker中的ViT，同时two frame identity embeddings are respectively added to template andsearch embeddings, in order to keep consistency with the pre-trainingstage. <imgsrc="https://d3i71xaburhd42.cloudfront.net/4569040e52aabdc92213d0687eafba0c73c1afdc/3-Figure3-1.png"alt="DropMAE" /> <span class="math inline">\(Fig.3^{[1]}\)</span>：DropMAE: The proposed adaptive spatial-attention droput(ASAD)facilitates temporal correspondence learning for temporal matchingtasks. TwinMAE follows the same pipeline except that the ASAD module isnot used.</li></ul></li></ol>]]></content>
    
    
    <summary type="html">&lt;h3
id=&quot;masked-autoencoders-with-saptial-attention-dropout-for-tracking-tasks1&quot;&gt;Masked
Autoencoders with Saptial-Attention Dropout for Tracking
Tasks&lt;sup&gt;[1]&lt;/sup&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;作者是来自CityU、IDEA、Tecent AI Lab、CUHK(SZ)的 Qiangqiang
Wu、Tianyu Yang、Ziquan Liu、Baoyuan Wu、Ying Shan、Antoni B.Chan.
论文引用[1]:Wu, Qiangqiang et al. “DropMAE: Masked Autoencoders with
Spatial-Attention Dropout for Tracking Tasks.” 2023 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) (2023):
14561-14571.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2023.Apr&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-words&quot;&gt;Key Words&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;masked autoencoder&lt;/li&gt;
&lt;li&gt;temporal matching-based&lt;/li&gt;
&lt;li&gt;spatial-attention dropout&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;动机&quot;&gt;动机&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;将MAE应用到下游任务如: visual object tracking(VOT) and video object
segmentation(VOS). 简单的扩展MAE是mask out frame patches in videos and
reconstruct the frame pixels.然而作者发现这个会严重依赖于spatial cues,
当进行frame reconstruction的时候忽略temporal relations,
这个导致sub-optimal temporal matching representations for VOT and
VOS.&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://young-eng.github.io/YoungBlogs/categories/Papers/"/>
    
    
    <category term="Autoencoder" scheme="https://young-eng.github.io/YoungBlogs/tags/Autoencoder/"/>
    
  </entry>
  
</feed>
