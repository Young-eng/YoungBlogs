<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/YoungBlogs/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/YoungBlogs/images/Icon.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/YoungBlogs/images/Icon.jpg">
  <link rel="mask-icon" href="/YoungBlogs/images/logo.svg" color="#222">

<link rel="stylesheet" href="/YoungBlogs/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"young-eng.github.io","root":"/YoungBlogs/","images":"/YoungBlogs/images","scheme":"Gemini","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/YoungBlogs/js/config.js"></script>

    <meta name="description" content="Watch Only Once：An End-to-end Video Action Detection Framework[1]  作者是来自港大的罗平老师组的Shoufa Chen、Peize Sun、Enze Xie等人。论文引用[1]:Chen, Shoufa et al. “Watch Only Once: An End-to-End Video Action Detecti">
<meta property="og:type" content="article">
<meta property="og:title" content="WOO">
<meta property="og:url" content="https://young-eng.github.io/YoungBlogs/2024/08/27/WOO/index.html">
<meta property="og:site_name" content="Young&#39;s Blog">
<meta property="og:description" content="Watch Only Once：An End-to-end Video Action Detection Framework[1]  作者是来自港大的罗平老师组的Shoufa Chen、Peize Sun、Enze Xie等人。论文引用[1]:Chen, Shoufa et al. “Watch Only Once: An End-to-End Video Action Detecti">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-08-27T07:23:26.000Z">
<meta property="article:modified_time" content="2024-08-29T05:58:17.880Z">
<meta property="article:author" content="Young">
<meta property="article:tag" content="Action Detection">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://young-eng.github.io/YoungBlogs/2024/08/27/WOO/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://young-eng.github.io/YoungBlogs/2024/08/27/WOO/","path":"2024/08/27/WOO/","title":"WOO"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>WOO | Young's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/YoungBlogs/css/noscript.css">
  </noscript>
<link rel="alternate" href="/YoungBlogs/atom.xml" title="Young's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/YoungBlogs/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Young's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/YoungBlogs/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/YoungBlogs/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/YoungBlogs/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/YoungBlogs/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/YoungBlogs/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#watch-only-oncean-end-to-end-video-action-detection-framework1"><span class="nav-number">1.</span> <span class="nav-text">Watch
Only Once：An End-to-end Video Action Detection
Framework[1]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#time"><span class="nav-number">2.</span> <span class="nav-text">Time</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#key-words"><span class="nav-number">3.</span> <span class="nav-text">Key Words</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Young"
      src="/YoungBlogs/images/Avatar1.png">
  <p class="site-author-name" itemprop="name">Young</p>
  <div class="site-description" itemprop="description">记录学习和生活</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/YoungBlogs/archives/">
          <span class="site-state-item-count">92</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/YoungBlogs/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/YoungBlogs/tags/">
        <span class="site-state-item-count">36</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Young-eng" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Young-eng" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yuen201718@163.com" title="Email → mailto:yuen201718@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>Email</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://young-eng.github.io/YoungBlogs/2024/08/27/WOO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/YoungBlogs/images/Avatar1.png">
      <meta itemprop="name" content="Young">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Young's Blog">
      <meta itemprop="description" content="记录学习和生活">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="WOO | Young's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          WOO
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-08-27 15:23:26" itemprop="dateCreated datePublished" datetime="2024-08-27T15:23:26+08:00">2024-08-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-08-29 13:58:17" itemprop="dateModified" datetime="2024-08-29T13:58:17+08:00">2024-08-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/YoungBlogs/categories/Papers/" itemprop="url" rel="index"><span itemprop="name">Papers</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h3
id="watch-only-oncean-end-to-end-video-action-detection-framework1">Watch
Only Once：An End-to-end Video Action Detection
Framework<sup>[1]</sup></h3>
<blockquote>
<p>作者是来自港大的罗平老师组的Shoufa Chen、Peize Sun、Enze
Xie等人。论文引用[1]:Chen, Shoufa et al. “Watch Only Once: An End-to-End
Video Action Detection Framework.” 2021 IEEE/CVF International
Conference on Computer Vision (ICCV) (2021): 8158-8167.</p>
</blockquote>
<h3 id="time">Time</h3>
<ul>
<li>2021.Oct</li>
</ul>
<h3 id="key-words">Key Words</h3>
<ul>
<li>end-to-end unified network</li>
<li>task-specific features</li>
</ul>
<h3 id="总结">总结</h3>
<ol type="1">
<li>提出了一个端到端的pipeline for video action
detection。<strong>当前的方法要么是将video action detection
这个任务解耦成action localization和action
classification这两个分离的阶段，要么在一个阶段里训练两个separated
models</strong>。相比之下，作者的方法将actor localization和action
classification弄在了一个网络里。通过统一backbone网络，去掉很多认为的手工components，整个pipeline被简化了。<strong>WOO</strong>用一个unified
video backbone来提取features for actor location 和action
localization,另外，引入了<font color=red>spatial-temporal action
embeddings</font>，设计了一个 spatial-temporal fusion
module来得到更多的含有丰富信息的discriminative features，提升了action
classification的性能。</li>
</ol>
<span id="more"></span>
<ol start="2" type="1">
<li><p>Video action detection 包含 <strong>actor bbox
localization</strong>和 <strong>action type
classification</strong>，当前方法的复杂性来自于actor
localization和action classification之间的基本的困境。that
is，*用一个single key frame is "positive" for actor localization，
但是"negative" for action
classification，然而用多个frames有相反的影响。这是因为action
localization需要一个2D的检测模型同预测video clip的key frame上的actor
bbox。在这个阶段，考虑clip中的相邻帧会带来额外的计算和存储成本，相比之下，action
classification严重依赖于一个 3D video model来提取video
sequence中的时序的信息，单帧图像有很少的temporal motion representation
for action classification。</p>
<p>之前提出了两种可能的替代方法来解决这个困境。第一个是用离线的person
detector，用来产生actor proposals，不是和action
classification一起训练的，然后一个独立的video model用这些actor
proposals和raw frames作为输入来预测action classes。单独的一个person
detector是已经足够复杂了，which is pre-trained on the ImageNet and COCO
human keypoint detection，and further fine-tuned on the target action
detection dataset。这个方法比较复杂，计算成本高，需要两个separate
models和两个训练阶段。更进一步，<strong>separate optimization on two
sub-problems leads to a sub-optimal solution</strong>。</p>
<p>第二种类型的方法，是将actor detection和action classification
models一起在单个training
stage中进行联合训练。虽然训练的pipeline在某种程度上是简化了，两个模型仍然需要独立的从raw
images中提取特征。因此，整个框架仍然有很高的computation和memory
cost。</p>
<p><strong>一个很自然的问题是：有没有可能设计一个简单的unified的网络来解决actor
localization和action classification in a single end-to-end
model</strong>。</p>
<p>本文提出了 <strong>Watch Only
Once(WOO)</strong>的框架，WOO直接预测actors的bbox和action classes from a
video clip。"watch" the clip only
once，就能够预测actor的位置和action的类别。方法主要包含3个key
components：<font color=red>unified backbone、a spatial-temporal action
embedding、a spatial-temporal knowledge fusion mechanism</font>。</p>
<p>首先，设计了一个简单的有效的module，使得单个backbone能偶提供task-specific
feature maps for actor localization head 和action classification
head。这个module是轻量的，能够用来isolate keyframe features from all
frames in the early stage of the
backbone。这个动机是，当model走深的时候，keyframe能够得到more
interaction with neighboring
frames，提出的module能够很容易地插入到现有的backbone中，例如
I3D,X3D等。</p>
<p><strong>另外，注意到，同一个的架构tends to behave well for actor
localization，但是对于action classification是有限的。这个困难是action
detection主要lies in action classification。因此，怀疑单个backbone for
both tasks是否会bias towards localization，然后找到一个undesired
solution，因此对action
classification的性能造成了影响。基于这个观察，提出了spatial和tempoal
action embedding和interaction mechanism between them，来使得action
classification features在spatial 和temporal perspectives更加的
discriminative。</strong></p>
<p>第三，提出了一个spatial-temporal fusion module来汇聚spatial和temporal
knowledge，这个spatial properties例如shape和pose，temporal
properties例如dynamic motion和temporal scale of
action,结合在一起，通过spatial-temporal fusion module，来生成action
features for actio classification。</p>
<p>主要贡献如下：</p>
<ol type="1">
<li><p>提出了一个端到端的框架for video action detection，给定一个video
clip作为输入，能够直接产生bboxes 和action classes，不需要独立的person
detector(这个是在之前的工作中必不可少的).</p></li>
<li><p>提出了一个spatial-temporal embedding，一个embedding interaction
mechanism，能够提高features的discriminativeness for action
classificatin。一个spatial-temporal fusion
module来进一步从spatial和temporal来汇聚features。</p></li>
</ol></li>
<li><p>相关工作：</p>
<ul>
<li><p><font color=red>two-stage, two
backbone</font>：当前的STAD的SOTA模型通常用了两阶段的pipeline，用了两个backbone。这些方法简单地把STAD任务划分成actor
localization和action classification。具体地说，在第一阶段，在COCO
keypoint上预训练一个model，然后再目标STAD
dataset上进行微调；第二阶段，用video clip的key
frame作为第一阶段中得到的detection model的输入，来预测actor
bboxes。然后将video clip和actor bboxes作为3D
backbone的输入，来提取RoI区域的特征 for action class
prediction。<strong>这些方法有很高的复杂度和很低的效率，因为sequential
training stage和separate
model架构。另外，在两个独立的阶段的独立的优化可能导致一个sub-optimal的结果</strong>。</p></li>
<li><p><font color=red>One-stage,
two-backbone</font>：<strong>YOWO和ACRN通过同时训练2D的actor
检测网络和3D video model，来简化网络。然而，这里仍然有两个separate
models来优化</strong>。<strong>以YOWO为例，它包含一个在Kinetics上预训练的一个
3D model和 YOLO pre-trained on PASCAL VOC上的 2D
model，有很高的计算量和memory
burden</strong>。是虽然这个pipeline在一定程度上简化了。</p></li>
</ul>
<p>相比于这些方法，WOOrefreshing的简单：给定一个video
clip，直接预测actor bboxes和对应的action classes。</p>
<ul>
<li><p><font color=red>End-to-end object
detection</font>：最近的端到端的目标检测框架，不需要任何手工设计的过程例如NMS，直接输出预测，实现了很好的性能。在这些工作中，DETR可以被视为端到端的目标检测方法，它采用了global
attention mechanism和双向matching between predictions和ground truth
objects。DETR抛弃了NMS步骤，实现了很好的性能。它在小目标上性能不太好，相比主流的检测器需要更长的训练时间。为了解决上述的问题，提出了<strong>Deformable-DETR</strong>，来将每个object
query 限制在 a small set of crucial sampling points around reference
points，而不是all points in the feature
map。Deformable-DETR是高效和快速收敛的；Sparse RCNN利用一组稀疏的learned
object proposals，以iterative way来进行分类和定位。Sparse
RCNN和well-established的检测器相比，展示了精度、实时和训练收敛。在本次工作中，主要采用<strong>Sparse
RCNN的检测头来做定位</strong>。</p></li>
<li><p><font color=red>Attention mechanism for action
recognition</font>：对于language相关的任务，注意力机制是一个流行的概念
。对于actio
recognition，Non-local网络利用自注意力来得到不同时间或者空间features的<strong>dependencies</strong>。使得注意力机制applicable
for action classification，有人利用non-local block作为一个long-term
feature bank operator，使得video models能得到long-term
information，提高了action detection的性能。</p></li>
</ul></li>
<li><p>Methods: <span
class="math inline">\(X\quad\in\quad\mathbb{R}^{C\times T\times H\times
W}\)</span> 是一个layer的输入的spatial-temporal feature
map。跟随前人的工作，in this work，将key frame放在video
clip的中间，<span class="math inline">\(X_{t=\lfloor
T/2\rfloor}\in\quad\mathbb{R}^{C\times H\times
W}\)</span>表示<strong>keyframe feature map</strong>。</p>
<ul>
<li><strong>union backbone</strong>：在之前的video backbone中，key frame
features将会和相邻的frame features通过temporal
pooling或者3D卷积(temporal kernel size 大于1，会给key
frame特征带来意想不到的disturbance)
进行interact。为了克服这个问题，video backbone设计的时候，将key
frame和temporal interaction之前的早期的网络的features隔离开。</li>
</ul>
<p>和之前的backbon Slowfast(将spatial stride of res5设为1，用a dilation
of 2 for its filters，来增加spatial resolution of res5 by <span
class="math inline">\(2 \times\)</span>)不同，作者去掉了res5中的dialted
conv，采用FPN module来做<strong>keyframe</strong>特征提取。FPN
module用res2,res3,res4,res5的输出的<strong>keyframe
feature</strong>作为输入。进一步用FPN的输出的特征 for actor
localization,res5输出的特征来做action
classification。为了这个目的，一个统一的action
backbone用来提供task-relevant features。</p>
<p>以上的设计有几个好处，首先，actor localization
head采用层次化的feature representation作为source
features。对于目标检测是有好处的。第二，用于actor localization的keyframe
features通过FPN结构，和所有的video frames的features隔离开，starting at
the early stage of the
backbone。这能减少邻近帧的干扰，因为keyframe会随着model的深入，和邻近帧有更多的interaction。第三，相比于存在的两个backbone的网络(用独立的backbone)
for actor localization，作者仅用一个轻量的FPN module that tasks image
features as input，减少了参数和FLOPS。</p>
<ul>
<li><p><strong>Acotr Localization Head</strong>：受最近的Sparse
RCNN的启发，设计了一个端到端的actor detection head for actor
localization,detection head在得到hierarchical features from
FPN之后，能够预测bbox和对应的scores indicating model's confidence on the
box containing an actor。另外，person detector利用 set prediction loss
for optimal bipartite matching between prediciton和ground truth at
training
stage，在验证的阶段不需要post-process。不同于two-backbone的方法，不许哟啊额外的预训练，因为person
detector和action classifier共享一个backbone。</p></li>
<li><p><strong>Action Classification Head</strong>：给定有person
detector生成的 <span class="math inline">\(N\)</span>个actor proposal
boxes。用RoIAlign来提取每个box的spatial和temporal
features，这两种类型的features然后融合，得到最终的action class
prediction。细节如下：</p>
<ol type="1">
<li><p><strong>Spatial Action Features</strong>：<span
class="math inline">\(X_{5} \in \mathbb{R}^{C\times T\times H\times
W}\)</span>表示 res5得到的feature，在时间维度上进行一个<strong>global
average pooling</strong>，得到一个spatial feature map，<span
class="math inline">\(f^{s} \in \mathbb{R}^{C\times1\times H\times
W}\)</span>，在 <span class="math inline">\(f^s\)</span> 上用RoIAlign
with <span class="math inline">\(N\)</span> 个actor proposals，得到
<span class="math inline">\(N\)</span>个 spatial RoI features。<span
class="math inline">\(f_{1}^{s},f_{2}^{s},\cdots,f_{N}^{s} \in
\mathbb{R}^{C\times S\times S},\)</span>， <span class="math inline">\(S
\times S\)</span>是 RoIAlign输出的spatial output size。</p></li>
<li><p><strong>Temporal Action Features</strong>：除了spatial action
features，temporal properties也很重要，为了得到temporal motion
information，从feature volume <span
class="math inline">\(X_5\)</span>中的every frame提取temporal
features。因为这里主要关注temporal information，在spatial
dimension上用一个global average pooling，来提取temporal RoI
features。temporal action feature表示为<span
class="math inline">\(f_{1}^{t},f_{2}^{t},\cdots,f_{N}^{t}\in
\mathbb{R}^{C\times T\times1\times1}\)</span>。</p></li>
<li><p><strong>Embedding Interaction</strong>：为了得到discriminative
features，为了增强instance的特性，引入了spatial 和temporal embedding to
be convolved with aforementioned spatial and temporal features。spatial
embedding期望能够encoder spatial properties例如shape,pose等。temporal
embedding能够encode temporal dynamic
properties，例如dynamics和action的temporal
scale。注意到embedding是对于每个 <span class="math inline">\(N\)</span>
features是exclusive的。定义 <span class="math inline">\(E^{s}\in
\mathbb{R}^{N\times d},E^{t}\in \mathbb{R}^{N\times d}\)</span> for
spatial and temporal embedding。<span class="math inline">\(E_n^s \in
\mathbb{R}^d,E_n^t \in \mathbb{R}^d\)</span> are working for n-th RoI
feature。为了获得不同actors之间的relation 信息，构建了一个attention
module for all RoI features。因为每个actor RoI有自己的spatial和temporal
embedding，embedding相比于feature
map更lighter，在对不同给的embedding之间而不是feature
maps之间采用attention mechanism for efficiency。这里给定一个query
element和一系列key elements，多头注意力module能够根据attention
weights(measure compatibility of query-key pairs adaptively)来汇聚key
contents。最后，<span class="math inline">\(x = (x_,...,x_n)\)</span>
表示 <span class="math inline">\(n\)</span>个input elements，输出 <span
class="math inline">\(z= (z_1,...,z_n)\)</span>，<span
class="math inline">\(z_i\)</span>是weighted sum of a linearly
transformed input：</p></li>
</ol>
<p><span
class="math display">\[z_i=\sum_{j=1}^n\alpha_{ij}(x_jW^V).\]</span></p>
<p>weight coefficient <span
class="math inline">\(\alpha_{ij}\)</span>是通过softmax计算出来的：
<span class="math display">\[\alpha_{ij}=\frac{\exp
z_{ij}}{\sum_{k=1}^n\exp z_{ik}}, \text{where}
z_ij=\frac{(x_iW^Q)(x_jW^K)^T}{\sqrt{d_z}}.\]</span> 将 <span
class="math inline">\(E^s\)</span>和 <span
class="math inline">\(E^t\)</span>送到 self-attention
module，得到对应的输出 <span
class="math inline">\(\phi^s，\phi^t\)</span>，和 原始的embeddings <span
class="math inline">\(E^s, E^t\)</span>有相同的shape，最后的action
feature是： <span
class="math display">\[f=\mathcal{G}(\mathcal{F}(f^s,\phi^s),\mathcal{F}(f^t,\phi^t)),\]</span></p>
<p><span class="math inline">\(F\)</span>是一个卷积操作 with parameters
<span class="math inline">\(\phi\)</span>，<span
class="math inline">\(G\)</span>是spatio-temporal fusion操作。初始化
<span class="math inline">\(F\)</span> with <span
class="math inline">\(1 \times 1\)</span> kernels for efficiency。</p>
<ol start="4" type="1">
<li><strong>Objective
Function</strong>：提出的model以端到端的方式解决了定位和分类，整个的目标函数由对应的两部分组成：
<span
class="math display">\[\mathcal{L}=\underbrace{\lambda_{cls}\cdot\mathcal{L}_{cls}+\lambda_{L1}\cdot\mathcal{L}_{L1}+\lambda_{giou}\cdot\mathcal{L}_{giou}}_{\text{set
prediction
loss}}+\underbrace{\lambda_{act}\cdot\mathcal{L}_{act}}_{\text{action}}.\]</span></li>
</ol>
<p>第一部分是 <em>set prediciton loss</em>，produces an optimal
<strong>bipartite matching between predictions and ground truth
objects</strong>。用 <span class="math inline">\(L_{cls}\)</span>
表示cross-entropy loss over two classes(containing actor vs not
containing actor)。<span class="math inline">\(L_{L1}\)</span> 和 <span
class="math inline">\(L_{giou}\)</span>是box loss。<span
class="math inline">\(\lambda_{cls},\lambda_{L1},\lambda_{giou}\)</span>
是常量，平衡这些loss的contributions。对于第二部分， <span
class="math inline">\(L_{act}\)</span>是一个binary cross entropy loss
used for action classification，<span
class="math inline">\(\lambda_{act}\)</span> 是对应的weight。</p></li>
</ul></li>
<li><p>实验：</p>
<ul>
<li>Spatial-temporal fusion：有不同的instantiations of fusing temporal
和spatial action features：summation、concatenation和
cross-attention(CA)，结果表明CA效果比另外两个好。</li>
</ul></li>
</ol>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/1-Figure1-1.png"
alt="Motivation of WOO" />
<figcaption aria-hidden="true">Motivation of WOO</figcaption>
</figure>
<p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Motivation of
WOO. (a) Previous dominant video action detection methods usually adopt
two separate networks: an independent 2D detection model for actor
localization from every key frames, and a 3D video model for action
classification from video clips. (b) Our end-to-end unified framework
uses a single backbone network to handle both 2D image detection and 3D
video classification (i.e.2D spatial dimensions plus a temporal
dimension). This unified backbone only “watches” an input video once,
and directly produces both actor localization and action
classification</p>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/3-Figure2-1.png"
alt="Comparison of Backbone" />
<figcaption aria-hidden="true">Comparison of Backbone</figcaption>
</figure>
<p><span class="math inline">\(Figure \ 2^{[1]}\)</span>: Comparisons of
backbone architecture. (a) Two separate backbones for actor localization
and action classification. Video backbone adopts res5 stage with dilated
convolution (DC5). (b) A single union backbone which can provide
task-specific features for actor localization and action classification
simultaneously, enabling nearly cost-free feature extraction for actor
localization compared to (a). Key frame features are illustrated in
light orange color. Here we purposely omit the res2 features for visual
simplicity</p>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/4-Figure3-1.png"
alt="Action classification head" />
<figcaption aria-hidden="true">Action classification head</figcaption>
</figure>
<p><span class="math inline">\(Figure \ 3^{[1]}\)</span>: Action
classification head. Given the RoI feature of a specific box for T
frames, spatial and temporal action features are generated. Then,
spatial and temporal embedding is used to make action feature
representation more discriminative through the interaction module.
Finally, the multi-layer perceptron (MLP) takes as input the fused
spatial-temporal feature and predicts the action class logits. See text
for details</p>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/068ff8c9c5cea92c4a14ac1036cc9033e4bf695d/5-Figure4-1.png"
alt="Structure of interaction module" />
<figcaption aria-hidden="true">Structure of interaction
module</figcaption>
</figure>
<p><span class="math inline">\(Figure \ 4^{[1]}\)</span>: Structure of
interaction module. Here we plot spatial embedding interaction as an
example. ‘⊗’ denotes matrix multiplication, and ‘⊛’ denotes 1 × 1
convolution</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/YoungBlogs/tags/Action-Detection/" rel="tag"># Action Detection</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/YoungBlogs/2024/08/26/YOWOv3/" rel="prev" title="YOWOv3">
                  <i class="fa fa-angle-left"></i> YOWOv3
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/YoungBlogs/2024/09/03/SLAM-%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/" rel="next" title="SLAM 学习记录">
                  SLAM 学习记录 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-user"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Young</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">88k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">5:22</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="100" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/YoungBlogs/js/comments.js"></script><script src="/YoungBlogs/js/utils.js"></script><script src="/YoungBlogs/js/motion.js"></script><script src="/YoungBlogs/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/YoungBlogs/js/third-party/math/mathjax.js"></script>



</body>
</html>
