<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/YoungBlogs/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/YoungBlogs/images/Icon.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/YoungBlogs/images/Icon.jpg">
  <link rel="mask-icon" href="/YoungBlogs/images/logo.svg" color="#222">

<link rel="stylesheet" href="/YoungBlogs/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"young-eng.github.io","root":"/YoungBlogs/","images":"/YoungBlogs/images","scheme":"Gemini","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/YoungBlogs/js/config.js"></script>

    <meta name="description" content="You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization[1]  作者是来自Technical Univ of Munich的Okan Kopuklu, Xiangyu Wei, Gerhard Rigoll。论文引用[1]:Köpüklü, Okan">
<meta property="og:type" content="article">
<meta property="og:title" content="YOWO">
<meta property="og:url" content="https://young-eng.github.io/YoungBlogs/2024/08/24/YOWO/index.html">
<meta property="og:site_name" content="Young&#39;s Blog">
<meta property="og:description" content="You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization[1]  作者是来自Technical Univ of Munich的Okan Kopuklu, Xiangyu Wei, Gerhard Rigoll。论文引用[1]:Köpüklü, Okan">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-08-24T08:51:02.000Z">
<meta property="article:modified_time" content="2024-08-27T07:15:14.850Z">
<meta property="article:author" content="Young">
<meta property="article:tag" content="Action Detection">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://young-eng.github.io/YoungBlogs/2024/08/24/YOWO/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://young-eng.github.io/YoungBlogs/2024/08/24/YOWO/","path":"2024/08/24/YOWO/","title":"YOWO"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>YOWO | Young's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/YoungBlogs/css/noscript.css">
  </noscript>
<link rel="alternate" href="/YoungBlogs/atom.xml" title="Young's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/YoungBlogs/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Young's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/YoungBlogs/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/YoungBlogs/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/YoungBlogs/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/YoungBlogs/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/YoungBlogs/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#you-only-watch-once-a-unified-cnn-architecture-for-real-time-spatiotemporal-action-localization1"><span class="nav-number">1.</span> <span class="nav-text">You
Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal
Action Localization[1]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#time"><span class="nav-number">2.</span> <span class="nav-text">Time</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#key-words"><span class="nav-number">3.</span> <span class="nav-text">Key Words</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Young"
      src="/YoungBlogs/images/Avatar1.png">
  <p class="site-author-name" itemprop="name">Young</p>
  <div class="site-description" itemprop="description">记录学习和生活</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/YoungBlogs/archives/">
          <span class="site-state-item-count">82</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/YoungBlogs/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/YoungBlogs/tags/">
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Young-eng" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Young-eng" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yuen201718@163.com" title="Email → mailto:yuen201718@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>Email</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://young-eng.github.io/YoungBlogs/2024/08/24/YOWO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/YoungBlogs/images/Avatar1.png">
      <meta itemprop="name" content="Young">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Young's Blog">
      <meta itemprop="description" content="记录学习和生活">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="YOWO | Young's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          YOWO
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-08-24 16:51:02" itemprop="dateCreated datePublished" datetime="2024-08-24T16:51:02+08:00">2024-08-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-08-27 15:15:14" itemprop="dateModified" datetime="2024-08-27T15:15:14+08:00">2024-08-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/YoungBlogs/categories/Papers/" itemprop="url" rel="index"><span itemprop="name">Papers</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h4
id="you-only-watch-once-a-unified-cnn-architecture-for-real-time-spatiotemporal-action-localization1">You
Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal
Action Localization<sup>[1]</sup></h4>
<blockquote>
<p>作者是来自Technical Univ of Munich的Okan Kopuklu, Xiangyu Wei,
Gerhard Rigoll。论文引用[1]:Köpüklü, Okan et al. “You Only Watch Once: A
Unified CNN Architecture for Real-Time Spatiotemporal Action
Localization.” ArXiv abs/1911.06644 (2019): n. pag.</p>
</blockquote>
<h4 id="time">Time</h4>
<ul>
<li>2019.Nov.15(v1)</li>
<li>2021.Oct.18(v5)</li>
</ul>
<h4 id="key-words">Key Words</h4>
<ul>
<li>single-stage with two branches</li>
</ul>
<h4 id="总结">总结</h4>
<ol type="1">
<li>当前的网络抽取时序信息和keyframe的空间信息是用两个分开的网络，然后用一个额外的mechanism来融合得到detections。YOWO是一个单阶段的架构，有两个分支，来同时抽取当前的时序和空间信息，预测bboxes和action
的概率 directly from video clips in one
evaluation。因为架构是统一的，因此可以端到端的优化。YOWO架构速度快，能够做到在16-frames
input clips上做到 34 frames-per-second，62 frames-per-second on 8-frames
input clips。是当前在STAD任务上最快的架构。</li>
</ol>
<span id="more"></span>
<ol start="2" type="1">
<li><p>和静态图像里的目标检测相比，时序信息很重要，受目标检测FasterRCNN的启发，SOTA的工作将经典两阶段的网络架构扩展到action
detection，第一阶段产生proposals，在第二阶段进行分类和定位的refinement，然而，两阶段的
STAD任务有3个主要的缺点：(1):action tube是由bboxes across
frames组成的，它的产生比2D
case更复杂和耗时。分类的性能极度依赖这些Proposals，然而detected
bboxes可能对于后续的分类任务是sub-optimal；(2)action
proposals只关注视频里的人物的features，忽略人和背景中的其它特征，而这这能够提供对于action
prediction相当重要的上下文信息。(3)训练RPN网络和分类网络是分开的，不能保证找到全局最优。只有局部最优
from the combination of two stages can be found.
训练的成本比单阶段的高，因此花费很多时间和存储。</p></li>
<li><p>YOWO克服了上述提到的缺点，YOWO的本能的idea是来自人类视觉认知系统，为了理解视频中的人物的行为，需要<strong>将当前帧的信息2D
features from key frame</strong>与之前记忆里获得的知识(3D features from
clip)相关联，之后，两种features融合到一起，提供一个合理的结论。YOWO架构是一个单阶段的、有两个分支的网络。一个分支<strong>提取key
frame的spatial features via a 2D-CNN，另一个分支models spatiotemporal
features of the clip consisting of previous frames via a 3D
CNN</strong>。YOWO是一个<strong>causal
架构(因果架构)，就是没有利用future frames</strong>，能够operate online
on incoming video streams。为了aggregate 2D CNN和3D CNN的features
smoothly, 用了一个channel fusion和attention机制，get the utmost out of
inter-channel dependencies。最后，用融合的特征，产生frame-level
detections，用一个<strong>linking 算法</strong>来产生action tubes.
YOWO不局限于RGB的模态，其它的例如光流也是可以的；任何一个CNN的架构根据实时性能的要求，都可以用。YOWO
operates with maximum 16 frames input，因为short clip
lengths对于实现STAD人物的实时性是必要的。然而，small clip
size是时序信息累计的限制因素。因此，利用<strong>long-term feature
bank</strong>，通过训练好的3D
CNN从整个视频中提取非重叠的8帧片段的特征。在推理的时候，averaged 3D
features centering the key-frame.</p></li>
<li><p>主要贡献如下：</p>
<ul>
<li>提出了在视频流里的单阶段的STAD框架，能够端到端的训练。实现了在3D
CNN和2D CNN上特征的bboxes 回归，同时，这两个特征对于彼此是互补的 for
final bboxes 回归和分类。用了<strong>channel attention</strong>
来汇聚两个branches的特征。实验证明：channel-wise attention机制，models
inter-channel relationship within the concatenated feature
maps，提高了性能。</li>
</ul></li>
<li><p>Related work: 为了考虑时序信息，有twe-stream
CNN来提取分别提取空间和时间特征，然后汇聚到一起；这样的工作大部分是基于光流的，很耗时和耗计算资源。然后就是3D
CNN，用它来提取时空特征。为了resource efficiency，一些工作用2D
CNN来学习2D 特征，然后用一个3D
CNN来将它们融合到一起，学习时间特征。Attention是一个有效的mechanism来capture
long-range dependencies，用在了CNNs中来尝试提高图像分类的性能。Attention
mechanism在<strong>spatial-wise和channel-wise</strong>来执行。<strong>spatial
attention解决inter-saptial relationship among features，channel
attention
增强最有意义的channels，弱化其它的</strong>。作为一个channel-wise
attention block，Squeeze-and-Excitation
moduel对于提高CNN的性能有益。另一方面，对于视频分类人物，non-local
block考虑时空信息，来学习across
frames的特征的dependencies。可以视为自注意力策略。不同于之前的工作，YOWO只使用clip一次，检测keyframe里的对应的actions。为了避免光流的复杂计算，用keyframe的2D
features和clip的3D features。之后，两种类型的features用<strong>attention
mechanism</strong>融合在一起，就能够考虑到丰富的上下文信息。</p></li>
<li><p>YOWO的架构主要分为4个部分：<strong>3D CNN
branch</strong>、<strong>2D CNN branch</strong>、<strong>CFAM</strong>
和 <strong>bbox regression parts</strong>.</p>
<ul>
<li><font color=red>3D
CNN</font>：因为上下文信息对于人类行为理解很重要，因此用3D
CNN来提取时空特征。3D
CNN能够获得运动信息(在空间和时间维度做卷积)，基本的3D CNN架构这里用的是
3D-ResNext-101，对于所有的3D
CNN架构，在最后的卷积层之后的所有层都会被丢弃。3D网络的输入是一个video
clip。是由时间上连续的视频帧组成，shape为 <span class="math inline">\(C
\times D \times H \times W\)</span>，最后一个3D
ResNext-101的输出的feature map的shape为 <span
class="math inline">\(C&#39; \times D&#39; \times H&#39; \times
W&#39;\)</span>，<span class="math inline">\(C=3, D&#39;=1, H&#39; =
\frac{H}{32}, W&#39; = \frac{W}{32}\)</span>，<span
class="math inline">\(D\)</span> 是输入的帧数。输出特征图的depth
维度减小到1，以至于 output volume可以squeezed to <span
class="math inline">\(C&#39; \times H&#39; \times
W&#39;\)</span>，为了和2D-CNN的输出匹配。</li>
<li><font color=red>2D CNN</font>：为了解决空间定位问题，keyframe的2D
特征被extracted in parallel，用Darknet-19作为2D
CNN的基本架构，因为它在精度和效率上取得了平衡。key frame with the shape
<span class="math inline">\(C \times H \times W\)</span> 是输入clip的
<strong>most recent
frame</strong>，因此不需要一个额外的dataloader，Darknet-19的输出特征图的shape为
<span class="math inline">\(C&#39;&#39; \times H&#39; \times
W&#39;\)</span>，<span class="math inline">\(C= 3\)</span>，<span
class="math inline">\(C&#39;&#39;\)</span> 是输出的channels，<span
class="math inline">\(H&#39;= \frac{H}{32}, W&#39; =
\frac{W}{32}\)</span>，和3D
CNN的情况类似。YOWO的另一个重要特性是，它的2D CNN和3D
CNN的分支能够被任意的CNN架构代替，使其更灵活。需要注意的是：<strong>虽然YOWO有两个分支，但是它是一个统一的架构，能够端到端的训练</strong>。</li>
<li><font color=red>Feature aggregation: Channel Fusion and Attention
Mechanism(CFAM) </font>：让3D和2D网络的输出有相同的shape in the last two
dimensions，以至于两个feature
maps能够简单地融合。用concatenation来融合两个feature
maps。因此，融合的feature map能够 <strong>encoders both motion and
appearance
informatin</strong>，然后这个融合的特征输到CFAM模块里，这个模块是基于<font color=red>
Gram matrix</font>来映射 <strong>inter-channel
dependencies</strong>。虽然<strong>Gram
matrix</strong>最初是用来做<strong>style
transfer</strong>，最近用在了segmentation
task，这样一个注意力机制对于来自不同sources的feature的融合是有益的。能够提高性能。concatenated
feature map <span class="math inline">\(A \in R^{(C&#39; + C&#39;&#39;)
\times H \times W}\)</span>，可以被视为3D 和2D 信息的一个 <strong>abrupt
combination</strong>，这忽略了它们之间的<strong>interrelationship</strong>。因此，将<span
class="math inline">\(A\)</span>输给两个conv layers来产生新的特征图
<span class="math inline">\(B \in R^{C \times H&#39; \times
W&#39;}\)</span>，之后，在特征图<span
class="math inline">\(B\)</span>上进行一些操作。$F R^{C N} $是 feature
map <span class="math inline">\(B\)</span>reshape之后的tensor， <span
class="math inline">\(N=H \times
W\)</span>，意味着每个channel中的features被 <strong>vectorized to one
dimension</strong>。然后 对 <span class="math inline">\(F \in R^{C
\times N}\)</span> 和它的转置 <span class="math inline">\(F^T \in R^{N
\times C}\)</span> 进行 矩阵乘法，来得到 <strong>Gram Matrix</strong>
<span class="math inline">\(G \in R^{C \times
C}\)</span>，这个矩阵能够表明不同channel之间的<strong>correlations</strong>。</li>
</ul>
<p><span
class="math display">\[\begin{array}{rcl}\mathbf{G}&amp;=&amp;\mathbf{F}\cdot\mathbf{F}^\mathrm{T}&amp;with&amp;G_{ij}&amp;=&amp;\sum_{k=1}^{N}F_{ik}\cdot
F_{jk}\end{array}\]</span></p>
<p><strong>Gram matrix G</strong>中的每个元素 <span
class="math inline">\(G_{ij}\)</span>代表 <strong>vectorize feature maps
i and j</strong> 之间的<strong>inner product</strong>。</p>
<p>在计算完Gram matrix之后，用一个softmax layer来得到 <strong>channel
attention map M</strong>, <span class="math inline">\(M \in R^{C \times
C}\)</span>，<span class="math inline">\(M_{ij}\)</span>表示<span
class="math inline">\(j^{th}\)</span> channel对 <span
class="math inline">\(i^{th}\)</span> channel之间的的影响。因此，<span
class="math inline">\(M\)</span> summaries 给定feature
map的features的<strong>inter-channel dependency</strong>。为了perform
impact of attention map to original features，对<span
class="math inline">\(M\)</span>和<span
class="math inline">\(F\)</span>做一个矩阵乘法，将结果shape到3维空间
<span class="math inline">\(R^{C \times H \times
W}\)</span>，这将和输入的tensor有相同的shape。</p>
<p><span class="math display">\[\mathbf{F}^{\prime} =
\mathbf{M}\cdot\mathbf{F}\\\mathbf{F}^{\prime}\in\mathbb{R}^{C\times
N}\xrightarrow{reshape}\mathbf{F}^{\prime\prime}\in\mathbb{R}^{C\times
H\times W}\]</span></p>
<p>channel attention module的输出 <span class="math inline">\(C \in R^{C
\times H \times W}\)</span>是 <span
class="math inline">\(F&#39;&#39;\)</span>和初始输入特征图 <span
class="math inline">\(B\)</span> 的的结合，with a trainable scalar
parameter <span class="math inline">\(\alpha\)</span>，用
<strong>element-wise sum operation</strong>，<span
class="math inline">\(\alpha\)</span> <strong>learns a weight from
0</strong>。 <span class="math display">\[\mathbf{C} =
\alpha\cdot\mathbf{F}^{\prime\prime}+\mathbf{B}\]</span></p>
<p>该方程表明：每个channel的最后的特征是 <strong>weighted sum of the
features of all channels and original features</strong>，这是对feature
maps之间的long-range的semantics dependencies的建模。最后 <span
class="math display">\[\mathbf{C}\in\mathbb{R}^{C\times H^{\prime}\times
W^{\prime}}\]</span> 给到了两个conv layer，来得到 <em>CFAM module</em>
的输出特征图 <span
class="math display">\[\mathbf{D}\in\mathbb{R}^{C^{*}\times
H^{\prime}\times W^{\prime}}\]</span>，在CFAM模块的开始和结束的2 个conv
layer很重要，因为它们 <strong>mix the features from different
distributions</strong>，没有这些conv
layers，CFAM模块的性能提升会有限。</p>
<p>这样的一个架构 promote feature representativeness in terms of
<strong>inter-dependencies among
channels</strong>，因此来自不同branches的features能够被reasonably and
smoothly汇聚到一起。另外，<strong>Gram matrix</strong> 考虑整个 feature
map。两个flattened feature vectors的<strong>点乘</strong> 展示了
<strong>它们之间的relation
information</strong>。一个比较大的product表明两个 channels的features
<strong>more correlated</strong>，smaller
product表明它们彼此不一样。对于一个给定的channel，<strong>allocate more
weights to other channels which are much correlated and have more impact
to it</strong>。通过这种机制，上下文的关系被
<strong>emphasized，features discriminability is enhanced</strong>。</p>
<ul>
<li><font color=red> Bounding box regression</font>：follow
YOLO相同的guidelines for bbox regression。最后一个conv layer with <span
class="math inline">\(1 \times 1\)</span> kernels用来产生
<strong>desired number of output channels。对于每个grid cel in <span
class="math inline">\(H&#39; \times W&#39;\)</span>。用 k-means
方法在对应的datasets上选择5个prior anchors, with </strong>NumCls class
conditional action scores, 4 coordinates and confidence
score**。YOWO的最后的输出的size是 <span
class="math inline">\([(5\times(NumCls+5))\times H&#39;\times
W&#39;]\)</span>。bboxes的回归然后基于这些anchors进行refined。</li>
</ul></li>
<li><p>在训练和测试阶段的输入分辨率都为 <span class="math inline">\(224
\times 224\)</span>，用不同的分辨率进行multi-scale
training在实验中没有发现有性能的提高。损失函数和原始的YOLOv2的网络中类似，除了这个采用了
smooth L1 Loss with beta=1 for localization，</p>
<p><span
class="math display">\[L_{1,smooth}(x,y)=\begin{cases}0.5(x-y)^2&amp;if|x-y|&lt;1\\\\|x-y|-0.5&amp;otherwise\end{cases}\]</span></p>
<p><span class="math inline">\(x,y\)</span>分别指prediction和ground
truth。L1 loss相比于MSE loss，对
outliers不那么敏感，能够在某些情况阻止梯度爆炸。用MSE loss for
confidence scores.</p>
<p><span class="math display">\[L_{MSE}(x,y)=(x-y)^2\]</span></p>
<p>最后的detection loss是 individual coordiante losses for
x,y,width,height和confidence score loss，</p>
<p><span
class="math display">\[L_D=L_x+L_y+L_w+L_h+L_{conf}\]</span></p>
<p>用focal loss for classification:</p>
<p><span class="math display">\[L_{focal}(x,y)=y(1-x)^\gamma
log(x)+(1-y)x^\gamma log(1-x)\]</span></p>
<p>x是 softmaxed network prediction, <span class="math inline">\(y \in
{0,1}\)</span> is grouth truth class label。<span
class="math inline">\(\gamma\)</span>是modulating factor，reduce loss of
samples with high confidence(easy samples), increase the loss of samples
with low confidence(hard
samples)。AVA数据集是一个多标签数据集，每个人执行一个pose
action和多个human-human or human-object interaction actions。因此，用
<strong>softmax to pose classes and sigmoid to the interaction
actions</strong>。另外，AVA是一个不平衡的数据集，modulating factor <span
class="math inline">\(\gamma\)</span>
不足以处理数据集的不平衡问题。因此用了一个focal loss的 <span
class="math inline">\(\alpha -balanced variant\)</span>，对于 <span
class="math inline">\(\alpha\)</span>，<strong>we have used exponential
of class sample
ratios</strong>。最后的YOWO用的loss是检测的loss和分类的loss的和。</p>
<p><span class="math display">\[L_{final}=\lambda
L_D+L_{Cls}\]</span></p>
<p>这里 <span class="math inline">\(\lambda =0.5\)</span>
在实验中表现最好。</p></li>
<li><p>分别初始化3D 和2D CNN网络：用在Kinetics上预训练的models来初始化
3D CNN，用在PASCAL VOC上预训练的models来初始化2D CNN。虽然架构是由2D
CNN和 3D CNN组成。这些参数能够一起更新。选择 <strong>mini-batch SGD with
momentum and weight decay</strong>来优化loss function。学习率初始为
0.0001。在训练的时候，由于J-HMDB-21的样本数量少，冻结所有的 3D conv
net的参数，因此收敛会更快，减小过拟合的风险。另外，在训练中用一些数据增强的方式例如flipping,
random scaling等。</p></li>
<li><p><strong>linking strategy</strong>：在得到了frame-level的 action
detection之后，下一步是将这些检测到的 bboxes 连接起来，构建
<strong>action tubes in the whole video</strong>。
利用连接算法，来找到最优的video-level action detections。 假设 <span
class="math inline">\(R_t\)</span>和 <span
class="math inline">\(R_{t+1}\)</span>是连续帧 <span
class="math inline">\(t\)</span> 和 <span
class="math inline">\(t+1\)</span>的两个区域。linking score for an
action class <span class="math inline">\(c\)</span> 定义为： <span
class="math display">\[\begin{aligned}s_{c}(R_{t},R_{t+1})&amp;=\quad\psi(ov)\cdot[s_{c}(R_{t})+s_{c}(R_{t+1})\\&amp;+\alpha\cdot
s_{c}(R_{t})\cdot s_{c}(R_{t+1})\\&amp;+\beta\cdot
ov(R_{t},R_{t+1})]\end{aligned}\]</span></p>
<p><span class="math inline">\(s_{c}(R_{t})\)</span>和<span
class="math inline">\(s_{c}(R_{t+1})\)</span> 是regions <span
class="math inline">\(R_t\)</span>和 <span
class="math inline">\(R_{t+1}\)</span>的 class specific socres。<span
class="math inline">\(ov\)</span>是
两个区域的IoU，如果overlap存在，则<span
class="math inline">\(\psi(ov)\)</span>为1，否则为0.在linking
score的定义上增加了一个额外的项：<span class="math inline">\(\alpha\cdot
s_{c}(R_{t})\cdot
s_{c}(R_{t+1})\)</span>。将两个连续帧的剧烈变化考虑进来了，能够提高video
detections的性能。在计算出所有的linking scores之后，用 <strong>Viterbi
algorithm</strong>来找到最有的路径，生成 action tubes。</p></li>
<li><p><strong>long-term feature
bank</strong>：虽然YOWO的推理是在线的和因果的 with small clip
size，但是16帧的输入限制了 temporal information required for action
understanding。因此，利用long-term feature
back(LFB)，这个包含了不同的timestamps的来自3D
CNN的features。在推理时，3D features centering the keyframe are
averaeged and resulting feature map用作输入，给到CFAM block，<strong>LFB
features are extracted for non-overlapping 8-frames clips using the
pretrained 3D ResNeXt-101 backbone</strong>。用 8 features centering the
key-frame。因此在推理的时候，利用了总共64帧的数据。LFB增加了action
classificatin的性能，类似于difference between clip accuracy and video
ccuracy in video datasets。然后，LFB会导致一个非因果的架构，因为 future
3D features在推理的时候用到了。</p></li>
</ol>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/08c936517b8e8b9fcab9a544c735be2909fab3ee/8-Figure2-1.png"
alt="YOWO" />
<figcaption aria-hidden="true">YOWO</figcaption>
</figure>
<p><span class="math inline">\(Figure \ \ 1^{[1]}\)</span>: The YOWO
architecture. An input clip and corresponding key frame is fed to a 3D
CNN and 2D-CNN to produce output feature volumes of <span
class="math inline">\([C&#39;&#39; × H&#39; × W&#39;]\)</span> and <span
class="math inline">\([C&#39; × H&#39; × W&#39;]\)</span>,respectively.
These output volumes are fed to channel fusion and attention mechanism
(CFAM) for a smooth feature aggregation. Finally, one last conv layer is
used to adjust the channel number for final bounding box
predictions.</p>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/08c936517b8e8b9fcab9a544c735be2909fab3ee/9-Figure3-1.png"
alt="Channel Fusion" />
<figcaption aria-hidden="true">Channel Fusion</figcaption>
</figure>
<p><span class="math inline">\(Figure \ \ 2^{[1]}\)</span>: Channel
fusion and attention mechanism for aggregating output feature maps
coming from 2D-CNN and 3D-CNN branches</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/YoungBlogs/tags/Action-Detection/" rel="tag"># Action Detection</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/YoungBlogs/2024/08/24/Holistic-Interaction-Transformer/" rel="prev" title="Holistic Interaction Transformer">
                  <i class="fa fa-angle-left"></i> Holistic Interaction Transformer
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/YoungBlogs/2024/08/24/EVAD/" rel="next" title="EVAD">
                  EVAD <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-user"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Young</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">64k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">3:53</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="100" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/YoungBlogs/js/comments.js"></script><script src="/YoungBlogs/js/utils.js"></script><script src="/YoungBlogs/js/motion.js"></script><script src="/YoungBlogs/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/YoungBlogs/js/third-party/math/mathjax.js"></script>



</body>
</html>
