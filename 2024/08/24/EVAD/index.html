<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/YoungBlogs/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/YoungBlogs/images/Icon.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/YoungBlogs/images/Icon.jpg">
  <link rel="mask-icon" href="/YoungBlogs/images/logo.svg" color="#222">

<link rel="stylesheet" href="/YoungBlogs/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"young-eng.github.io","root":"/YoungBlogs/","images":"/YoungBlogs/images","scheme":"Gemini","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/YoungBlogs/js/config.js"></script>

    <meta name="description" content="Efficient Video Action Detection with Token Dropout and Context Refinement[1]  作者是来自nju、蚂蚁集团、复旦和上海AI Lab的Lei Chen、Zhan Tong、Yibing Song等人。论文引用[1]:Chen, Lei et al. “Efficient Video Action Detecti">
<meta property="og:type" content="article">
<meta property="og:title" content="EVAD">
<meta property="og:url" content="https://young-eng.github.io/YoungBlogs/2024/08/24/EVAD/index.html">
<meta property="og:site_name" content="Young&#39;s Blog">
<meta property="og:description" content="Efficient Video Action Detection with Token Dropout and Context Refinement[1]  作者是来自nju、蚂蚁集团、复旦和上海AI Lab的Lei Chen、Zhan Tong、Yibing Song等人。论文引用[1]:Chen, Lei et al. “Efficient Video Action Detecti">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-08-24T08:51:19.000Z">
<meta property="article:modified_time" content="2024-08-27T07:18:15.529Z">
<meta property="article:author" content="Young">
<meta property="article:tag" content="Action Detection">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://young-eng.github.io/YoungBlogs/2024/08/24/EVAD/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://young-eng.github.io/YoungBlogs/2024/08/24/EVAD/","path":"2024/08/24/EVAD/","title":"EVAD"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>EVAD | Young's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/YoungBlogs/css/noscript.css">
  </noscript>
<link rel="alternate" href="/YoungBlogs/atom.xml" title="Young's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/YoungBlogs/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Young's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/YoungBlogs/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/YoungBlogs/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/YoungBlogs/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/YoungBlogs/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/YoungBlogs/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#efficient-video-action-detection-with-token-dropout-and-context-refinement1"><span class="nav-number">1.</span> <span class="nav-text">Efficient
Video Action Detection with Token Dropout and Context
Refinement[1]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#time"><span class="nav-number">2.</span> <span class="nav-text">Time</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#key-words"><span class="nav-number">3.</span> <span class="nav-text">Key Words</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Young"
      src="/YoungBlogs/images/Avatar1.png">
  <p class="site-author-name" itemprop="name">Young</p>
  <div class="site-description" itemprop="description">记录学习和生活</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/YoungBlogs/archives/">
          <span class="site-state-item-count">82</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/YoungBlogs/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/YoungBlogs/tags/">
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Young-eng" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Young-eng" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yuen201718@163.com" title="Email → mailto:yuen201718@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>Email</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://young-eng.github.io/YoungBlogs/2024/08/24/EVAD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/YoungBlogs/images/Avatar1.png">
      <meta itemprop="name" content="Young">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Young's Blog">
      <meta itemprop="description" content="记录学习和生活">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="EVAD | Young's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          EVAD
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-08-24 16:51:19" itemprop="dateCreated datePublished" datetime="2024-08-24T16:51:19+08:00">2024-08-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-08-27 15:18:15" itemprop="dateModified" datetime="2024-08-27T15:18:15+08:00">2024-08-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/YoungBlogs/categories/Papers/" itemprop="url" rel="index"><span itemprop="name">Papers</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h3
id="efficient-video-action-detection-with-token-dropout-and-context-refinement1">Efficient
Video Action Detection with Token Dropout and Context
Refinement<sup>[1]</sup></h3>
<blockquote>
<p>作者是来自nju、蚂蚁集团、复旦和上海AI Lab的Lei Chen、Zhan
Tong、Yibing Song等人。论文引用[1]:Chen, Lei et al. “Efficient Video
Action Detection with Token Dropout and Context Refinement.” 2023
IEEE/CVF International Conference on Computer Vision (ICCV) (2023):
10354-10365.</p>
</blockquote>
<h3 id="time">Time</h3>
<ul>
<li>2023.Aug</li>
</ul>
<h3 id="key-words">Key Words</h3>
<ul>
<li>spatiotemporal token dropout</li>
<li>maintain all tokens in keyframe representing scene context</li>
<li>select tokens from other frames representing actor motions</li>
<li>drop out irrelavant tokens.</li>
</ul>
<h3 id="总结">总结</h3>
<ol type="1">
<li>视频流clips with large-scale vieo tokens 阻止了ViTs for efficient
recognition，特别是在video action
detection领域，这是需要大量的时空representations来精确地actor
identification。这篇工作，提出了端到端的框架 <strong>for efficient video
action detection(EVAD) based on vanilla
ViTs</strong>。EVAD包含两个为视频行为检测的特殊设计。首先：提出来时空token
dropout from a keyframe-centric perspective. 在一个video clip中，main
all tokens from its keyframe，保留其它帧中和actor
motions相关的tokens。第二：通过利用剩余的tokens，refine scene context
for better recognizing actor identities。action
detector中的RoI扩展到时间域。获得的时空actor identity representations
are refined via scene context in a decoder with the attention
mechanism。这两个设计使得EVAD高效的同时保持精度。</li>
</ol>
<span id="more"></span>
<ol start="2" type="1">
<li><p>image patches视为ViT输入的tokens for
自注意力的计算。当识别一个video clip的时候，tokens
来自每个frame，形成大规模的input for ViTs，这些video
tokens会在训练和推理的时候引入很多计算，特别是在计算自注意力的时候。有一些尝试来减少vision
tokens for fast computations。对于video action detection
任务来说，平衡精度和效率是个挑战。这是因为在VAD中，需要定位每一帧中的actors，视频序列中的temporal
motion需要保持 for consistent identification。同时，scene context
应该被保留以区别其它actors。大量的表示actor motions和scene
context的video tokens会保持VAD的精度。这篇文章中，保留表示actor
motions和scene context的video tokens，同时dropping out
不相关的tokens。<strong>基于视频clips的时序连贯性，从keyframe-centric的角度，提出来时空token
dropout</strong>。<strong>对于每个视频clip,选择代表scene
context的keyframe，这里的所有tokens将被maintained。同时，从其它代表actor
motions的帧中选择tokens。另外，drop out 这个clip中的剩余的video
tokens</strong>。</p>
<p><font color=red>通过一个keyframe-centric token pruning module with
the ViT encoder backbone，实现spatiotemporal token
dropout。这个keyframe要么是均匀采样得到的，要么人为在video
clips中定义的。默认是选择input clip中带有box annotations的middle
frame</font>。另外，提取由这个keyframe增强的attention map。这个attention
map在non-keyframe中知道toke
dropout。在定位之后，需要分类每个定位好的bbox for actor
identification。当video tokens
在non-keyframes中是不完整的时候，分类的性能在bbox regions (where tokens
have been dropped
out)中是较差的。然而，视频的内在时序一致性使得能够refine both
actor和scene context from the remaining video
tokens。扩展时域上的localized bboxes for RoIAlign，来获得与actor
motion相关的token
features。然后，引入一个decoder，通过这个clip中的剩余video
tokens的指导，来refine actor features。这个decoder concatenates
actor和token features，进行自注意力操作来产生丰富的actor features for
better identification。在token dropout之后，退化的action
classification能够用剩余的video tokens for context
refinement来恢复。这个恢复的性能和用整个video tokens for action
classification是一样的。通过这个context refinement，用减少了的video
tokens保持了VAD的性能。</p></li>
<li><p>相关工作:</p>
<ul>
<li><strong>Spatio-temporal Action
Detection</strong>：当前的SOTA方法采用两个分开的backbones、2个stages
pipelines。例如2D backbone for actor localization on keyframes，3D
backbone for video feature
extraction.之前的方法通过端到端的方式训练这两个backbone，来简化pipeline。这个会导致很高的复杂度和优化困难。最近的方法用统一的一个backbone来进行action
detection。 VAT是一个transformer风格的action
detector，来汇聚目标actor附近的时空上下文。WOO和TubeR是query-based的action
detectors，来预测actor bboxes和action
classes。STMixer是一个但阶段的query-based detector，来adaptively sample
discriminative
features。几个新的基于transformer的方法，用ViT的变种backbone，用两阶段的pipeline得到了一个比较好的结果。</li>
<li><strong>Spatio-temporal Redundancy</strong>：
<ol type="1">
<li>Spatial
Redundancy：<strong>DynamicViT</strong>观察到，精确的图像识别是基于最富有信息的tokens的子集，设计了一个dynamic
token
sparsification的框架，来剪掉多余的tokens。<strong>EViT</strong>计算了class
token to each token的attentiveness，确定了top-k tokens using the
attentiveness value。<strong>ATS</strong>引入了一个differentiable
adaptive token sampler for adaptively sampling 重要的tokens based on
image content。</li>
<li>Spatio-temporal
redundancy：由于视频数据集的高冗余性，有很多的研究关注于开发高效的视频识别。最近的方法展示了高效的schemes
specialized for video
transformers。<strong>MAR</strong>提出了一个人为设计的masking
策略，来丢弃 部分patches，减少video
tokens。<strong>K-centered</strong>提出了patch-based
sampling，超越了传统的frame-based
sampling。<strong>STTS</strong>用transformer在时间和空间上sequentially
选择tokens。</li>
</ol>
为了和这些方法对比，考虑keyframe和邻近帧的correlation,来drop out
冗余的tokens，然后执行端到端的基于transformer的action detector。</li>
</ul></li>
<li><p>Method：EVAD使encoder with token pruning
来去掉多余的tokens，decoder来refine actor spatiotemporal
features。和<strong>WOO</strong>设置相同，利用encoder中的keyframe的多个中间的spatial
feature maps for actor localization，最后一个encoder
layer输出的spatiotemporal feature map 用来做action classification。</p>
<ul>
<li><strong>Keyframe-centric Token
Pruning</strong>：相邻帧有着相似语义信息的video是高度冗余的，使得用一个比较高的dropout
rate on video transformers来做token
pruning是可能的。这里，将所有的spatiotemporal tokens分成keyframe
tokens和non-keyframe tokens。keyframe中，保留所有的tokens for accurate
actor localization。</li>
</ul>
<ol type="1">
<li><font color=red>Non-keyframe token pruning</font>：用
<strong>EViT</strong>中的方法，预先计算一个attention
map来表示每个token的重要性，不需要额外的learnable
parameters和计算开销。首先，average attention
map的<em>num_heads</em>维度，来得到一个 <span class="math inline">\(N
\times N\)</span>的matrix，表示tokens之间的
<strong>attentiveness</strong>。因为没有classification token for
video-level recognition，所以计算每个token的平均重要性分数 <span
class="math inline">\(I_{j}=\frac{1}{N}\sum_{i=1}^{N}attn(i,j)\)</span>。然后从
<span class="math inline">\(N_2\)</span> non-keyframe
tokens中，通过重要性分数的降序排列，找到 top(<span
class="math inline">\(N \times \rho - N_1\)</span>) tokens，这里<span
class="math inline">\(N, N_1, N_2\)</span>分别表示所有的tokens、keyframe
tokens和non-keyframe tokens。<span class="math inline">\(\rho\)</span>
表示 token keeping
rate，通常情况下，keyframe包含当前样本的最精确的语义信息，其它帧会带来信息bias。guided
by keyframe进行token pruning是符合实际的。为了这个目的，在acquiring
attention map和计算non-keyframe token重要之间插入一个 <strong>Keyframe
Attentiveness Enhancement</strong> step。用一个greater weight value to
keyframe queries，保留和keyframe
tokens高关联的tokens。每个token的重要性分数是这样更新的：</li>
</ol>
<p><span class="math display">\[I_j=\frac{1}{N}\sum_{i=1}^N
\begin{cases} w_{tf} \cdot attn(i,j),&amp;i \in
(0,N_1)\\\\attn(i,j),&amp;i \in (N_1,N)\end{cases}\]</span></p>
<p>假设first <span class="math inline">\(N_1\)</span>
tokens属于keyframe，weigth value <span
class="math inline">\(w_{kf}\)</span>
是一个超参数。丢掉仅和non-keyframes 有high
response的tokens，这些可能不是高质量的tokens。仅当non-keyframe变成了previous
or next samples的keyframe的时候，这些highly responsive
tokens才是高质量的。通过dropout
这些冗余的tokens，进一步减少tokens的数量；做完token
pruning之后，将这些保留的tokens给到之后的FFN。</p>
<p>第一次token pruning是在encoder layers 的1/3开始，之后，每个 1/4的
total layers进行一次 token pruning，丢弃冗余的tokens，保留effective
ones。</p>
<ul>
<li><strong>Video Action Detection</strong>：</li>
</ul>
<ol type="1">
<li><p><font color=red>Actor localization branch</font> 得到了keyframe
tokens，就能得到多个完整的keyframe feature maps。然后对这些feature
maps进行上采样或者下采样，来从palin ViT中产生hierarchical
features。受Sparse RCNN的启发，引入了query-based actor localization
head，来检测keyframe中的actors。actor localization branch的输出是 <span
class="math inline">\(n\)</span>个 prediction boxes in the
keyframe和对应的actor confidence scores。</p></li>
<li><p><font color=red>Action Classification branch</font>
不同于传统的特征提取，EVAD产生<span
class="math inline">\(M\)</span>个离散的video tokens。需要恢复video
feature map的时空结构，然后进行location-related
操作例如RoIAlign。初始化blank feature map shape as <span
class="math inline">\((T/2,H/16,W/16)\)</span>，用保留的tokens来填充这个feature
map according to 它们对应的时空位置，剩余的用0进行pad。</p></li>
</ol>
<p>然后，用localization branch产生的boxes，通过3D RoIAlign来提取actor
RoI features for subsequent action prediction。由于actor
movement或者相机的变化，actor的空间位置是逐帧变化的，用keyframe box for
3D RoIAlign不能得到partial actor feature deviated from the
box。直接扩展scope of the box来cover整个motion
trajectory，可能会引入背景或者其它的干扰信息，对actor feature
representation不利。然而，在EVAD feature
extraction阶段，<strong>视频中的干扰项会被逐步去掉，因此可以扩展scope of
box来增加deviated feature</strong>. <strong>
观察发现：</strong>直接用vision classification的token pruning
方法能减少计算中的tokens的数量，但是会对最后的检测性能有负面的影响。Video
action detection要localizing和classifying actions of all
actors，但是token pruning算法会导致时空上不连贯的actor
features**。在encoder中，pair-wise self-attention能够modeling global
dependency among tokens，actor regions内的dropout
tokens的语义信息能够被incorporated into some preserved
tokens，因此能够从保留的video tokens中恢复去掉的actor
features。为了这个目的，设计了一个<font color=red>context refinement
decoder</font>来refine actor的spatiotemporal
representation。具体地说：concatenate <span
class="math inline">\(M\)</span>个video tokens的 <span
class="math inline">\(n\)</span>个actor RoI
features，然后送到deocder中，guiding by 保留的tokens，actor features can
enrich themselves with actor represntation and motion information from
other frames。没有token pruning，decoder会被用来作为relational modeling
modules，来得到inter-actor和actor-context的interaction information。</p>
<p>decoder输出的<span class="math inline">\(n\)</span>个refined actor
features are retrieved，然后通过一个classification layer，做最后的action
prediction。</p></li>
<li><p>实验：</p>
<ul>
<li><strong>RoI extension</strong>：由于人的large
motion，从keyframe中得到的box不能cover整个motion
trajectory，Intuitively，通过适当地扩展box scope来解决这个问题。pruning
mechanism能够消除extension带来的interference information。结合了RoI
extension的pruning能够在一定程度上消除human movements的影响。</li>
<li>EVAD能够实现实时的推理in an end-to-end manner。</li>
<li>之前流行的model
是VideoMAE，是一个两阶段的model，需要一个离线的person
detector来pre-compute person
proposals。现在EVAD，用同样的预训练的backbone，能够获得和VideoMAE相当的性能。和其它端到端的模型例如
<strong>WOO</strong>和<strong>TubeR</strong>，用明显的性能提升；比基于CNN的model有更快的推理速度,
more friendly to real-time action detection。
比STMixer的性能略微好一点，<strong>STMixer是最近的端到端的模型，设计了一个decoder来采样discriminative
features。EVAD is deivsed for efficient video feature
extraction，进一步结合这两个或许会得到更好的检测性能</strong>。</li>
</ul></li>
<li><p>Conclusion：受video
sequence的transformer的大量的计算开销和视频检测中高度冗余时空信息的的启发，通过
<strong>droping out spatiotemporal tokens and refining scene context to
enable efficient transformer-based action
detection</strong>,设计了EVAD的方法。 EVAD的局限是：它需要
<strong>retraining once to take the benefits of reduced computations and
faster inference from removing redundancy</strong>。一个潜在的方法是探索
transformer-adaptive token pruning algorithms。另外，follow 端到端的框架
<strong>WOO</strong>来验证<strong>EVAD的效率和有效性</strong>，但是<strong>WOO</strong>是一个两阶段的pipeline，sequentially
执行actor localization和action classification
modules。在未来的工作中，旨在将这两个modules集成到一个 unified
head，能够减小通过 detector head的推理时间，能够amplify
EVAD的高效的特点。</p></li>
</ol>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/f3ea68b1948c43518259dc22dc121b2c9730103a/3-Figure2-1.png"
alt="Structure" />
<figcaption aria-hidden="true">Structure</figcaption>
</figure>
<p><span class="math inline">\(Figure \ 1^{[1]}\)</span>: Pipeline of
EVAD. Our detector consists of three parts: an encoder with
keyframe-centric token pruning for efficient video feature extraction, a
query-based localization branch using multiscale features of the
keyframe for actor boxes prediction, and a classification branch
conducting actor spatiotemporal feature refinement and relational
modeling between actor RoI features and compact context tokens from ViT
encoder.</p>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/f3ea68b1948c43518259dc22dc121b2c9730103a/4-Figure3-1.png"
alt="Keyframe-centric token pruning" />
<figcaption aria-hidden="true">Keyframe-centric token
pruning</figcaption>
</figure>
<p><span class="math inline">\(Figure \ 2^{[1]}\)</span>: Structure of
keyframe-centric token pruning.We conduct token pruning within the
original encoder layer.Specifically, we calculate the importance scores
of non-keyframe tokens using a keyframe-enhanced attention map.Then, we
preserve the top-k important non-keyframe tokens concatenated with
keyframe tokens as the results</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/YoungBlogs/tags/Action-Detection/" rel="tag"># Action Detection</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/YoungBlogs/2024/08/24/YOWO/" rel="prev" title="YOWO">
                  <i class="fa fa-angle-left"></i> YOWO
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/YoungBlogs/2024/08/24/TubeR/" rel="next" title="TubeR">
                  TubeR <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-user"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Young</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">71k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">4:17</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="100" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/YoungBlogs/js/comments.js"></script><script src="/YoungBlogs/js/utils.js"></script><script src="/YoungBlogs/js/motion.js"></script><script src="/YoungBlogs/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/YoungBlogs/js/third-party/math/mathjax.js"></script>



</body>
</html>
