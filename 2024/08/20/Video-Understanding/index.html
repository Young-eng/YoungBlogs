<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/YoungBlogs/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/YoungBlogs/images/Icon.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/YoungBlogs/images/Icon.jpg">
  <link rel="mask-icon" href="/YoungBlogs/images/logo.svg" color="#222">

<link rel="stylesheet" href="/YoungBlogs/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"young-eng.github.io","root":"/YoungBlogs/","images":"/YoungBlogs/images","scheme":"Gemini","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/YoungBlogs/js/config.js"></script>

    <meta name="description" content="视频理解及分析的计算机视觉任务  之前看的时候，不管是论文还是一些博客，感觉都不是很清晰和全面，大家的定义不全面，特别是英文的名称上，这里写一下我的理解： 几个任务：  行为识别(Action Recognition): 实质是对视频的分类任务，可以类别图像领域的分类任务 时序动作定位(Temporal Action Localization): 在时间上对视频进行分类，给出动作的">
<meta property="og:type" content="article">
<meta property="og:title" content="Video Understanding">
<meta property="og:url" content="https://young-eng.github.io/YoungBlogs/2024/08/20/Video-Understanding/index.html">
<meta property="og:site_name" content="Young&#39;s Blog">
<meta property="og:description" content="视频理解及分析的计算机视觉任务  之前看的时候，不管是论文还是一些博客，感觉都不是很清晰和全面，大家的定义不全面，特别是英文的名称上，这里写一下我的理解： 几个任务：  行为识别(Action Recognition): 实质是对视频的分类任务，可以类别图像领域的分类任务 时序动作定位(Temporal Action Localization): 在时间上对视频进行分类，给出动作的">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-08-20T06:12:07.000Z">
<meta property="article:modified_time" content="2024-08-23T08:38:57.194Z">
<meta property="article:author" content="Young">
<meta property="article:tag" content="Video Understanding">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://young-eng.github.io/YoungBlogs/2024/08/20/Video-Understanding/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://young-eng.github.io/YoungBlogs/2024/08/20/Video-Understanding/","path":"2024/08/20/Video-Understanding/","title":"Video Understanding"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Video Understanding | Young's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/YoungBlogs/css/noscript.css">
  </noscript>
<link rel="alternate" href="/YoungBlogs/atom.xml" title="Young's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/YoungBlogs/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Young's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/YoungBlogs/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/YoungBlogs/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/YoungBlogs/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/YoungBlogs/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/YoungBlogs/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E5%8F%8A%E5%88%86%E6%9E%90%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%BB%BB%E5%8A%A1"><span class="nav-number">1.</span> <span class="nav-text">视频理解及分析的计算机视觉任务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#deep-learnign-based-action-detection-in-untrimmed-videos-a-survey"><span class="nav-number">2.</span> <span class="nav-text">Deep
Learnign-based Action Detection in Untrimmed Videos: A Survey</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#a-survey-on-deep-learning-based-spatio-temporal-action-detection"><span class="nav-number">3.</span> <span class="nav-text">A
Survey on Deep Learning-based Spatio-temporal Action
Detection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">4.</span> <span class="nav-text">参考链接：</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Young"
      src="/YoungBlogs/images/Avatar1.png">
  <p class="site-author-name" itemprop="name">Young</p>
  <div class="site-description" itemprop="description">记录学习和生活</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/YoungBlogs/archives/">
          <span class="site-state-item-count">82</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/YoungBlogs/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/YoungBlogs/tags/">
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Young-eng" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Young-eng" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yuen201718@163.com" title="Email → mailto:yuen201718@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>Email</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://young-eng.github.io/YoungBlogs/2024/08/20/Video-Understanding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/YoungBlogs/images/Avatar1.png">
      <meta itemprop="name" content="Young">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Young's Blog">
      <meta itemprop="description" content="记录学习和生活">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Video Understanding | Young's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Video Understanding
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-08-20 14:12:07" itemprop="dateCreated datePublished" datetime="2024-08-20T14:12:07+08:00">2024-08-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-08-23 16:38:57" itemprop="dateModified" datetime="2024-08-23T16:38:57+08:00">2024-08-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/YoungBlogs/categories/Papers/" itemprop="url" rel="index"><span itemprop="name">Papers</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>22 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h4
id="视频理解及分析的计算机视觉任务">视频理解及分析的计算机视觉任务</h4>
<ol type="1">
<li><p>之前看的时候，不管是论文还是一些博客，感觉都不是很清晰和全面，大家的定义不全面，特别是英文的名称上，这里写一下我的理解：</p></li>
<li><p>几个任务：</p>
<ul>
<li><strong>行为识别(Action Recognition)</strong>:
实质是对视频的分类任务，可以类别图像领域的分类任务</li>
<li><strong>时序动作定位(Temporal Action Localization)</strong>:
在时间上对视频进行分类，给出动作的起止时间和类别</li>
<li><strong>时空行为检测(Spatio-Temporal Action Detection)</strong>:
不仅识别出动作出现的<strong>区间</strong>和<strong>类别</strong>，还要在空间范围内用一个bounding
box标记处目标的<strong>位置</strong>。</li>
<li><strong>还有人提出了时空动作定位(Spatio-temporal Action
localization)</strong>：和上一个是一样的</li>
<li>Action Detection在Paperswithcode上的定义： aims to find both where
and when an action occurs within a video clip and classify what the
action is taking place. Typically results are given in the form of
<strong>action tublets</strong>, which are action bounding boxes linked
across time in the video. <em>This is related to temporal localization,
which seeks to identify the start and end frame of an action, and action
recognition, which seeks only to classify which action is taking place
and typically assumes a trimmed video.</em></li>
<li>论文里还提到了<strong>temporal action segmentation</strong>：
针对细粒度的actions和videos with dense occurrence of actions to predict
action label labels at every frame of the video.</li>
</ul></li>
<li><p>时空行为检测的算法：之前的论文都是都是基于行为识别(action
recognition)的，很多都是基于早期的Slowfast的那个检测的方式：需要一个额外的检测器，实现行为检测。也就是在行为识别的基础上，再进行时空行为检测。但这并不是我理想中的方式，所以很多行为识别的算法，在AVA上也能上榜；最近看VideoMAE看了之后，就一直在看这个，没有去看看其它的。</p></li>
<li><p>Action Detection数据集：</p>
<ul>
<li>J-HMDB</li>
<li>UCF101-24</li>
<li>MultiSports</li>
<li>AVA</li>
<li>其中，JHMDB和UCF101-24是密集标注数据集(每一帧都标注，25fps)，这类数据集每个视频只有一个动作，大部分视频是单人做一些语义简单的重复动作；AVA为代表的稀疏标注数据集(隔一段时间标注一帧，1fps)，没有给出明确的动作边界</li>
</ul></li>
</ol>
<span id="more"></span>
<h4
id="deep-learnign-based-action-detection-in-untrimmed-videos-a-survey">Deep
Learnign-based Action Detection in Untrimmed Videos: A Survey</h4>
<blockquote>
<p>作者是来自纽约城市大学的Elahe Vahdani and Yingli
Tian,论文引用[1]:Vahdani, Elahe and Yingli Tian. “Deep Learning-Based
Action Detection in Untrimmed Videos: A Survey.” IEEE Transactions on
Pattern Analysis and Machine Intelligence 45 (2021): 4302-4320.</p>
</blockquote>
<ol type="1">
<li><p>很多action recognition 算法是在untrimmed
video里，真实世界中的视频大部分是漫长的和untrimmed with sparse segments
of interest. temporal activity
detection在没有剪辑的视频里的任务是定位行为的时间边界和分类行为类别。spatio-temporal
action
detection：action在temporal和spatial维度上都进行定位，还需识别行为的类别。因为长的未修剪的视频的标注费时费力，所以
<strong>action detection with limited supervision</strong>
是一个重要的研究方向。</p></li>
<li><p><strong>Temporal Action Detection</strong> 旨在 在untrimmed
video里找到精确的时间边界和行为实例的label。依赖于训练集的标注的availability，可以分为：</p>
<ul>
<li>全监督 action detection: 时间边界和labels of action instances are
available</li>
<li>弱监督action detection：only the video-level labels of action
instances are available, the order of action labels can be provided or
not.</li>
<li>unsupervised action detection: no annotations for action
instances</li>
<li>semi-supervised action detection: 数据被划分为小的子集 <span
class="math inline">\(S_1\)</span> 和大的子集 <span
class="math inline">\(S_2\)</span>，<span
class="math inline">\(S_1\)</span> 中的视频是全标注的，<span
class="math inline">\(S_2\)</span>中的视频没有标注(as in
fully-supervised)或者only annotated with video-level labels(as
weakly-supervised)</li>
<li>self-supervised action detection:
用一个代理任务从数据中抽取信息，然后用于提高性能，例如一般的自监督预训练，然后有监督微调。</li>
<li>Temporal action detection：或者说temporal action localization,
思路和图像中的目标检测类似，会用到 proposals、RoI
pooling这些类似的思路。</li>
</ul></li>
<li><p>Untrimmed
videos通常很长，由于计算资源的限制，很难直接把整个视频给到visual encoder
来提取特征。通常的做法是把视频划分为相同大小的temporal intervals called
<em>snippets</em>，然后对每个snippet都用visual encoder。</p></li>
<li><p><strong>Spatio-temporal Action Detection</strong>:有frame-level
action detection和clip-level action detection</p>
<ul>
<li><strong>frame-level action detection</strong>:
早期的方式是基于滑动窗口的一些扩展方法，要求一些很强的假设：例如cuboid
shape，一个actor的跨帧的固定的空间范围。图像的目标检测启发了识别人类行为
at frame level; 第一阶段，通过region proposal 或者 densely sampled
anchors 产生action proposals，然后第二阶段proposals用于action
classification和localization refinement. 在检测frames里的action
regions之后，一些方法，用光流来获取运动信息，<strong>用linking
algorithm来连接frame-level bounding box into spatio-temporal action
tubes</strong>；有人用dynamic programming approach来连接 resulting
per-frame detection，这个cost
function是基于boxes的检测分数和连续帧之间的重叠；也有人用
tracking-by-detection方法来代替linking
algorithm；另外一组是依赖于actionness measure, 例如pixel-wise
probability of containing any action.
为了估计actionness，它们用low-level cues例如光流；通过thresholding the
actionness scores来抽取action tubes，这个输出是action的rough
localization。这些方法的主要的缺点是没有完全利用视频的时序信息，检测是在每一帧上独立做的，<strong>有效的时序建模是很重要，因为当时序上下文信息是可用的时候，大量的actions才是可识别的。</strong></li>
<li><strong>Clip-level action detection</strong>: 通过在clip
level执行action detection来利用时序信息。Kalogeiton提出了action tubelet
detector(ACT-detector)，输入为一系列的frames，输出action来别和回归的tubelets：系列的带有associated
scores的bounding box。tubelets被连接，来构造action
tubes。Gu等人进一步通过用longer clips和利用I3D pre-trained on the
large-scale video dataset，展示了时序信息的重要性。为了生成action
proposals，把2D region proposals扩展到3D，假定spatial
extent在一个clip内是固定的。随着时间的推移，用较大空间唯一的action
tubes将要违反假设，特别是当clip是很长，涉及actors或者camera的快速移动。</li>
<li><strong>modeling spatio-temporal dependencies</strong>:
理解人类行为要求理解它们身边的人和物体。一些方法用图结构的网络、注意力机制来汇集视频中的物体和人的上下文信息。时空关系通过多层图结构的自注意力来学习，这个能够连接连续clips的entities，因此考虑long-range
spatial and temporal dependencies。</li>
<li>Metrics for Spatio-temporal action detection:
<strong>frame-AP</strong>: measures the area under the precision-recall
curve of the detections for each frame. frame中的IoU大于某个阈值且action
label是正确的，则detection 是正确的。<strong>video-AP</strong>: measures
the area under the precision-recall curve of the action tubes
predictions。如果整个视频帧中，mean per frame IoU大于某个阈值且action
label预测正确，则tube 是正确的.</li>
</ul></li>
</ol>
<h4
id="a-survey-on-deep-learning-based-spatio-temporal-action-detection"><strong>A
Survey on Deep Learning-based Spatio-temporal Action
Detection</strong></h4>
<blockquote>
<p>作者是来自浙大和蚂蚁集团的Peng Wang, Fanwei Zeng和Yuntao
Qian，论文引用[2]:Wang, Peng et al. “A Survey on Deep Learning-based
Spatio-temporal Action Detection.” ArXiv abs/2308.01618 (2023): n.
pag.</p>
</blockquote>
<ol type="1">
<li><p>Spatio-temporal action detection(STAD)
旨在对视频中出现的行为进行分类，然后在空间和时间上进行定位。传统的STAD方式涉及到了滑动窗口，例如deformable
part models, branch and bound approach.
模型主要划分为2类：frame-level和clip-level；<em>frame-level</em>预测 2D
bounding box for a frame; <em>clip-level</em>预测 3D spatio-temporal
tubelets for a clip.</p></li>
<li><p><strong>Frame-level</strong>:
目标检测做的很成功，研究人员将目标检测的模型泛化到STAD
领域，直接的思路是：把STAD in video视为 2D image
检测的集合。具体地说，在每一帧上用action detector来检测得到
<strong>frame-level 2D bounding box</strong>。然后用<strong>linking or
tracking算法</strong>关联这些frame-level detection results，生成
<strong>3D action proposals</strong>。作者从<strong>Temporal
context</strong>、<strong>3D CNN</strong>、<strong>High efficiency and
real-time speed</strong>、<strong>Visual Relation
Modeling</strong>这几个角度给出了相关的算法。</p>
<p>有些借鉴了
RCNN、FasterRCNN的思路，用了RPN网络，然后用两个分支分别处理RGB和光流；然后融合外观和运动信息，Linked
up里得到 class-specific action tubes. 也有基于actionness
maps的方法。<strong>actionness是指在图像的特定位置包含一般的action
instance的可能性</strong>。
上述这些STAD方法独立的对待frame，忽视了时序上下文关系。为了克服这个问题，有人提出了<strong>cascade
proposal and location anticipation
model(CPLA)</strong>的方法，能够推理发生在两帧之间的运动趋势。用
<em>frame <span class="math inline">\(I_t\)</span> 上检测到的bbox来推理
<span class="math inline">\(I_{t+k}\)</span> frame上对应的 bbox。<span
class="math inline">\(k\)</span>是 anticipation gap</em>.
除了通过光流来获取视频里的运动特性之外，可以用 <font color=red> 3D CNN
</font>
来从多个相邻帧提取运动信息。后续的还有用<strong>X3D</strong>网络、<strong>ACDnet</strong>、将<strong>光流和RGB</strong>嵌入到一个单流网络中，利用光流来modulate
RGB特征、用<strong>SSD</strong>作为检测器、借鉴YOLO的<strong>YOWO</strong>：3D
CNN来提取时空信息，2D
model来提取空间信息、<strong>WOO</strong>：单个统一的网络，只用一个backbone来做actor
localization和action
classification、<strong>SE-STAD</strong>用FCOS作为目标检测器、<strong>EVAD</strong>用ViTs，通过drop
out non-kkeyframe tokens减小计算开销，refine scenen
context来增强模型性能。</p></li>
<li><p>frame-level的方法没有完全利用时序的信息，将视频帧视为独立的图像，因此提出了clip-level的STAD方法，将一系列的frames作为输入，直接输出检测到的<font color=red>
tubelet proposals(short sequence of bounding boxes)</font></p></li>
<li><p><strong>Clip-level</strong>: 输入一个video clip，模型输出一个3D
spatio-temporal tubelet proposals。3D tubelet proposals
是由一系列的bboxes that tightly bound the actions of interest
形成。然后这些tubelet proposals在successive
clips连接在一起，形成完整的action tubes。作者从几个<strong>Large
motion</strong>、<strong>Progressive learning</strong>、<strong>Anchor
free</strong>、<strong>Visual Relation
Modeling</strong>这几个角度给出了相关的算法。为了克服 3D anchors的 fixed
spatial exntet的问题，有人提出了 <font color=red> two-frame
micor-tubes</font>的方法。为了避免3D cuboid anchor，也有人提出了
<font color=red> 通过frame-level actor detection，然后将detected
bboxes连接起来形成class-independent action tubelets,然后给到temporal
understanding module来做行为分类 </font>。还有
<strong>sparse-to-dense</strong>的方法。在<em>progressive
learning</em>方面，通过progressive learning 方法，反复修正proposals
towards actions over a few
steps。有人提出了<strong>PCSC</strong>框架，以迭代的方式，用一个stream(RGB/Flow)里的region
proposals和features来帮助另一个stream(RGB/Flow)提高action localization
results。计算anchor是一个比较费劲的事情，提出了一些<strong>anchor-free</strong>的方法：有人把每个action
instance 视为moving points的轨迹。</p>
<ul>
<li><strong>MovingCenter
Detector(MOC-detector)</strong>，它由3个branches组成：center-branch for
instance center detection and action recognition; movement branch for
movement estimation;box branch for spatial extent detection.</li>
<li><strong>VideoCapsuleNet</strong>：用3D conv along with
capsules来学习必要的语义信息 for action detection and recognition。
有一个定位的component，利用capsules得到的action representation for a
pixel-wise localization of actions.</li>
<li><strong>TubeR</strong>：直接检测视频里的action
tubelet，同时执行action localization和recognition from a single
representation. 设计了一个tubelet-attention moduel 来model dynamic
spatio-temporal nature of a video clip. TubeR学习了tubelet
queries的集合，输出actio tubelets.</li>
</ul></li>
</ol>
<p>在<strong>Visual Relation Modeling</strong>方面，clip-level 的visual
relations也被探索了，来增强STAD模型；有人提出了 long short-term relation
network(LSTR)，获取short-term 和long-term relations in videos。
具体地说： LSTR先产生3D bboxes(tubelets) in each
video。然后通过<strong>spatio-temporal attention mechanism</strong> in
each clip 来建模human-context interactions。推理long-term temporal
dynamics across video clips via <em>graph ConvNet in a cascaded
manner</em>。 actor tubelets和object
proposals的特征然后被用于构建关系图，建模human-object manipulations and
human-human interaction actions。</p>
<ol start="4" type="1">
<li><p><strong>Linking up the Detection Results</strong>:
actions会持续一段时间，通常跨很多帧和clips。在frame-level或者clip-level检测结果得到之后，很多方法用一个<strong>linking
algorithm</strong>来detections across frames or
clips连接起来，形成video-level的action tubes。</p>
<ul>
<li><strong>linking up frame-level detection boxes</strong>：
第一个frame-level action detection linking
算法是由<strong>Gkioxari</strong>提出的，他们假设两个相邻region
proposals(bboxes)的空间范围有很好的重叠，且scores很高，有很大的可能性be
linked。计算两个region proposals的linking score的公式为： <span
class="math display">\[s_c(R_t,R_{t+1})=s_c(R_t)+s_c(R_{t+1})+\lambda\cdot
ov(R_t,R_{t+1}), Eq.(1)\]</span></li>
</ul>
<p><span class="math inline">\(s_c\)</span>(R_i)是region proposal
R_i的class specific score，<span
class="math inline">\(ov(R_i,R_j)\)</span> 是<span
class="math inline">\(R_i\)</span>和<span
class="math inline">\(R_j\)</span>的 IoU(overlap)。<span
class="math inline">\(\lambda\)</span>是一个超参数，对IoU项进行加权，有些模型输出的bbox是带有actionness
scores，这里就用actionness scores代替class-specific
scores。计算出所有的linking scores之后，<font color=red>
最优的path</font>通过这个来搜索： <span class="math display">\[\bar
R_c^*=\underset{\bar
R}{\text{argmax}}\frac{1}{T}\sum_{t=1}^{T-1}s_c(R_t,R_{t+1}),
Eq.(2)\]</span></p>
<p><span class="math inline">\(\bar{R}_{c} =
[R_{1},R_{2},\ldots,R_{T}]\)</span> 是action class <span
class="math inline">\(c\)</span>的一系列的linked region.
通过维特比算法来解这个优化问题。找到最有的path之后，region proposals in
<span class="math inline">\(\bar{R}_{c}\)</span> 从 set of region
proposals中去掉，然后再继续解该方程，直到set of region
proposals是空的。从Eq.(2)中计算得到的path被称为 <font color=red> action
tube</font>。 action tube <span
class="math inline">\(\bar{R}_{c}\)</span> 定义为：<span
class="math inline">\(S_{c}(\bar{R}_{c})=\frac1T\sum_{t=1}^{T-1}s_{c}(R_{t},R_{t+1}).\)</span></p>
<ul>
<li>基于Gkioxari的思路，Peng等人提出了在 Eq.(1)增加一个阈值函数，linking
score between two region proposals变成了： <span
class="math display">\[s_{c}(R_{t},R_{t+1})=s_{c}(R_{t})+s_{c}(R_{t+1})+\lambda\cdot
ov(R_{t},R_{t+1})\cdot\psi(ov)\]</span></li>
</ul>
<p><span class="math inline">\(\psi(ov)\)</span>是一个阈值函数，当<span
class="math inline">\(ov\)</span>大于<span
class="math inline">\(\tau\)</span>时，<span
class="math inline">\(\psi(ov)=1\)</span>，否则<span
class="math inline">\(\psi(ov)=0\)</span>。Peng在实验中发现，有了这个阈值函数，linking
score比之前更好了更robust了。Kopuklu进一步扩展了这个linking
score的定义：</p>
<p><span class="math display">\[\begin{aligned}
s_{c}\left(R_{t},R_{t+1}\right)=&amp;
\psi(ov)\cdot[s_{c}\left(R_{t}\right)+s_{c}\left(R_{t+1}\right) \\
&amp;+\alpha\cdot s_{c}\left(R_{t}\right)\cdot s_{c}\left(R_{t+1}\right)
\\
&amp;+\beta\cdot ov\left(R_{t},R_{t+1}\right)] ,
\end{aligned}\]</span></p>
<p>其中<span class="math inline">\(\alpha\)</span>和<span
class="math inline">\(\beta\)</span>是超参数，<span
class="math inline">\(\alpha \cdot s_c(R_t)\cdot
s_c(R_{t+1})\)</span>项将两个连续帧之间的dramatic change
考虑进去，提高video detection的性能</p>
<ul>
<li>在<strong>Temporal trimming</strong>中，上述的linking
算法得到了action tubes 横跨整个video duration，然而，human actions
通常只占很小一部分。为了决定一个action
instance的<strong>时间范围</strong>。有一些temporal
trimming的工作。Saha限制了consecutive proposals来得到smooth actionness
scores。通过动态规划解一个energy
maximization的问题。Peng等人依赖一个高校的maximum subarray
方法：给定一个video-level action tube <span
class="math inline">\(\bar{R}\)</span>, 它的理想的时间范围是从frame
<span class="math inline">\(s\)</span> to frame <span
class="math inline">\(e\)</span>，满足下列公式：</li>
</ul>
<p><span
class="math display">\[s_{c}(\bar{R}_{(s,e)}^{\star})=\underset{(s,e)}{\operatorname*{argmax}}\{\frac{1}{L_{(s,e)}}\sum_{i=s}^{e}s_{c}(R_{i})-\lambda\frac{|L_{(s,e)}-L_{c}|}{L_{c}}\},\]</span></p>
<p><span class="math inline">\(L_{(s,e)}\)</span>是 action tube
的长度，<span class="math inline">\(L_c\)</span>是 class <span
class="math inline">\(c\)</span>在训练集中的平均时长。</p>
<ul>
<li><p>在<strong>online action tube
generation</strong>中，在视频的第一帧，用 <span
class="math inline">\(n\)</span>个detected bboxes来初始化 <span
class="math inline">\(n\)</span> action tubes for each class <span
class="math inline">\(c\)</span>。然后，action
tubes通过增加frame中的box扩大或者在<span
class="math inline">\(k\)</span>个连续帧之后没有匹配的boxes，就会终结。最后，每个更新的tube通过执行binary
labeling using a online Viterbi算法来进行temporally trimmed。</p></li>
<li><p><strong>Linking up Clip-level Detection
Results</strong>:clip-level tubelet linking算法旨在 <strong>associate a
sequence of clip-level tubelets into video-level action
tubes</strong>。它们通常是从frame-level
box中得到。一个tubelet内的内容应该获取一个action，在任何两个连续的clips连接的tubelets应该有一个大的
<font color=red> temporal overlap
</font>，因此，他们定义tubelet's的linking score是这样的： <span
class="math display">\[S=\frac{1}{m}\sum_{i=1}^{m}Actionness_{i}+\frac{1}{m-1}\sum_{j=1}^{m-1}Overlap_{j,j+1}\]</span></p></li>
</ul>
<p><span class="math inline">\(Actionness_i\)</span> 表示 第<span
class="math inline">\(i\)</span>个clip的tubelet的actionness score。<span
class="math inline">\(Overlap_{j,j+1}\)</span>表示来自第<span
class="math inline">\(j\)</span>和第<span
class="math inline">\(j+1\)</span>个clip的两个proposals的Overlap。<span
class="math inline">\(m\)</span>是video
clips的总数，两个tubelets之间的overlap是基于<strong>第<span
class="math inline">\(j\)</span>个tubelet的最后一帧和<span
class="math inline">\(j+1\)</span>个tubelet的第一帧</strong>来计算的。在计算出了tubelets'
分数之后；另外，有人把frame linking的算法扩展到tubelet linking
来构建action tubes。核心的idea是这样的： -
初始化：在视频的第一帧，对每个tubelet开始一个new link，这里a link 指a
sequence of linked tubelets - Linking: 给定一个new frame <span
class="math inline">\(f\)</span>，扩展存在的links with one o the tubelet
candidates starting at this frame. 选择tubelet
candidate的标准如下：没有被其它links选择；有最高的action
score；和要被扩展的link的overlap高于给定的阈值。 -
终止：对于一个存在的Link，如果这个标注在<span
class="math inline">\(K\)</span>个连续帧之后没有被满足，这个link就会终止，<span
class="math inline">\(K\)</span>是一个给定的超参数。</p>
<p>由于它的简单和高效，tubelet linking算法被很多最近的工作采用。</p>
<p>在<strong>temporal trimming</strong>方面。tubelet linking
算法，初始和终止决定了action
tubes的时间范围，有人发现它不能彻底的解决transition state产生的temporal
location error。定义为ambiguous states，但不属于target
actions。为了解决这个问题，有人提出了transition-aware
classifier：能够区分transitional states和real
actions；后续也有人通过引入一个action switch regression
head：决定一个box
prediction是否描述了一个执行actions的actor。这个regression
head给出了一个tubelet每个bbox的action switch
score。如果这个score高于给定的阈值，这个box就包含这个action。这个action
switch regression head能够有效减小transitional states的误分类。</p></li>
<li><p>数据集：STAD中经常用到的数据集有：</p>
<ul>
<li><strong>Weizmann</strong>：在一个统一的背景中用一个静态相机记录，包含90个video
clips grouped 10 action classes, performed by 9 diffferent
subjects，每个video clip 包含多个单一行为的实例，空间分辨率为$180
$，每个clip是从1-5s。</li>
<li><strong>CMU Crowded Videos</strong>：包含5个
actions，每个action有5个training videos和48个test
videos。所有的video被缩放，空间分辨率为<span class="math inline">\(120
\times 160\)</span>，test
videos是5-37秒(166-1115帧)，这个数据集是在一个凌乱的和动态的环境中记录的，以至于这个数据集上的action
detection更加有挑战性。数据集是densely
annotated，提供时间和空间坐标(x,y,height,width,start,end and
frames)。</li>
<li><strong>MSR Action I and
II</strong>：是微软研究组弄的，II是I的扩展，Action I包含62个action
instances in 16个video sequences。II包含203 instances in 54
videos，每个video包含不同个体执行的多个actions。所有的视频是32-76秒，每个action
instance的时间和空间标注是提供的。包含3个action 类别。</li>
<li><strong>J-HMDB</strong>：是joint-annotated
HMDB数据集，HMDB包含5个action categories，每个category
包含至少101个视频片段，数据集包含6849个视频片段，分布在51个action
categories中。J-HMDB包含从HMDB数据集中宣导的21类视频，选择的视频涉及单个任务的动作，每个action
class有36-55个clips，每个clip包含15-40帧，总共928个clips，每个clip被裁剪了，第一帧和最后一帧对应一个action的开始和终止。frame的分辨率是
<span class="math inline">\(320 \times 240\)</span>, frame rate 是30
fps。</li>
<li><strong>UCF Sports</strong>：包含体育领域的10
个actions，所有的视频包含相机运动和复杂背景，包含150个clips，每个clip的frame
rate是10 fps，空间分辨率是 <span class="math inline">\(480 \times
360\)</span> to <span class="math inline">\(720 \times
576\)</span>，持续时间是 2.2 - 14.4秒，平均6.39秒。</li>
<li><strong>UCF101-24</strong>：UCF101的数据来自Youtube，包含101行为类别，总共13320个视频。对于行为检测任务，包含24个行为类别的3207个视频子集提供了密集标注，这个子集称为UCF101-24，不同于UCF
Sports和J-HMDB，视频是被剪过的，UCF101-24是没有被剪过的。</li>
<li><strong>THUMOS and
MultiTHUMOS</strong>：THMOS系列数据集包含4个数据集：THUMOS13，THUMOS14,THUMOS15和MultiTHUMOS，所有的视频是来自UCF101，THUMOS数据集包含24个action
classes，视频的时长从几秒到几分钟不等。数据集包含13000个被剪过的视频，超过1000个没有剪过的视频，超过2500个negative
sample
video。这些视频可能包含none、one、或者单个行为或者多个行为的实例。MultiTHUMOS是一个THUMOS的增强的版本，是一个dense、multi-class、frame-wise
labeled video dataset with 400 videos of 30
hours和65个类别的38690个标注。平均每帧有1.5个标注，每个视频10.5个行为类别。</li>
<li><strong>AVA</strong>：来自Youtube的430个movies，每个movie提供了第15到30分钟的这个clip，每个clip分成897个重叠3s的segments
with a stride of 1
second。对于每个segment，中间帧被选为keyframe，在每个keyframe，每个人都用bbox和actions标注，430个movies分成235个training，64个validation和131
test movies，差不多是55:15:30的比例，包含80个原子行为，60个actions
用来evaluation。</li>
<li><strong>MultiSports</strong>：这个视频来自Youtube上奥林匹克和世界杯的竞赛，包含4个运动，66个行为类别，每个运动800个clips，共3200个clips；包含37701个action
instances with
902k个bboxes，每个行为类别的instance从3个到3477个不等，显示了自然的长尾分布。每个视频被多个行为类别的多个实例标注，视频的平均长度是750帧，每个行为的segment比较短，平均24帧。</li>
</ul></li>
<li><p>评估指标：主要是两个：frame mAP和video-mAP</p>
<ul>
<li><strong>Frame mAP</strong>: area under the PR curve of bbox
detections at each frame.<strong>如果和GT
bbox的IoU大于给定的阈值且action
label是正确的，则detection是对的，阈值设为0.5</strong>。Frame-mAP能够独立于linking
strategy来比较检测精度。</li>
<li><strong>Video-mAP</strong>：area under PR curve of action tube
predictions。<strong>如果和GT tube的IoU大于给定的阈值且action
label是正确的，则tube
detection是对的</strong>，两个tubes之间的IoU被定义为时序上的IoU，<strong>multiplied
by the average of the IoU between boxes averaged over all overlapping
frames</strong>。 video-mAP的阈值通常设为0.2、0.5、0.75，and
0.5:0.95。对应于average video-mAP for thresholds with step 0.05 in this
range.
然而frame-mAP衡量的是单帧里的分类和空间检测的能力，video-mAP能够进一步评估时序检测的能力。</li>
</ul></li>
<li><p>未来的方向：</p>
<ul>
<li><strong>Lable-efficient learning for
STAD</strong>：STAD需要密集的标注，然而密集的标注是昂贵的。</li>
<li><strong>Online real-time
STAD</strong>：STAD有很多的在线的应用，必须基于过去的数据来给出当前帧的预测。这要求模型必须是轻量和高效的，还有很长的路。</li>
<li><strong>STAD under large
motion</strong>：在真实场景中，很多行为由于fast actor
displacement，camera motion,actions有很大的motion。</li>
<li><strong>Multimodal learning for STAD</strong>：action
video包含多个模态，包括视觉、声音甚至语言，因此，通过多模态学习，有潜力实现比单个模态更好的检测精度。另一方面，actions可以通过多种传感器得到，例如深度相机，红外相机，Lidar等，STAD或者可以从多个模态数据中学到的融合表征受益。</li>
<li><strong>Diffusion models for
STAD</strong>：扩散模型作为一类生成模型，从 sample in
随机分布开始，通过逐步地去噪恢复样本数据。尽管它们属于生成模型，它们对于表征的感知任务(例如目标检测和时序动作定位)，表现有效,输入随机的spatial
boxes(temporal proposals)，基于扩散的模型能够精确地产生目标框(action
proposals)，自从STAD视为目标检测和时序动作定位(temporal action
location)和结合体，有一些工作展示了利用diffusion
models来解决STAD任务。</li>
</ul></li>
</ol>
<h4 id="参考链接">参考链接：</h4>
<ul>
<li>https://0809zheng.github.io/2021/07/15/stad.html：
时空行为检测的比较好的介绍</li>
</ul>
<p>一些STAD相关的解释和算法图示：</p>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/2217f65a3fade532f079dbefe1230b47063417b7/8-Figure7-1.png"
alt="Spatio-Temporal Action Detection Task" />
<figcaption aria-hidden="true">Spatio-Temporal Action Detection
Task</figcaption>
</figure>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/2-Figure1-1.png"
alt="Spatio-temporal action detection" />
<figcaption aria-hidden="true">Spatio-temporal action
detection</figcaption>
</figure>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/2-Table1-1.png"
alt="Comparision" />
<figcaption aria-hidden="true">Comparision</figcaption>
</figure>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/2-Figure2-1.png"
alt="illustration" />
<figcaption aria-hidden="true">illustration</figcaption>
</figure>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/3-Figure3-1.png"
alt="Taxonomy of STAD models" />
<figcaption aria-hidden="true">Taxonomy of STAD models</figcaption>
</figure>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/3-Figure4-1.png"
alt="RPN for STAD" />
<figcaption aria-hidden="true">RPN for STAD</figcaption>
</figure>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/4-Figure5-1.png"
alt="I3D for STAD" />
<figcaption aria-hidden="true">I3D for STAD</figcaption>
</figure>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/4-Figure6-1.png"
alt="YOWO" />
<figcaption aria-hidden="true">YOWO</figcaption>
</figure>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/5-Figure7-1.png"
alt="ARCN" />
<figcaption aria-hidden="true">ARCN</figcaption>
</figure>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/6-Figure8-1.png"
alt="CFAD" />
<figcaption aria-hidden="true">CFAD</figcaption>
</figure>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/10-Table2-1.png"
alt="STAD datasets" />
<figcaption aria-hidden="true">STAD datasets</figcaption>
</figure>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/12-Table3-1.png"
alt="Performance on J-HMDB and UCF101-24" />
<figcaption aria-hidden="true">Performance on J-HMDB and
UCF101-24</figcaption>
</figure>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/13-Table4-1.png"
alt="Performance on UCF Sports and MultiSports datasets" />
<figcaption aria-hidden="true">Performance on UCF Sports and MultiSports
datasets</figcaption>
</figure>
<figure>
<img
src="https://d3i71xaburhd42.cloudfront.net/471baa377495a4175bbb2097809e0c64b8655d93/13-Table5-1.png"
alt="Performance on AVA" />
<figcaption aria-hidden="true">Performance on AVA</figcaption>
</figure>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/YoungBlogs/tags/Video-Understanding/" rel="tag"># Video Understanding</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/YoungBlogs/2024/08/19/ollama-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" rel="prev" title="ollama 大模型部署">
                  <i class="fa fa-angle-left"></i> ollama 大模型部署
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/YoungBlogs/2024/08/24/Holistic-Interaction-Transformer/" rel="next" title="Holistic Interaction Transformer">
                  Holistic Interaction Transformer <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-user"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Young</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">64k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">3:53</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="100" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/YoungBlogs/js/comments.js"></script><script src="/YoungBlogs/js/utils.js"></script><script src="/YoungBlogs/js/motion.js"></script><script src="/YoungBlogs/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/YoungBlogs/js/third-party/math/mathjax.js"></script>



</body>
</html>
