<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/YoungBlogs/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/YoungBlogs/images/Icon.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/YoungBlogs/images/Icon.jpg">
  <link rel="mask-icon" href="/YoungBlogs/images/logo.svg" color="#222">

<link rel="stylesheet" href="/YoungBlogs/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"young-eng.github.io","root":"/YoungBlogs/","images":"/YoungBlogs/images","scheme":"Gemini","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/YoungBlogs/js/config.js"></script>

    <meta name="description" content="An Image is Worth \(16 \times 16\) Words:Transformers For Image Recognition at Scale[1]  作者比较多，都是来自Google Research, Brain Team的Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenb">
<meta property="og:type" content="article">
<meta property="og:title" content="ViT">
<meta property="og:url" content="https://young-eng.github.io/YoungBlogs/2024/04/14/ViT/index.html">
<meta property="og:site_name" content="Young&#39;s Blog">
<meta property="og:description" content="An Image is Worth \(16 \times 16\) Words:Transformers For Image Recognition at Scale[1]  作者比较多，都是来自Google Research, Brain Team的Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenb">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-04-14T02:45:07.000Z">
<meta property="article:modified_time" content="2024-04-15T14:36:42.000Z">
<meta property="article:author" content="Young">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://young-eng.github.io/YoungBlogs/2024/04/14/ViT/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://young-eng.github.io/YoungBlogs/2024/04/14/ViT/","path":"2024/04/14/ViT/","title":"ViT"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>ViT | Young's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/YoungBlogs/css/noscript.css">
  </noscript>
<link rel="alternate" href="/YoungBlogs/atom.xml" title="Young's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/YoungBlogs/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Young's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/YoungBlogs/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/YoungBlogs/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/YoungBlogs/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/YoungBlogs/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/YoungBlogs/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#an-image-is-worth-16-times-16-wordstransformers-for-image-recognition-at-scale1"><span class="nav-number">1.</span> <span class="nav-text">An
Image is Worth \(16 \times 16\)
Words:Transformers For Image Recognition at Scale[1]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#time"><span class="nav-number">2.</span> <span class="nav-text">Time</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#key-words"><span class="nav-number">3.</span> <span class="nav-text">Key Words</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Young"
      src="/YoungBlogs/images/Avatar1.png">
  <p class="site-author-name" itemprop="name">Young</p>
  <div class="site-description" itemprop="description">记录学习和生活</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/YoungBlogs/archives/">
          <span class="site-state-item-count">89</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/YoungBlogs/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/YoungBlogs/tags/">
        <span class="site-state-item-count">35</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Young-eng" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Young-eng" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yuen201718@163.com" title="Email → mailto:yuen201718@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>Email</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://young-eng.github.io/YoungBlogs/2024/04/14/ViT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/YoungBlogs/images/Avatar1.png">
      <meta itemprop="name" content="Young">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Young's Blog">
      <meta itemprop="description" content="记录学习和生活">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="ViT | Young's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ViT
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-14 10:45:07" itemprop="dateCreated datePublished" datetime="2024-04-14T10:45:07+08:00">2024-04-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-15 22:36:42" itemprop="dateModified" datetime="2024-04-15T22:36:42+08:00">2024-04-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/YoungBlogs/categories/Papers/" itemprop="url" rel="index"><span itemprop="name">Papers</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h3
id="an-image-is-worth-16-times-16-wordstransformers-for-image-recognition-at-scale1">An
Image is Worth <span class="math inline">\(16 \times 16\)</span>
Words:Transformers For Image Recognition at Scale<sup>[1]</sup></h3>
<blockquote>
<p>作者比较多，都是来自Google Research, Brain Team的Alexey Dosovitskiy,
Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,Georg Heigold,
Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. 论文引用[1]:Dosovitskiy,
Alexey et al. “An Image is Worth 16x16 Words: Transformers for Image
Recognition at Scale.” ArXiv abs/2010.11929 (2020): n. pag.</p>
</blockquote>
<h3 id="time">Time</h3>
<ul>
<li>2020.Oct</li>
</ul>
<h3 id="key-words">Key Words</h3>
<ul>
<li>Vision Transformer</li>
<li>Image patches (in Vision) <span
class="math inline">\(\Leftrightarrow\)</span> tokens (words) in
NLP</li>
<li>larger scale training</li>
</ul>
<h3 id="总结">总结</h3>
<ol type="1">
<li>自注意力机制的dominant approach is 在large text
corpus预训练，然后在smaller task-specific dataset上进行微调。Thanks to
Transformers' computational efficiency 和可扩展性(scalability),
训练一个over 100B parameters、unprecedented
size的model成为可能。随着model和dataset的growing，没有饱和的迹象(no sign
of saturating performance)</li>
</ol>
<span id="more"></span>
<ol start="2" type="1">
<li><p>当在mid-sized dataset上 without strong regularization训练时,
model产生的modest accuracies of a few percentage points below ResNets of
comparable size. Transformer 缺少 <strong>inductive
biases</strong>,(inherent to CNNS),例如 translation equivariance and
locality(局部性)，因此在训练不足的数据时，泛化性不好。</p></li>
<li><p>当在更大的数据集上训练时(14M-300M
images)，大规模训练胜过了归纳偏置，(large scale training trumps
inductive bias), 在足够规模的的数据上进行训练，然后用fewer
datapoints迁移到任务时，取得了很好的效果。</p></li>
<li><p>自Transformer于2017年提出之后，有很人尝试将Transformer应用到Vision任务上，有的是将自注意力应用到局部邻域
for each query pixel instead of globally. 这样的局部多头自注意力
dot-product self-attention blocks能够完全替代卷积。还有人用Sparse
Transformers employ scalable approximations to global self-attention.
另外一种scale attention的方式是 apply it in blocks of varying
sizes。最相近的做法是从input image 里抽取出 <span
class="math inline">\(2 \times 2\)</span>的
patches。还有很多将self-attention和卷积结合起来的做法：e.g
用self-attention来处理CNN的输出。最近的一个模型是image
GPT(iGPT)，在减小image resolution和color space之后，将Transformer应用到
image pixels,
作为生成式模型，该模型是以无监督的方式进行训练的。</p></li>
<li><p>ViT:</p>
<ul>
<li>标准的Transformer接受1D的 sequence of token embeddings， 为了处理2D
image，将image <span class="math inline">\(X \in R^{H \times W \times
C}\)</span>, reshape成 a sequence of flattened 2D patches <span
class="math inline">\(X_p \in R^{N \times (P^2 * C)}\)</span>, <span
class="math inline">\((H,W)\)</span>是原始图片的resolution, <span
class="math inline">\(C\)</span>是 channel的数量，<span
class="math inline">\((P,P)\)</span>是 resolution of each image patch,
<span class="math inline">\(N = HW/P^2\)</span>，N是 patch的数量，也是
effective input sequence length for the Transformer.
Transformer在所有层中用一个常量<span
class="math inline">\(D\)</span>，表示latent vector size。所以将flatten
patches 然后map to D 维 with a trainable linear
projection，将这个project的输出称为patch embeddings。</li>
<li>与BERT的 <em>[class]</em> token类似，prepend一个learnable embedding
to the sequence of embeded patches(<span class="math inline">\(z^0 =
X_{class}\)</span>)，其在Transformer encoder输出的state(<span
class="math inline">\(z^0_L\)</span>)作为image representation
<strong>y</strong>，在预训练的时候，分类头是一个隐藏层的MLP，在fine-tuning的时候，是单个linear
layer.</li>
<li>Position embedding也加入到patch embeddings
来保持位置信息。用一个标准的可学习的1D position
embeddings，没有观察到用更高级的2D-aware position
embeddings带来更好的效果。</li>
<li>用公式表示为：</li>
</ul>
<p><span class="math display">\[\begin{gathered}
\mathbf{z}_{0} =[\mathbf{x}_{\mathrm{class}};
\mathbf{x}_{p}^{1}\mathbf{E}; \mathbf{x}_{p}^{2}\mathbf{E};\cdots;
\mathbf{x}_{p}^{N}\mathbf{E}]+\mathbf{E}_{pos}
\\\mathbf{E}\in\mathbb{R}^{(P^{2}\cdot C)\times D},
\mathbf{E}_{pos}\in\mathbb{R}^{(N+1)\times D} \left(1\right) \\
\mathbf{z}^{\prime}
_\ell=\mathrm{MSA}(\mathrm{LN}(\mathbf{z}_{\ell-1}))+\mathbf{z}_{\ell-1},
\ell=1\ldots L \left(2\right) \\
\text{z}
_{\ell}=\mathrm{MLP}(\mathrm{LN}(\mathbf{z}^{\prime}{}_{\ell}))+\mathbf{z}^{\prime}{}_{\ell},
\ell=1\ldots L \left(3\right) \\
\mathbf{y}=\mathrm{LN}(\mathbf{z}_{L}^{0}) \left(4\right)
\end{gathered}\]</span></p>
<ul>
<li><p>Inductive bias：ViT 有更少的image-specific 归纳偏置 than
CNNs，在CNNs，locality, two-dim neighborhood structure and translation
equivariance are baked into each layer throughout the whole
model。在ViT中，只有MLP层是local 和translation
equivariant，self-attention是global的。two-dim neighborhood
structure用的很少：在模型的开始，将image
切成patches，在fine-tuning的时候调整position embeddings for images of
different resolution。除此之外，position embeddings
在初始化的时候没有carry 关于patches的2D的位置信息，所有的spatial
relations between patches 是学来的from scratch.</p></li>
<li><p>混合架构：就是从CNN feature map中得到patches, project to
Transformer。</p></li>
<li><p>当feeding 高分辨率的图像时，保持patch size相同， results in a
larger effective sequence length。 ViT能够处理任意的sequence
length(取决于memory constraints),然而，pre-trained position
embeddings可能不再有意义。因此执行2D interpolation(2D插值) of the
pre-trained position
embeddings，根据它们在原始图像中的位置。注意到：这个resolution
调整和patch extraction 是 only points at which 关于Image 的2D
结构的归纳偏置引入到ViT中。</p></li>
</ul></li>
<li><p>评估ResNet, ViT和hybrid的representation
learning：在不同的datasets上预训练，ViT以更低的pre-training
cost(compoutiaional
cost)得到了state-of-the-art。最后用self-supervised做了一个小实验，展示了self-supervised
ViT的光明前景(self-supervised ViT holds promise for the
future).</p></li>
<li><p>做实验的时候，ViT的configurations是基于BERT,“Base" and "Large"
models 采用BERT的。fine-tuning
accuracies反应了模型在各自的数据集上fine-tune之后的性能。后面这个few-shot
accuracies就不太理解了。<strong>few-shot accuracies are obtained by
soling a regularized least-squares regression problem that maps the
(frozen) representation of a subset of training images to <span
class="math inline">\({[-1,1]}^K\)</span> 目标vectors。 this formulation
allows us to recover the exact solution in closed
form.</strong>。有时候用linear few-shot accuracies for fast on-the-fly
evaluation where fine-tuning would be too costly.</p>
<ul>
<li>ViT 似乎在尝试的范围内，没有saturate，motivating future scaling
efforts.</li>
<li>NLP的成功不仅在于Transformer的scalability,也在于 large scale
self-supervised pretraining.</li>
</ul></li>
<li><p>另外一个挑战是：继续探索 <strong>self-supervised
pre-training</strong>
方法。最初的实验表明自监督预训练带来的提升。最后，scaling of ViT would
likely lead to improved performance.</p></li>
</ol>
<p><img
src="https://user-images.githubusercontent.com/24582831/142903144-f80a12cc-8698-48ab-843c-49dedf558121.png"
alt="Framework" /> $ Fig.1^{[1]} $: Model overview. We split an image
into fixed-size patches, linearly embed each of them, add position
embeddings, and feed the resulting sequence of vectors to a standard
Transformer encoder. In order to perform classification, we use the
standard approach of adding an extra learnable “classification token” to
the sequence. The illustration of the Transformer encoder was inspired
by Vaswani et al. (2017)</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/YoungBlogs/tags/Transformer/" rel="tag"># Transformer</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/YoungBlogs/2024/04/09/AVADataset/" rel="prev" title="AVA Dataset">
                  <i class="fa fa-angle-left"></i> AVA Dataset
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/YoungBlogs/2024/04/14/Transformer-and-Survey/" rel="next" title="Transformer and Survey">
                  Transformer and Survey <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-user"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Young</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">77k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">4:41</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="100" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/YoungBlogs/js/comments.js"></script><script src="/YoungBlogs/js/utils.js"></script><script src="/YoungBlogs/js/motion.js"></script><script src="/YoungBlogs/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/YoungBlogs/js/third-party/math/mathjax.js"></script>



</body>
</html>
